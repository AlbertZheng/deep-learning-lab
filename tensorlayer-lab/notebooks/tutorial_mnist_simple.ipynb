{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tutorial_mnist_simple.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/AlbertZheng/deep-learning-lab/blob/master/tensorlayer-lab/notebooks/tutorial_mnist_simple.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "gQW-n1n8jS88",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6293
        },
        "outputId": "5657a820-3de0-4525-d84b-6c81df68175b"
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorlayer\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorlayer as tl\n",
        "\n",
        "# Confirm TensorFlow can see the GPU on Colaboratory\n",
        "device_name = tf.test.gpu_device_name()\n",
        "#if device_name != '/device:GPU:0':\n",
        "#  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "# Testing matrix multiply using GPU\n",
        "#sess = tf.InteractiveSession()\n",
        "\n",
        "#a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
        "#b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
        "#c = tf.matmul(a, b)\n",
        "# Runs the op.\n",
        "#print(sess.run(c))\n",
        "\n",
        "#sess.close()\n",
        "\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.DEBUG)\n",
        "tl.logging.set_verbosity(tl.logging.DEBUG)\n",
        "\n",
        "#sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
        "sess = tf.InteractiveSession()\n",
        "\n",
        "# prepare data\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = tl.files.load_mnist_dataset(shape=(-1, 784))\n",
        "# define placeholder\n",
        "x = tf.placeholder(tf.float32, shape=[None, 784], name='x')\n",
        "y_ = tf.placeholder(tf.int64, shape=[None], name='y_')\n",
        "\n",
        "# define the network\n",
        "network = tl.layers.InputLayer(x, name='input')\n",
        "network = tl.layers.DropoutLayer(network, keep=0.8, name='drop1')\n",
        "network = tl.layers.DenseLayer(network, 800, tf.nn.relu, name='relu1')\n",
        "network = tl.layers.DropoutLayer(network, keep=0.5, name='drop2')\n",
        "network = tl.layers.DenseLayer(network, 800, tf.nn.relu, name='relu2')\n",
        "network = tl.layers.DropoutLayer(network, keep=0.5, name='drop3')\n",
        "# the softmax is implemented internally in tl.cost.cross_entropy(y, y_) to\n",
        "# speed up computation, so we use identity here.\n",
        "# see tf.nn.sparse_softmax_cross_entropy_with_logits()\n",
        "network = tl.layers.DenseLayer(network, n_units=10, act=None, name='output')\n",
        "\n",
        "# define cost function and metric.\n",
        "y = network.outputs\n",
        "cost = tl.cost.cross_entropy(y, y_, name='cost')\n",
        "correct_prediction = tf.equal(tf.argmax(y, 1), y_)\n",
        "acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "y_op = tf.argmax(tf.nn.softmax(y), 1)\n",
        "\n",
        "# define the optimizer\n",
        "train_params = network.all_params\n",
        "train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost, var_list=train_params)\n",
        "\n",
        "# initialize all variables in the session\n",
        "tl.layers.initialize_global_variables(sess)\n",
        "\n",
        "# print network information\n",
        "network.print_params()\n",
        "network.print_layers()\n",
        "\n",
        "# train the network\n",
        "tl.utils.fit(sess, network, train_op, cost, X_train, y_train, x, y_, acc=acc, batch_size=500, \\\n",
        "    n_epoch=500, print_freq=5, X_val=X_val, y_val=y_val, eval_train=False)\n",
        "\n",
        "# evaluation\n",
        "tl.utils.test(sess, network, acc, X_test, y_test, x, y_, batch_size=None, cost=cost)\n",
        "\n",
        "# save the network to .npz file\n",
        "tl.files.save_npz(network.all_params, name='model.npz')\n",
        "\n",
        "sess.close()\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorlayer in /usr/local/lib/python3.6/dist-packages (1.9.1)\n",
            "Requirement already satisfied: tqdm<4.24,>=4.23 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (4.23.4)\n",
            "Requirement already satisfied: numpy<1.15,>=1.14 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (1.14.5)\n",
            "Requirement already satisfied: matplotlib<2.3,>=2.2 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (2.2.3)\n",
            "Requirement already satisfied: progressbar2<3.39,>=3.38 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (3.38.0)\n",
            "Requirement already satisfied: requests<2.20,>=2.19 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (2.19.1)\n",
            "Requirement already satisfied: scikit-learn<0.20,>=0.19 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (0.19.2)\n",
            "Requirement already satisfied: wrapt<1.11,>=1.10 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (1.10.11)\n",
            "Requirement already satisfied: scipy<1.2,>=1.1 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (1.1.0)\n",
            "Requirement already satisfied: lxml<4.3,>=4.2 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (4.2.4)\n",
            "Requirement already satisfied: scikit-image<0.15,>=0.14 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (0.14.0)\n",
            "Requirement already satisfied: imageio<2.4,>=2.3 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (2.3.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib<2.3,>=2.2->tensorlayer) (1.0.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib<2.3,>=2.2->tensorlayer) (2.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib<2.3,>=2.2->tensorlayer) (2.5.3)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib<2.3,>=2.2->tensorlayer) (1.11.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib<2.3,>=2.2->tensorlayer) (0.10.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from matplotlib<2.3,>=2.2->tensorlayer) (2018.5)\n",
            "Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2<3.39,>=3.38->tensorlayer) (2.3.0)\n",
            "Requirement already satisfied: urllib3<1.24,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<2.20,>=2.19->tensorlayer) (1.22)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<2.20,>=2.19->tensorlayer) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<2.20,>=2.19->tensorlayer) (2018.8.13)\n",
            "Requirement already satisfied: idna<2.8,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<2.20,>=2.19->tensorlayer) (2.6)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15,>=0.14->tensorlayer) (5.2.0)\n",
            "Requirement already satisfied: networkx>=1.8 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15,>=0.14->tensorlayer) (2.1)\n",
            "Requirement already satisfied: cloudpickle>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15,>=0.14->tensorlayer) (0.5.3)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15,>=0.14->tensorlayer) (0.5.2)\n",
            "Requirement already satisfied: dask[array]>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15,>=0.14->tensorlayer) (0.18.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib<2.3,>=2.2->tensorlayer) (39.1.0)\n",
            "Requirement already satisfied: decorator>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=1.8->scikit-image<0.15,>=0.14->tensorlayer) (4.3.0)\n",
            "Requirement already satisfied: toolz>=0.7.3; extra == \"array\" in /usr/local/lib/python3.6/dist-packages (from dask[array]>=0.9.0->scikit-image<0.15,>=0.14->tensorlayer) (0.9.0)\n",
            "Found GPU at: \n",
            "[TL] Load or Download MNIST > data/mnist\n",
            "[TL] data/mnist/train-images-idx3-ubyte.gz\n",
            "[TL] data/mnist/t10k-images-idx3-ubyte.gz\n",
            "[TL] InputLayer  input: (?, 784)\n",
            "[TL] DropoutLayer drop1: keep: 0.800000 is_fix: False\n",
            "[TL] DenseLayer  relu1: 800 relu\n",
            "[TL] DropoutLayer drop2: keep: 0.500000 is_fix: False\n",
            "[TL] DenseLayer  relu2: 800 relu\n",
            "[TL] DropoutLayer drop3: keep: 0.500000 is_fix: False\n",
            "[TL] DenseLayer  output: 10 No Activation\n",
            "[TL] WARNING: From <ipython-input-1-7d9b836f4a62>:60: initialize_global_variables (from tensorlayer.layers.utils) is deprecated and will be removed after 2018-09-30.\n",
            "Instructions for updating: This API is deprecated in favor of `tf.global_variables_initializer`\n",
            "\n",
            "[TL]   param   0: relu1/W:0            (784, 800)         float32_ref (mean: 7.035251474007964e-05, median: 1.4608026503992733e-06, std: 0.08787340670824051)   \n",
            "[TL]   param   1: relu1/b:0            (800,)             float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
            "[TL]   param   2: relu2/W:0            (800, 800)         float32_ref (mean: -9.861050784820691e-05, median: -9.416468674317002e-05, std: 0.08789864182472229)   \n",
            "[TL]   param   3: relu2/b:0            (800,)             float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
            "[TL]   param   4: output/W:0           (800, 10)          float32_ref (mean: -0.0018986096838489175, median: -0.002820536494255066, std: 0.08665783703327179)   \n",
            "[TL]   param   5: output/b:0           (10,)              float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
            "[TL]   num of params: 1276810\n",
            "[TL]   layer   0: x:0                  (?, 784)           float32\n",
            "[TL]   layer   1: drop1/mul:0          (?, 784)           float32\n",
            "[TL]   layer   2: relu1/Relu:0         (?, 800)           float32\n",
            "[TL]   layer   3: drop2/mul:0          (?, 800)           float32\n",
            "[TL]   layer   4: relu2/Relu:0         (?, 800)           float32\n",
            "[TL]   layer   5: drop3/mul:0          (?, 800)           float32\n",
            "[TL]   layer   6: output/bias_add:0    (?, 10)            float32\n",
            "[TL] Start training the network ...\n",
            "[TL] Epoch 1 of 500 took 9.530333s\n",
            "[TL]    val loss: 0.572504\n",
            "[TL]    val acc: 0.810600\n",
            "[TL] Epoch 5 of 500 took 9.455547s\n",
            "[TL]    val loss: 0.287238\n",
            "[TL]    val acc: 0.912000\n",
            "[TL] Epoch 10 of 500 took 9.490602s\n",
            "[TL]    val loss: 0.223958\n",
            "[TL]    val acc: 0.935800\n",
            "[TL] Epoch 15 of 500 took 9.913020s\n",
            "[TL]    val loss: 0.187767\n",
            "[TL]    val acc: 0.946200\n",
            "[TL] Epoch 20 of 500 took 9.548518s\n",
            "[TL]    val loss: 0.165006\n",
            "[TL]    val acc: 0.955300\n",
            "[TL] Epoch 25 of 500 took 9.616616s\n",
            "[TL]    val loss: 0.145953\n",
            "[TL]    val acc: 0.959900\n",
            "[TL] Epoch 30 of 500 took 9.442202s\n",
            "[TL]    val loss: 0.132670\n",
            "[TL]    val acc: 0.964200\n",
            "[TL] Epoch 35 of 500 took 9.509423s\n",
            "[TL]    val loss: 0.121866\n",
            "[TL]    val acc: 0.967400\n",
            "[TL] Epoch 40 of 500 took 9.565634s\n",
            "[TL]    val loss: 0.113898\n",
            "[TL]    val acc: 0.968600\n",
            "[TL] Epoch 45 of 500 took 9.545814s\n",
            "[TL]    val loss: 0.106099\n",
            "[TL]    val acc: 0.970500\n",
            "[TL] Epoch 50 of 500 took 9.526330s\n",
            "[TL]    val loss: 0.099798\n",
            "[TL]    val acc: 0.972000\n",
            "[TL] Epoch 55 of 500 took 9.477773s\n",
            "[TL]    val loss: 0.094662\n",
            "[TL]    val acc: 0.973700\n",
            "[TL] Epoch 60 of 500 took 9.464234s\n",
            "[TL]    val loss: 0.090466\n",
            "[TL]    val acc: 0.975200\n",
            "[TL] Epoch 65 of 500 took 9.476199s\n",
            "[TL]    val loss: 0.086684\n",
            "[TL]    val acc: 0.975700\n",
            "[TL] Epoch 70 of 500 took 9.505693s\n",
            "[TL]    val loss: 0.083045\n",
            "[TL]    val acc: 0.977100\n",
            "[TL] Epoch 75 of 500 took 9.503104s\n",
            "[TL]    val loss: 0.080044\n",
            "[TL]    val acc: 0.977900\n",
            "[TL] Epoch 80 of 500 took 9.691603s\n",
            "[TL]    val loss: 0.077667\n",
            "[TL]    val acc: 0.978600\n",
            "[TL] Epoch 85 of 500 took 9.531634s\n",
            "[TL]    val loss: 0.075249\n",
            "[TL]    val acc: 0.979200\n",
            "[TL] Epoch 90 of 500 took 9.481331s\n",
            "[TL]    val loss: 0.072981\n",
            "[TL]    val acc: 0.979200\n",
            "[TL] Epoch 95 of 500 took 9.510541s\n",
            "[TL]    val loss: 0.071376\n",
            "[TL]    val acc: 0.979700\n",
            "[TL] Epoch 100 of 500 took 9.541363s\n",
            "[TL]    val loss: 0.069779\n",
            "[TL]    val acc: 0.980100\n",
            "[TL] Epoch 105 of 500 took 9.522356s\n",
            "[TL]    val loss: 0.067755\n",
            "[TL]    val acc: 0.981200\n",
            "[TL] Epoch 110 of 500 took 9.453362s\n",
            "[TL]    val loss: 0.067154\n",
            "[TL]    val acc: 0.981200\n",
            "[TL] Epoch 115 of 500 took 9.473183s\n",
            "[TL]    val loss: 0.066566\n",
            "[TL]    val acc: 0.980900\n",
            "[TL] Epoch 120 of 500 took 9.492385s\n",
            "[TL]    val loss: 0.064949\n",
            "[TL]    val acc: 0.981600\n",
            "[TL] Epoch 125 of 500 took 9.491891s\n",
            "[TL]    val loss: 0.063663\n",
            "[TL]    val acc: 0.981900\n",
            "[TL] Epoch 130 of 500 took 9.472057s\n",
            "[TL]    val loss: 0.061827\n",
            "[TL]    val acc: 0.982900\n",
            "[TL] Epoch 135 of 500 took 9.517726s\n",
            "[TL]    val loss: 0.061880\n",
            "[TL]    val acc: 0.982700\n",
            "[TL] Epoch 140 of 500 took 9.445590s\n",
            "[TL]    val loss: 0.060774\n",
            "[TL]    val acc: 0.982600\n",
            "[TL] Epoch 145 of 500 took 9.512906s\n",
            "[TL]    val loss: 0.060496\n",
            "[TL]    val acc: 0.983300\n",
            "[TL] Epoch 150 of 500 took 9.456977s\n",
            "[TL]    val loss: 0.060185\n",
            "[TL]    val acc: 0.982900\n",
            "[TL] Epoch 155 of 500 took 9.540819s\n",
            "[TL]    val loss: 0.059276\n",
            "[TL]    val acc: 0.983400\n",
            "[TL] Epoch 160 of 500 took 9.565852s\n",
            "[TL]    val loss: 0.059418\n",
            "[TL]    val acc: 0.983400\n",
            "[TL] Epoch 165 of 500 took 9.472358s\n",
            "[TL]    val loss: 0.058509\n",
            "[TL]    val acc: 0.983800\n",
            "[TL] Epoch 170 of 500 took 9.485151s\n",
            "[TL]    val loss: 0.058843\n",
            "[TL]    val acc: 0.983600\n",
            "[TL] Epoch 175 of 500 took 9.590506s\n",
            "[TL]    val loss: 0.057399\n",
            "[TL]    val acc: 0.984100\n",
            "[TL] Epoch 180 of 500 took 9.666070s\n",
            "[TL]    val loss: 0.057315\n",
            "[TL]    val acc: 0.983800\n",
            "[TL] Epoch 185 of 500 took 9.570946s\n",
            "[TL]    val loss: 0.056492\n",
            "[TL]    val acc: 0.984400\n",
            "[TL] Epoch 190 of 500 took 9.496249s\n",
            "[TL]    val loss: 0.056959\n",
            "[TL]    val acc: 0.984600\n",
            "[TL] Epoch 195 of 500 took 9.524553s\n",
            "[TL]    val loss: 0.055549\n",
            "[TL]    val acc: 0.985000\n",
            "[TL] Epoch 200 of 500 took 9.597140s\n",
            "[TL]    val loss: 0.056610\n",
            "[TL]    val acc: 0.984400\n",
            "[TL] Epoch 205 of 500 took 9.531467s\n",
            "[TL]    val loss: 0.055963\n",
            "[TL]    val acc: 0.984800\n",
            "[TL] Epoch 210 of 500 took 9.515894s\n",
            "[TL]    val loss: 0.055866\n",
            "[TL]    val acc: 0.984800\n",
            "[TL] Epoch 215 of 500 took 9.514131s\n",
            "[TL]    val loss: 0.055318\n",
            "[TL]    val acc: 0.984500\n",
            "[TL] Epoch 220 of 500 took 9.523651s\n",
            "[TL]    val loss: 0.055461\n",
            "[TL]    val acc: 0.984900\n",
            "[TL] Epoch 225 of 500 took 9.564536s\n",
            "[TL]    val loss: 0.055619\n",
            "[TL]    val acc: 0.984800\n",
            "[TL] Epoch 230 of 500 took 9.537370s\n",
            "[TL]    val loss: 0.055043\n",
            "[TL]    val acc: 0.985200\n",
            "[TL] Epoch 235 of 500 took 9.605176s\n",
            "[TL]    val loss: 0.055266\n",
            "[TL]    val acc: 0.984700\n",
            "[TL] Epoch 240 of 500 took 9.947517s\n",
            "[TL]    val loss: 0.054161\n",
            "[TL]    val acc: 0.985300\n",
            "[TL] Epoch 245 of 500 took 9.467617s\n",
            "[TL]    val loss: 0.054699\n",
            "[TL]    val acc: 0.984800\n",
            "[TL] Epoch 250 of 500 took 9.550740s\n",
            "[TL]    val loss: 0.053781\n",
            "[TL]    val acc: 0.985100\n",
            "[TL] Epoch 255 of 500 took 9.508842s\n",
            "[TL]    val loss: 0.054564\n",
            "[TL]    val acc: 0.985700\n",
            "[TL] Epoch 260 of 500 took 9.506545s\n",
            "[TL]    val loss: 0.054992\n",
            "[TL]    val acc: 0.984900\n",
            "[TL] Epoch 265 of 500 took 9.468003s\n",
            "[TL]    val loss: 0.055088\n",
            "[TL]    val acc: 0.985000\n",
            "[TL] Epoch 270 of 500 took 9.521571s\n",
            "[TL]    val loss: 0.054206\n",
            "[TL]    val acc: 0.985300\n",
            "[TL] Epoch 275 of 500 took 9.484054s\n",
            "[TL]    val loss: 0.055422\n",
            "[TL]    val acc: 0.985500\n",
            "[TL] Epoch 280 of 500 took 9.608136s\n",
            "[TL]    val loss: 0.054362\n",
            "[TL]    val acc: 0.986100\n",
            "[TL] Epoch 285 of 500 took 9.501990s\n",
            "[TL]    val loss: 0.054716\n",
            "[TL]    val acc: 0.985300\n",
            "[TL] Epoch 290 of 500 took 9.475052s\n",
            "[TL]    val loss: 0.055541\n",
            "[TL]    val acc: 0.985100\n",
            "[TL] Epoch 295 of 500 took 9.614093s\n",
            "[TL]    val loss: 0.055501\n",
            "[TL]    val acc: 0.986100\n",
            "[TL] Epoch 300 of 500 took 9.509364s\n",
            "[TL]    val loss: 0.055486\n",
            "[TL]    val acc: 0.985900\n",
            "[TL] Epoch 305 of 500 took 9.617074s\n",
            "[TL]    val loss: 0.055182\n",
            "[TL]    val acc: 0.985600\n",
            "[TL] Epoch 310 of 500 took 9.504499s\n",
            "[TL]    val loss: 0.056369\n",
            "[TL]    val acc: 0.985200\n",
            "[TL] Epoch 315 of 500 took 9.503326s\n",
            "[TL]    val loss: 0.055011\n",
            "[TL]    val acc: 0.985400\n",
            "[TL] Epoch 320 of 500 took 9.527377s\n",
            "[TL]    val loss: 0.054452\n",
            "[TL]    val acc: 0.985600\n",
            "[TL] Epoch 325 of 500 took 9.481635s\n",
            "[TL]    val loss: 0.054619\n",
            "[TL]    val acc: 0.985600\n",
            "[TL] Epoch 330 of 500 took 9.529997s\n",
            "[TL]    val loss: 0.055474\n",
            "[TL]    val acc: 0.986000\n",
            "[TL] Epoch 335 of 500 took 9.636274s\n",
            "[TL]    val loss: 0.056022\n",
            "[TL]    val acc: 0.985600\n",
            "[TL] Epoch 340 of 500 took 9.557464s\n",
            "[TL]    val loss: 0.055067\n",
            "[TL]    val acc: 0.985900\n",
            "[TL] Epoch 345 of 500 took 9.548420s\n",
            "[TL]    val loss: 0.056079\n",
            "[TL]    val acc: 0.986000\n",
            "[TL] Epoch 350 of 500 took 9.428876s\n",
            "[TL]    val loss: 0.054884\n",
            "[TL]    val acc: 0.986300\n",
            "[TL] Epoch 355 of 500 took 9.529407s\n",
            "[TL]    val loss: 0.055162\n",
            "[TL]    val acc: 0.986700\n",
            "[TL] Epoch 360 of 500 took 9.538515s\n",
            "[TL]    val loss: 0.054352\n",
            "[TL]    val acc: 0.986100\n",
            "[TL] Epoch 365 of 500 took 9.485439s\n",
            "[TL]    val loss: 0.056428\n",
            "[TL]    val acc: 0.985600\n",
            "[TL] Epoch 370 of 500 took 9.482942s\n",
            "[TL]    val loss: 0.055279\n",
            "[TL]    val acc: 0.986300\n",
            "[TL] Epoch 375 of 500 took 9.488233s\n",
            "[TL]    val loss: 0.056321\n",
            "[TL]    val acc: 0.986300\n",
            "[TL] Epoch 380 of 500 took 9.422866s\n",
            "[TL]    val loss: 0.056373\n",
            "[TL]    val acc: 0.986200\n",
            "[TL] Epoch 385 of 500 took 9.483933s\n",
            "[TL]    val loss: 0.055855\n",
            "[TL]    val acc: 0.986000\n",
            "[TL] Epoch 390 of 500 took 9.514391s\n",
            "[TL]    val loss: 0.054821\n",
            "[TL]    val acc: 0.985900\n",
            "[TL] Epoch 395 of 500 took 9.528985s\n",
            "[TL]    val loss: 0.056287\n",
            "[TL]    val acc: 0.985900\n",
            "[TL] Epoch 400 of 500 took 9.808451s\n",
            "[TL]    val loss: 0.055403\n",
            "[TL]    val acc: 0.986500\n",
            "[TL] Epoch 405 of 500 took 9.526878s\n",
            "[TL]    val loss: 0.056705\n",
            "[TL]    val acc: 0.986100\n",
            "[TL] Epoch 410 of 500 took 9.565817s\n",
            "[TL]    val loss: 0.057029\n",
            "[TL]    val acc: 0.985600\n",
            "[TL] Epoch 415 of 500 took 9.422239s\n",
            "[TL]    val loss: 0.057035\n",
            "[TL]    val acc: 0.986300\n",
            "[TL] Epoch 420 of 500 took 9.452715s\n",
            "[TL]    val loss: 0.055956\n",
            "[TL]    val acc: 0.986100\n",
            "[TL] Epoch 425 of 500 took 9.487434s\n",
            "[TL]    val loss: 0.056488\n",
            "[TL]    val acc: 0.986200\n",
            "[TL] Epoch 430 of 500 took 9.526770s\n",
            "[TL]    val loss: 0.055456\n",
            "[TL]    val acc: 0.986400\n",
            "[TL] Epoch 435 of 500 took 9.535824s\n",
            "[TL]    val loss: 0.056105\n",
            "[TL]    val acc: 0.985300\n",
            "[TL] Epoch 440 of 500 took 9.491559s\n",
            "[TL]    val loss: 0.058140\n",
            "[TL]    val acc: 0.986800\n",
            "[TL] Epoch 445 of 500 took 9.515768s\n",
            "[TL]    val loss: 0.055526\n",
            "[TL]    val acc: 0.985900\n",
            "[TL] Epoch 450 of 500 took 9.568411s\n",
            "[TL]    val loss: 0.056243\n",
            "[TL]    val acc: 0.986400\n",
            "[TL] Epoch 455 of 500 took 9.669399s\n",
            "[TL]    val loss: 0.055596\n",
            "[TL]    val acc: 0.986800\n",
            "[TL] Epoch 460 of 500 took 9.585413s\n",
            "[TL]    val loss: 0.056253\n",
            "[TL]    val acc: 0.986400\n",
            "[TL] Epoch 465 of 500 took 9.816337s\n",
            "[TL]    val loss: 0.056116\n",
            "[TL]    val acc: 0.986400\n",
            "[TL] Epoch 470 of 500 took 9.518415s\n",
            "[TL]    val loss: 0.057325\n",
            "[TL]    val acc: 0.985700\n",
            "[TL] Epoch 475 of 500 took 9.534283s\n",
            "[TL]    val loss: 0.057194\n",
            "[TL]    val acc: 0.985800\n",
            "[TL] Epoch 480 of 500 took 9.666443s\n",
            "[TL]    val loss: 0.057000\n",
            "[TL]    val acc: 0.986300\n",
            "[TL] Epoch 485 of 500 took 9.536279s\n",
            "[TL]    val loss: 0.056808\n",
            "[TL]    val acc: 0.986200\n",
            "[TL] Epoch 490 of 500 took 9.577079s\n",
            "[TL]    val loss: 0.056962\n",
            "[TL]    val acc: 0.985800\n",
            "[TL] Epoch 495 of 500 took 9.507922s\n",
            "[TL]    val loss: 0.057271\n",
            "[TL]    val acc: 0.986400\n",
            "[TL] Epoch 500 of 500 took 9.632677s\n",
            "[TL]    val loss: 0.058276\n",
            "[TL]    val acc: 0.985700\n",
            "[TL] Total training time: 4843.882022s\n",
            "[TL] Start testing the network ...\n",
            "[TL]    test loss: 0.048069\n",
            "[TL]    test acc: 0.987600\n",
            "[TL] [*] model.npz saved\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}