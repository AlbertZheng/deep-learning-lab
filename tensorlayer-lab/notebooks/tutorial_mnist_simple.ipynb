{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tutorial_mnist_simple.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/AlbertZheng/deep-learning-lab/blob/master/tensorlayer-lab/notebooks/tutorial_mnist_simple.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "gQW-n1n8jS88",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6293
        },
        "outputId": "c4108837-b861-4d19-e497-15db542019f7"
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorlayer\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorlayer as tl\n",
        "\n",
        "# Confirm TensorFlow can see the GPU on Colaboratory\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "# Testing matrix multiply using GPU\n",
        "#sess = tf.InteractiveSession()\n",
        "\n",
        "#a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
        "#b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
        "#c = tf.matmul(a, b)\n",
        "# Runs the op.\n",
        "#print(sess.run(c))\n",
        "\n",
        "#sess.close()\n",
        "\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.DEBUG)\n",
        "tl.logging.set_verbosity(tl.logging.DEBUG)\n",
        "\n",
        "#sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
        "sess = tf.InteractiveSession()\n",
        "\n",
        "# prepare data\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = tl.files.load_mnist_dataset(shape=(-1, 784))\n",
        "# define placeholder\n",
        "x = tf.placeholder(tf.float32, shape=[None, 784], name='x')\n",
        "y_ = tf.placeholder(tf.int64, shape=[None], name='y_')\n",
        "\n",
        "# define the network\n",
        "network = tl.layers.InputLayer(x, name='input')\n",
        "network = tl.layers.DropoutLayer(network, keep=0.8, name='drop1')\n",
        "network = tl.layers.DenseLayer(network, 800, tf.nn.relu, name='relu1')\n",
        "network = tl.layers.DropoutLayer(network, keep=0.5, name='drop2')\n",
        "network = tl.layers.DenseLayer(network, 800, tf.nn.relu, name='relu2')\n",
        "network = tl.layers.DropoutLayer(network, keep=0.5, name='drop3')\n",
        "# the softmax is implemented internally in tl.cost.cross_entropy(y, y_) to\n",
        "# speed up computation, so we use identity here.\n",
        "# see tf.nn.sparse_softmax_cross_entropy_with_logits()\n",
        "network = tl.layers.DenseLayer(network, n_units=10, act=None, name='output')\n",
        "\n",
        "# define cost function and metric.\n",
        "y = network.outputs\n",
        "cost = tl.cost.cross_entropy(y, y_, name='cost')\n",
        "correct_prediction = tf.equal(tf.argmax(y, 1), y_)\n",
        "acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "y_op = tf.argmax(tf.nn.softmax(y), 1)\n",
        "\n",
        "# define the optimizer\n",
        "train_params = network.all_params\n",
        "train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost, var_list=train_params)\n",
        "\n",
        "# initialize all variables in the session\n",
        "tl.layers.initialize_global_variables(sess)\n",
        "\n",
        "# print network information\n",
        "network.print_params()\n",
        "network.print_layers()\n",
        "\n",
        "# train the network\n",
        "tl.utils.fit(sess, network, train_op, cost, X_train, y_train, x, y_, acc=acc, batch_size=500, \\\n",
        "    n_epoch=500, print_freq=5, X_val=X_val, y_val=y_val, eval_train=False)\n",
        "\n",
        "# evaluation\n",
        "tl.utils.test(sess, network, acc, X_test, y_test, x, y_, batch_size=None, cost=cost)\n",
        "\n",
        "# save the network to .npz file\n",
        "tl.files.save_npz(network.all_params, name='model.npz')\n",
        "\n",
        "sess.close()\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorlayer in /usr/local/lib/python3.6/dist-packages (1.9.1)\n",
            "Requirement already satisfied: numpy<1.15,>=1.14 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (1.14.5)\n",
            "Requirement already satisfied: wrapt<1.11,>=1.10 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (1.10.11)\n",
            "Requirement already satisfied: lxml<4.3,>=4.2 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (4.2.4)\n",
            "Requirement already satisfied: scikit-image<0.15,>=0.14 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (0.14.0)\n",
            "Requirement already satisfied: matplotlib<2.3,>=2.2 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (2.2.3)\n",
            "Requirement already satisfied: progressbar2<3.39,>=3.38 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (3.38.0)\n",
            "Requirement already satisfied: imageio<2.4,>=2.3 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (2.3.0)\n",
            "Requirement already satisfied: scipy<1.2,>=1.1 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (1.1.0)\n",
            "Requirement already satisfied: tqdm<4.24,>=4.23 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (4.23.4)\n",
            "Requirement already satisfied: requests<2.20,>=2.19 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (2.19.1)\n",
            "Requirement already satisfied: scikit-learn<0.20,>=0.19 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (0.19.2)\n",
            "Requirement already satisfied: networkx>=1.8 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15,>=0.14->tensorlayer) (2.1)\n",
            "Requirement already satisfied: dask[array]>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15,>=0.14->tensorlayer) (0.18.2)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15,>=0.14->tensorlayer) (0.5.2)\n",
            "Requirement already satisfied: cloudpickle>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15,>=0.14->tensorlayer) (0.5.3)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15,>=0.14->tensorlayer) (5.2.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15,>=0.14->tensorlayer) (1.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib<2.3,>=2.2->tensorlayer) (2.5.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib<2.3,>=2.2->tensorlayer) (0.10.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from matplotlib<2.3,>=2.2->tensorlayer) (2018.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib<2.3,>=2.2->tensorlayer) (2.2.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib<2.3,>=2.2->tensorlayer) (1.0.1)\n",
            "Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2<3.39,>=3.38->tensorlayer) (2.3.0)\n",
            "Requirement already satisfied: idna<2.8,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<2.20,>=2.19->tensorlayer) (2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<2.20,>=2.19->tensorlayer) (2018.8.13)\n",
            "Requirement already satisfied: urllib3<1.24,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<2.20,>=2.19->tensorlayer) (1.22)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<2.20,>=2.19->tensorlayer) (3.0.4)\n",
            "Requirement already satisfied: decorator>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=1.8->scikit-image<0.15,>=0.14->tensorlayer) (4.3.0)\n",
            "Requirement already satisfied: toolz>=0.7.3; extra == \"array\" in /usr/local/lib/python3.6/dist-packages (from dask[array]>=0.9.0->scikit-image<0.15,>=0.14->tensorlayer) (0.9.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib<2.3,>=2.2->tensorlayer) (39.1.0)\n",
            "Found GPU at: /device:GPU:0\n",
            "[TL] Load or Download MNIST > data/mnist\n",
            "[TL] data/mnist/train-images-idx3-ubyte.gz\n",
            "[TL] data/mnist/t10k-images-idx3-ubyte.gz\n",
            "[TL] InputLayer  input: (?, 784)\n",
            "[TL] DropoutLayer drop1: keep: 0.800000 is_fix: False\n",
            "[TL] DenseLayer  relu1: 800 relu\n",
            "[TL] DropoutLayer drop2: keep: 0.500000 is_fix: False\n",
            "[TL] DenseLayer  relu2: 800 relu\n",
            "[TL] DropoutLayer drop3: keep: 0.500000 is_fix: False\n",
            "[TL] DenseLayer  output: 10 No Activation\n",
            "[TL] WARNING: From <ipython-input-3-22cf4562ec00>:60: initialize_global_variables (from tensorlayer.layers.utils) is deprecated and will be removed after 2018-09-30.\n",
            "Instructions for updating: This API is deprecated in favor of `tf.global_variables_initializer`\n",
            "\n",
            "[TL]   param   0: relu1/W:0            (784, 800)         float32_ref (mean: 0.0001742555177770555, median: 0.0002281174820382148, std: 0.08794597536325455)   \n",
            "[TL]   param   1: relu1/b:0            (800,)             float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
            "[TL]   param   2: relu2/W:0            (800, 800)         float32_ref (mean: -6.874206883367151e-05, median: -3.28694841300603e-05, std: 0.08791900426149368)   \n",
            "[TL]   param   3: relu2/b:0            (800,)             float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
            "[TL]   param   4: output/W:0           (800, 10)          float32_ref (mean: -0.000570310337934643, median: -0.0010456908494234085, std: 0.08875949680805206)   \n",
            "[TL]   param   5: output/b:0           (10,)              float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
            "[TL]   num of params: 1276810\n",
            "[TL]   layer   0: x:0                  (?, 784)           float32\n",
            "[TL]   layer   1: drop1/mul:0          (?, 784)           float32\n",
            "[TL]   layer   2: relu1/Relu:0         (?, 800)           float32\n",
            "[TL]   layer   3: drop2/mul:0          (?, 800)           float32\n",
            "[TL]   layer   4: relu2/Relu:0         (?, 800)           float32\n",
            "[TL]   layer   5: drop3/mul:0          (?, 800)           float32\n",
            "[TL]   layer   6: output/bias_add:0    (?, 10)            float32\n",
            "[TL] Start training the network ...\n",
            "[TL] Epoch 1 of 500 took 1.493325s\n",
            "[TL]    val loss: 0.545283\n",
            "[TL]    val acc: 0.825700\n",
            "[TL] Epoch 5 of 500 took 0.958070s\n",
            "[TL]    val loss: 0.289557\n",
            "[TL]    val acc: 0.914400\n",
            "[TL] Epoch 10 of 500 took 1.287524s\n",
            "[TL]    val loss: 0.223334\n",
            "[TL]    val acc: 0.938200\n",
            "[TL] Epoch 15 of 500 took 1.329031s\n",
            "[TL]    val loss: 0.189116\n",
            "[TL]    val acc: 0.948200\n",
            "[TL] Epoch 20 of 500 took 0.994651s\n",
            "[TL]    val loss: 0.162881\n",
            "[TL]    val acc: 0.955800\n",
            "[TL] Epoch 25 of 500 took 1.125811s\n",
            "[TL]    val loss: 0.145788\n",
            "[TL]    val acc: 0.959700\n",
            "[TL] Epoch 30 of 500 took 1.013118s\n",
            "[TL]    val loss: 0.132188\n",
            "[TL]    val acc: 0.965000\n",
            "[TL] Epoch 35 of 500 took 1.004443s\n",
            "[TL]    val loss: 0.120328\n",
            "[TL]    val acc: 0.967700\n",
            "[TL] Epoch 40 of 500 took 0.979478s\n",
            "[TL]    val loss: 0.112298\n",
            "[TL]    val acc: 0.969200\n",
            "[TL] Epoch 45 of 500 took 0.958795s\n",
            "[TL]    val loss: 0.105677\n",
            "[TL]    val acc: 0.970700\n",
            "[TL] Epoch 50 of 500 took 0.942721s\n",
            "[TL]    val loss: 0.099177\n",
            "[TL]    val acc: 0.971600\n",
            "[TL] Epoch 55 of 500 took 1.313692s\n",
            "[TL]    val loss: 0.093165\n",
            "[TL]    val acc: 0.974000\n",
            "[TL] Epoch 60 of 500 took 1.321131s\n",
            "[TL]    val loss: 0.090184\n",
            "[TL]    val acc: 0.974400\n",
            "[TL] Epoch 65 of 500 took 0.991733s\n",
            "[TL]    val loss: 0.085585\n",
            "[TL]    val acc: 0.975100\n",
            "[TL] Epoch 70 of 500 took 1.026922s\n",
            "[TL]    val loss: 0.082940\n",
            "[TL]    val acc: 0.976800\n",
            "[TL] Epoch 75 of 500 took 1.116644s\n",
            "[TL]    val loss: 0.079460\n",
            "[TL]    val acc: 0.977200\n",
            "[TL] Epoch 80 of 500 took 0.993888s\n",
            "[TL]    val loss: 0.077449\n",
            "[TL]    val acc: 0.977700\n",
            "[TL] Epoch 85 of 500 took 1.006927s\n",
            "[TL]    val loss: 0.074793\n",
            "[TL]    val acc: 0.978500\n",
            "[TL] Epoch 90 of 500 took 1.012538s\n",
            "[TL]    val loss: 0.071794\n",
            "[TL]    val acc: 0.978700\n",
            "[TL] Epoch 95 of 500 took 0.980954s\n",
            "[TL]    val loss: 0.069461\n",
            "[TL]    val acc: 0.979500\n",
            "[TL] Epoch 100 of 500 took 1.185700s\n",
            "[TL]    val loss: 0.068163\n",
            "[TL]    val acc: 0.980500\n",
            "[TL] Epoch 105 of 500 took 1.002617s\n",
            "[TL]    val loss: 0.066595\n",
            "[TL]    val acc: 0.980600\n",
            "[TL] Epoch 110 of 500 took 1.053000s\n",
            "[TL]    val loss: 0.065968\n",
            "[TL]    val acc: 0.980800\n",
            "[TL] Epoch 115 of 500 took 1.043479s\n",
            "[TL]    val loss: 0.065386\n",
            "[TL]    val acc: 0.980700\n",
            "[TL] Epoch 120 of 500 took 1.020889s\n",
            "[TL]    val loss: 0.064634\n",
            "[TL]    val acc: 0.981000\n",
            "[TL] Epoch 125 of 500 took 0.918637s\n",
            "[TL]    val loss: 0.062452\n",
            "[TL]    val acc: 0.982400\n",
            "[TL] Epoch 130 of 500 took 0.913416s\n",
            "[TL]    val loss: 0.061227\n",
            "[TL]    val acc: 0.982200\n",
            "[TL] Epoch 135 of 500 took 0.912307s\n",
            "[TL]    val loss: 0.060616\n",
            "[TL]    val acc: 0.982100\n",
            "[TL] Epoch 140 of 500 took 0.953065s\n",
            "[TL]    val loss: 0.059851\n",
            "[TL]    val acc: 0.983100\n",
            "[TL] Epoch 145 of 500 took 0.940917s\n",
            "[TL]    val loss: 0.059070\n",
            "[TL]    val acc: 0.983600\n",
            "[TL] Epoch 150 of 500 took 1.168780s\n",
            "[TL]    val loss: 0.057591\n",
            "[TL]    val acc: 0.983300\n",
            "[TL] Epoch 155 of 500 took 1.092881s\n",
            "[TL]    val loss: 0.058805\n",
            "[TL]    val acc: 0.983700\n",
            "[TL] Epoch 160 of 500 took 1.062466s\n",
            "[TL]    val loss: 0.057465\n",
            "[TL]    val acc: 0.983700\n",
            "[TL] Epoch 165 of 500 took 1.210207s\n",
            "[TL]    val loss: 0.056910\n",
            "[TL]    val acc: 0.984400\n",
            "[TL] Epoch 170 of 500 took 1.096900s\n",
            "[TL]    val loss: 0.056879\n",
            "[TL]    val acc: 0.984100\n",
            "[TL] Epoch 175 of 500 took 1.026894s\n",
            "[TL]    val loss: 0.056777\n",
            "[TL]    val acc: 0.983600\n",
            "[TL] Epoch 180 of 500 took 1.018838s\n",
            "[TL]    val loss: 0.056723\n",
            "[TL]    val acc: 0.983700\n",
            "[TL] Epoch 185 of 500 took 0.951945s\n",
            "[TL]    val loss: 0.055004\n",
            "[TL]    val acc: 0.984400\n",
            "[TL] Epoch 190 of 500 took 0.947455s\n",
            "[TL]    val loss: 0.055173\n",
            "[TL]    val acc: 0.984700\n",
            "[TL] Epoch 195 of 500 took 0.939623s\n",
            "[TL]    val loss: 0.055830\n",
            "[TL]    val acc: 0.983800\n",
            "[TL] Epoch 200 of 500 took 0.939664s\n",
            "[TL]    val loss: 0.053720\n",
            "[TL]    val acc: 0.984300\n",
            "[TL] Epoch 205 of 500 took 0.968281s\n",
            "[TL]    val loss: 0.054418\n",
            "[TL]    val acc: 0.984600\n",
            "[TL] Epoch 210 of 500 took 1.062834s\n",
            "[TL]    val loss: 0.053514\n",
            "[TL]    val acc: 0.985200\n",
            "[TL] Epoch 215 of 500 took 0.965425s\n",
            "[TL]    val loss: 0.053398\n",
            "[TL]    val acc: 0.985100\n",
            "[TL] Epoch 220 of 500 took 0.965963s\n",
            "[TL]    val loss: 0.053238\n",
            "[TL]    val acc: 0.985100\n",
            "[TL] Epoch 225 of 500 took 1.086206s\n",
            "[TL]    val loss: 0.052591\n",
            "[TL]    val acc: 0.985200\n",
            "[TL] Epoch 230 of 500 took 0.963417s\n",
            "[TL]    val loss: 0.053721\n",
            "[TL]    val acc: 0.985200\n",
            "[TL] Epoch 235 of 500 took 0.982419s\n",
            "[TL]    val loss: 0.052857\n",
            "[TL]    val acc: 0.985400\n",
            "[TL] Epoch 240 of 500 took 0.921047s\n",
            "[TL]    val loss: 0.053640\n",
            "[TL]    val acc: 0.984800\n",
            "[TL] Epoch 245 of 500 took 0.910055s\n",
            "[TL]    val loss: 0.053235\n",
            "[TL]    val acc: 0.985100\n",
            "[TL] Epoch 250 of 500 took 0.915372s\n",
            "[TL]    val loss: 0.053034\n",
            "[TL]    val acc: 0.985600\n",
            "[TL] Epoch 255 of 500 took 0.926819s\n",
            "[TL]    val loss: 0.054222\n",
            "[TL]    val acc: 0.985400\n",
            "[TL] Epoch 260 of 500 took 0.951856s\n",
            "[TL]    val loss: 0.053004\n",
            "[TL]    val acc: 0.985500\n",
            "[TL] Epoch 265 of 500 took 1.341538s\n",
            "[TL]    val loss: 0.053042\n",
            "[TL]    val acc: 0.985800\n",
            "[TL] Epoch 270 of 500 took 0.992370s\n",
            "[TL]    val loss: 0.053495\n",
            "[TL]    val acc: 0.985400\n",
            "[TL] Epoch 275 of 500 took 1.131766s\n",
            "[TL]    val loss: 0.052996\n",
            "[TL]    val acc: 0.985900\n",
            "[TL] Epoch 280 of 500 took 0.953162s\n",
            "[TL]    val loss: 0.052994\n",
            "[TL]    val acc: 0.985000\n",
            "[TL] Epoch 285 of 500 took 0.971247s\n",
            "[TL]    val loss: 0.051670\n",
            "[TL]    val acc: 0.985900\n",
            "[TL] Epoch 290 of 500 took 0.989321s\n",
            "[TL]    val loss: 0.053166\n",
            "[TL]    val acc: 0.986400\n",
            "[TL] Epoch 295 of 500 took 1.005360s\n",
            "[TL]    val loss: 0.051970\n",
            "[TL]    val acc: 0.986500\n",
            "[TL] Epoch 300 of 500 took 0.933295s\n",
            "[TL]    val loss: 0.052781\n",
            "[TL]    val acc: 0.986400\n",
            "[TL] Epoch 305 of 500 took 0.956926s\n",
            "[TL]    val loss: 0.052743\n",
            "[TL]    val acc: 0.985900\n",
            "[TL] Epoch 310 of 500 took 0.948664s\n",
            "[TL]    val loss: 0.051822\n",
            "[TL]    val acc: 0.985900\n",
            "[TL] Epoch 315 of 500 took 1.336170s\n",
            "[TL]    val loss: 0.052346\n",
            "[TL]    val acc: 0.986000\n",
            "[TL] Epoch 320 of 500 took 1.432804s\n",
            "[TL]    val loss: 0.052868\n",
            "[TL]    val acc: 0.986400\n",
            "[TL] Epoch 325 of 500 took 1.024362s\n",
            "[TL]    val loss: 0.052195\n",
            "[TL]    val acc: 0.986600\n",
            "[TL] Epoch 330 of 500 took 1.091607s\n",
            "[TL]    val loss: 0.052116\n",
            "[TL]    val acc: 0.986700\n",
            "[TL] Epoch 335 of 500 took 0.986635s\n",
            "[TL]    val loss: 0.052130\n",
            "[TL]    val acc: 0.986200\n",
            "[TL] Epoch 340 of 500 took 1.104414s\n",
            "[TL]    val loss: 0.054011\n",
            "[TL]    val acc: 0.986400\n",
            "[TL] Epoch 345 of 500 took 0.983428s\n",
            "[TL]    val loss: 0.052369\n",
            "[TL]    val acc: 0.986600\n",
            "[TL] Epoch 350 of 500 took 0.960381s\n",
            "[TL]    val loss: 0.053394\n",
            "[TL]    val acc: 0.985700\n",
            "[TL] Epoch 355 of 500 took 0.956802s\n",
            "[TL]    val loss: 0.052896\n",
            "[TL]    val acc: 0.986200\n",
            "[TL] Epoch 360 of 500 took 0.922157s\n",
            "[TL]    val loss: 0.053196\n",
            "[TL]    val acc: 0.986500\n",
            "[TL] Epoch 365 of 500 took 1.301841s\n",
            "[TL]    val loss: 0.053717\n",
            "[TL]    val acc: 0.985800\n",
            "[TL] Epoch 370 of 500 took 1.349011s\n",
            "[TL]    val loss: 0.052604\n",
            "[TL]    val acc: 0.986100\n",
            "[TL] Epoch 375 of 500 took 1.008518s\n",
            "[TL]    val loss: 0.051934\n",
            "[TL]    val acc: 0.986600\n",
            "[TL] Epoch 380 of 500 took 1.156641s\n",
            "[TL]    val loss: 0.052624\n",
            "[TL]    val acc: 0.985600\n",
            "[TL] Epoch 385 of 500 took 0.923491s\n",
            "[TL]    val loss: 0.053198\n",
            "[TL]    val acc: 0.986000\n",
            "[TL] Epoch 390 of 500 took 1.003614s\n",
            "[TL]    val loss: 0.054326\n",
            "[TL]    val acc: 0.986000\n",
            "[TL] Epoch 395 of 500 took 0.925244s\n",
            "[TL]    val loss: 0.053870\n",
            "[TL]    val acc: 0.985500\n",
            "[TL] Epoch 400 of 500 took 0.975761s\n",
            "[TL]    val loss: 0.052526\n",
            "[TL]    val acc: 0.986200\n",
            "[TL] Epoch 405 of 500 took 0.972085s\n",
            "[TL]    val loss: 0.053333\n",
            "[TL]    val acc: 0.985700\n",
            "[TL] Epoch 410 of 500 took 1.096000s\n",
            "[TL]    val loss: 0.054178\n",
            "[TL]    val acc: 0.986000\n",
            "[TL] Epoch 415 of 500 took 0.996980s\n",
            "[TL]    val loss: 0.053693\n",
            "[TL]    val acc: 0.986000\n",
            "[TL] Epoch 420 of 500 took 0.936522s\n",
            "[TL]    val loss: 0.052945\n",
            "[TL]    val acc: 0.986400\n",
            "[TL] Epoch 425 of 500 took 1.478422s\n",
            "[TL]    val loss: 0.052136\n",
            "[TL]    val acc: 0.986700\n",
            "[TL] Epoch 430 of 500 took 1.024814s\n",
            "[TL]    val loss: 0.053539\n",
            "[TL]    val acc: 0.985800\n",
            "[TL] Epoch 435 of 500 took 1.043894s\n",
            "[TL]    val loss: 0.053397\n",
            "[TL]    val acc: 0.986200\n",
            "[TL] Epoch 440 of 500 took 1.005590s\n",
            "[TL]    val loss: 0.053386\n",
            "[TL]    val acc: 0.987100\n",
            "[TL] Epoch 445 of 500 took 1.036995s\n",
            "[TL]    val loss: 0.053643\n",
            "[TL]    val acc: 0.986400\n",
            "[TL] Epoch 450 of 500 took 0.940000s\n",
            "[TL]    val loss: 0.053758\n",
            "[TL]    val acc: 0.986400\n",
            "[TL] Epoch 455 of 500 took 0.957848s\n",
            "[TL]    val loss: 0.053936\n",
            "[TL]    val acc: 0.986300\n",
            "[TL] Epoch 460 of 500 took 0.948961s\n",
            "[TL]    val loss: 0.054430\n",
            "[TL]    val acc: 0.986100\n",
            "[TL] Epoch 465 of 500 took 0.950399s\n",
            "[TL]    val loss: 0.054079\n",
            "[TL]    val acc: 0.986600\n",
            "[TL] Epoch 470 of 500 took 1.240623s\n",
            "[TL]    val loss: 0.053563\n",
            "[TL]    val acc: 0.986500\n",
            "[TL] Epoch 475 of 500 took 1.507638s\n",
            "[TL]    val loss: 0.054491\n",
            "[TL]    val acc: 0.986200\n",
            "[TL] Epoch 480 of 500 took 0.994807s\n",
            "[TL]    val loss: 0.054257\n",
            "[TL]    val acc: 0.986500\n",
            "[TL] Epoch 485 of 500 took 1.032671s\n",
            "[TL]    val loss: 0.052922\n",
            "[TL]    val acc: 0.986700\n",
            "[TL] Epoch 490 of 500 took 1.021066s\n",
            "[TL]    val loss: 0.053675\n",
            "[TL]    val acc: 0.986700\n",
            "[TL] Epoch 495 of 500 took 0.974174s\n",
            "[TL]    val loss: 0.053488\n",
            "[TL]    val acc: 0.986600\n",
            "[TL] Epoch 500 of 500 took 0.988865s\n",
            "[TL]    val loss: 0.054500\n",
            "[TL]    val acc: 0.986800\n",
            "[TL] Total training time: 532.307045s\n",
            "[TL] Start testing the network ...\n",
            "[TL]    test loss: 0.046915\n",
            "[TL]    test acc: 0.987700\n",
            "[TL] [*] model.npz saved\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}