{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tutorial_cifar10.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/AlbertZheng/deep-learning-lab/blob/master/tensorlayer/notebooks/tutorial_cifar10.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "SrrrsgNnj6J-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2587
        },
        "outputId": "75370211-279a-46a9-d704-32279fc6daa5"
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorlayer\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorlayer as tl\n",
        "from tensorlayer.layers import *\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.DEBUG)\n",
        "tl.logging.set_verbosity(tl.logging.DEBUG)\n",
        "\n",
        "sess = tf.InteractiveSession()\n",
        "\n",
        "X_train, y_train, X_test, y_test = tl.files.load_cifar10_dataset(shape=(-1, 32, 32, 3), plotable=False)\n",
        "\n",
        "\n",
        "def model(x, y_, reuse):\n",
        "    W_init = tf.truncated_normal_initializer(stddev=5e-2)\n",
        "    W_init2 = tf.truncated_normal_initializer(stddev=0.04)\n",
        "    b_init2 = tf.constant_initializer(value=0.1)\n",
        "    with tf.variable_scope(\"model\", reuse=reuse):\n",
        "        net = InputLayer(x, name='input')\n",
        "        net = Conv2d(net, 64, (5, 5), (1, 1), act=tf.nn.relu, padding='SAME', W_init=W_init, name='cnn1')\n",
        "        net = MaxPool2d(net, (3, 3), (2, 2), padding='SAME', name='pool1')\n",
        "        net = LocalResponseNormLayer(net, depth_radius=4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm1')\n",
        "\n",
        "        net = Conv2d(net, 64, (5, 5), (1, 1), act=tf.nn.relu, padding='SAME', W_init=W_init, name='cnn2')\n",
        "        net = LocalResponseNormLayer(net, depth_radius=4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm2')\n",
        "        net = MaxPool2d(net, (3, 3), (2, 2), padding='SAME', name='pool2')\n",
        "\n",
        "        net = FlattenLayer(net, name='flatten')\n",
        "        net = DenseLayer(net, 384, act=tf.nn.relu, W_init=W_init2, b_init=b_init2, name='d1relu')\n",
        "        net = DenseLayer(net, 192, act=tf.nn.relu, W_init=W_init2, b_init=b_init2, name='d2relu')\n",
        "        net = DenseLayer(net, 10, act=None, W_init=W_init2, name='output')\n",
        "        y = net.outputs\n",
        "\n",
        "        ce = tl.cost.cross_entropy(y, y_, name='cost')\n",
        "        # L2 for the MLP, without this, the accuracy will be reduced by 15%.\n",
        "        L2 = 0\n",
        "        for p in tl.layers.get_variables_with_name('relu/W', True, True):\n",
        "            L2 += tf.contrib.layers.l2_regularizer(0.004)(p)\n",
        "        cost = ce + L2\n",
        "\n",
        "        correct_prediction = tf.equal(tf.argmax(y, 1), y_)\n",
        "        acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "        return net, cost, acc\n",
        "\n",
        "\n",
        "def model_batch_norm(x, y_, reuse, is_train):\n",
        "    \"\"\"Batch normalization should be placed before rectifier.\"\"\"\n",
        "    W_init = tf.truncated_normal_initializer(stddev=5e-2)\n",
        "    W_init2 = tf.truncated_normal_initializer(stddev=0.04)\n",
        "    b_init2 = tf.constant_initializer(value=0.1)\n",
        "    with tf.variable_scope(\"model\", reuse=reuse):\n",
        "        net = InputLayer(x, name='input')\n",
        "        net = Conv2d(net, 64, (5, 5), (1, 1), padding='SAME', W_init=W_init, b_init=None, name='cnn1')\n",
        "        net = BatchNormLayer(net, is_train, act=tf.nn.relu, name='batch1')\n",
        "        net = MaxPool2d(net, (3, 3), (2, 2), padding='SAME', name='pool1')\n",
        "\n",
        "        net = Conv2d(net, 64, (5, 5), (1, 1), padding='SAME', W_init=W_init, b_init=None, name='cnn2')\n",
        "        net = BatchNormLayer(net, is_train, act=tf.nn.relu, name='batch2')\n",
        "        net = MaxPool2d(net, (3, 3), (2, 2), padding='SAME', name='pool2')\n",
        "\n",
        "        net = FlattenLayer(net, name='flatten')  # output: (batch_size, 2304)\n",
        "        net = DenseLayer(net, 384, act=tf.nn.relu, W_init=W_init2, b_init=b_init2, name='d1relu')\n",
        "        net = DenseLayer(net, 192, act=tf.nn.relu, W_init=W_init2, b_init=b_init2, name='d2relu')\n",
        "        net = DenseLayer(net, 10, act=None, W_init=W_init2, name='output')\n",
        "        y = net.outputs\n",
        "\n",
        "        ce = tl.cost.cross_entropy(y, y_, name='cost')\n",
        "        # L2 for the MLP, without this, the accuracy will be reduced by 15%.\n",
        "        L2 = 0\n",
        "        for p in tl.layers.get_variables_with_name('relu/W', True, True):\n",
        "            L2 += tf.contrib.layers.l2_regularizer(0.004)(p)\n",
        "        cost = ce + L2\n",
        "\n",
        "        correct_prediction = tf.equal(tf.argmax(y, 1), y_)\n",
        "        acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "        return net, cost, acc\n",
        "\n",
        "\n",
        "def distort_fn(x, is_train=False):\n",
        "    \"\"\"\n",
        "    The images are processed as follows:\n",
        "    .. They are cropped to 24 x 24 pixels, centrally for evaluation or randomly for training.\n",
        "    .. They are approximately whitened to make the model insensitive to dynamic range.\n",
        "    For training, we additionally apply a series of random distortions to\n",
        "    artificially increase the data set size:\n",
        "    .. Randomly flip the image from left to right.\n",
        "    .. Randomly distort the image brightness.\n",
        "    \"\"\"\n",
        "    # print('begin',x.shape, np.min(x), np.max(x))\n",
        "    x = tl.prepro.crop(x, 24, 24, is_random=is_train)\n",
        "    # print('after crop',x.shape, np.min(x), np.max(x))\n",
        "    if is_train:\n",
        "        # x = tl.prepro.zoom(x, zoom_range=(0.9, 1.0), is_random=True)\n",
        "        # print('after zoom', x.shape, np.min(x), np.max(x))\n",
        "        x = tl.prepro.flip_axis(x, axis=1, is_random=True)\n",
        "        # print('after flip',x.shape, np.min(x), np.max(x))\n",
        "        x = tl.prepro.brightness(x, gamma=0.1, gain=1, is_random=True)\n",
        "        # print('after brightness',x.shape, np.min(x), np.max(x))\n",
        "        # tmp = np.max(x)\n",
        "        # x += np.random.uniform(-20, 20)\n",
        "        # x /= tmp\n",
        "    # normalize the image\n",
        "    x = (x - np.mean(x)) / max(np.std(x), 1e-5)  # avoid values divided by 0\n",
        "    # print('after norm', x.shape, np.min(x), np.max(x), np.mean(x))\n",
        "    return x\n",
        "\n",
        "\n",
        "x = tf.placeholder(dtype=tf.float32, shape=[None, 24, 24, 3], name='x')\n",
        "y_ = tf.placeholder(dtype=tf.int64, shape=[None], name='y_')\n",
        "\n",
        "# using local response normalization\n",
        "# network, cost, _ = model(x, y_, False)\n",
        "# _, cost_test, acc = model(x, y_, True)\n",
        "# you may want to try batch normalization\n",
        "network, cost, _ = model_batch_norm(x, y_, False, is_train=True)\n",
        "_, cost_test, acc = model_batch_norm(x, y_, True, is_train=False)\n",
        "\n",
        "# train\n",
        "n_epoch = 50000\n",
        "learning_rate = 0.0001\n",
        "print_freq = 1\n",
        "batch_size = 128\n",
        "\n",
        "train_params = network.all_params\n",
        "train_op = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-08,\n",
        "                                  use_locking=False).minimize(cost, var_list=train_params)\n",
        "\n",
        "tl.layers.initialize_global_variables(sess)\n",
        "\n",
        "network.print_params(False)\n",
        "network.print_layers()\n",
        "\n",
        "print('   learning_rate: %f' % learning_rate)\n",
        "print('   batch_size: %d' % batch_size)\n",
        "\n",
        "for epoch in range(n_epoch):\n",
        "    start_time = time.time()\n",
        "    for X_train_a, y_train_a in tl.iterate.minibatches(X_train, y_train, batch_size, shuffle=True):\n",
        "        X_train_a = tl.prepro.threading_data(X_train_a, fn=distort_fn, is_train=True)  # data augmentation for training\n",
        "        sess.run(train_op, feed_dict={x: X_train_a, y_: y_train_a})\n",
        "\n",
        "    if epoch + 1 == 1 or (epoch + 1) % print_freq == 0:\n",
        "        print(\"Epoch %d of %d took %fs\" % (epoch + 1, n_epoch, time.time() - start_time))\n",
        "        # train_loss, train_acc, n_batch = 0, 0, 0\n",
        "        # for X_train_a, y_train_a in tl.iterate.minibatches(\n",
        "        #                         X_train, y_train, batch_size, shuffle=True):\n",
        "        #     X_train_a = tl.prepro.threading_data(X_train_a, fn=distort_fn, is_train=False)  # central crop\n",
        "        #     err, ac = sess.run([cost_test, acc], feed_dict={x: X_train_a, y_: y_train_a})\n",
        "        #     train_loss += err; train_acc += ac; n_batch += 1\n",
        "        # print(\"   train loss: %f\" % (train_loss/ n_batch))\n",
        "        # print(\"   train acc: %f\" % (train_acc/ n_batch))\n",
        "        test_loss, test_acc, n_batch = 0, 0, 0\n",
        "        for X_test_a, y_test_a in tl.iterate.minibatches(X_test, y_test, batch_size, shuffle=False):\n",
        "            X_test_a = tl.prepro.threading_data(X_test_a, fn=distort_fn, is_train=False)  # central crop\n",
        "            err, ac = sess.run([cost_test, acc], feed_dict={x: X_test_a, y_: y_test_a})\n",
        "            test_loss += err\n",
        "            test_acc += ac\n",
        "            n_batch += 1\n",
        "        print(\"   test loss: %f\" % (test_loss / n_batch))\n",
        "        print(\"   test acc: %f\" % (test_acc / n_batch))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorlayer\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c4/e2/736458723564163cfd87e3a9719a9fdece9011429bf556fb910d3691352e/tensorlayer-1.10.1-py2.py3-none-any.whl (313kB)\n",
            "\r\u001b[K    3% |█                               | 10kB 10.9MB/s eta 0:00:01\r\u001b[K    6% |██                              | 20kB 2.0MB/s eta 0:00:01\r\u001b[K    9% |███▏                            | 30kB 2.2MB/s eta 0:00:01\r\u001b[K    13% |████▏                           | 40kB 1.8MB/s eta 0:00:01\r\u001b[K    16% |█████▏                          | 51kB 2.0MB/s eta 0:00:01\r\u001b[K    19% |██████▎                         | 61kB 2.4MB/s eta 0:00:01\r\u001b[K    22% |███████▎                        | 71kB 2.6MB/s eta 0:00:01\r\u001b[K    26% |████████▍                       | 81kB 2.4MB/s eta 0:00:01\r\u001b[K    29% |█████████▍                      | 92kB 2.7MB/s eta 0:00:01\r\u001b[K    32% |██████████▍                     | 102kB 2.7MB/s eta 0:00:01\r\u001b[K    35% |███████████▌                    | 112kB 2.7MB/s eta 0:00:01\r\u001b[K    39% |████████████▌                   | 122kB 3.2MB/s eta 0:00:01\r\u001b[K    42% |█████████████▋                  | 133kB 3.3MB/s eta 0:00:01\r\u001b[K    45% |██████████████▋                 | 143kB 4.4MB/s eta 0:00:01\r\u001b[K    48% |███████████████▋                | 153kB 4.1MB/s eta 0:00:01\r\u001b[K    52% |████████████████▊               | 163kB 3.7MB/s eta 0:00:01\r\u001b[K    55% |█████████████████▊              | 174kB 4.0MB/s eta 0:00:01\r\u001b[K    58% |██████████████████▉             | 184kB 4.5MB/s eta 0:00:01\r\u001b[K    62% |███████████████████▉            | 194kB 4.5MB/s eta 0:00:01\r\u001b[K    65% |████████████████████▉           | 204kB 4.6MB/s eta 0:00:01\r\u001b[K    68% |██████████████████████          | 215kB 4.0MB/s eta 0:00:01\r\u001b[K    71% |███████████████████████         | 225kB 4.6MB/s eta 0:00:01\r\u001b[K    75% |████████████████████████        | 235kB 4.6MB/s eta 0:00:01\r\u001b[K    78% |█████████████████████████       | 245kB 3.9MB/s eta 0:00:01\r\u001b[K    81% |██████████████████████████      | 256kB 4.7MB/s eta 0:00:01\r\u001b[K    84% |███████████████████████████▏    | 266kB 4.6MB/s eta 0:00:01\r\u001b[K    88% |████████████████████████████▏   | 276kB 4.7MB/s eta 0:00:01\r\u001b[K    91% |█████████████████████████████▎  | 286kB 4.9MB/s eta 0:00:01\r\u001b[K    94% |██████████████████████████████▎ | 296kB 4.3MB/s eta 0:00:01\r\u001b[K    97% |███████████████████████████████▎| 307kB 4.9MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 317kB 5.1MB/s \n",
            "\u001b[?25hCollecting tqdm<4.26,>=4.23 (from tensorlayer)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c7/e0/52b2faaef4fd87f86eb8a8f1afa2cd6eb11146822033e29c04ac48ada32c/tqdm-4.25.0-py2.py3-none-any.whl (43kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 9.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn<0.20,>=0.19 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (0.19.2)\n",
            "Collecting matplotlib<2.3,>=2.2 (from tensorlayer)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/59/f235ab21bbe7b7c6570c4abf17ffb893071f4fa3b9cf557b09b60359ad9a/matplotlib-2.2.3-cp36-cp36m-manylinux1_x86_64.whl (12.6MB)\n",
            "\u001b[K    100% |████████████████████████████████| 12.6MB 2.9MB/s \n",
            "\u001b[?25hCollecting imageio<2.5,>=2.3 (from tensorlayer)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/b4/cbb592964dfd71a9de6a5b08f882fd334fb99ae09ddc82081dbb2f718c81/imageio-2.4.1.tar.gz (3.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 3.3MB 1.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt<1.11,>=1.10 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (1.10.11)\n",
            "Collecting lxml<4.3,>=4.2 (from tensorlayer)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/a4/9eea8035fc7c7670e5eab97f34ff2ef0ddd78a491bf96df5accedb0e63f5/lxml-4.2.5-cp36-cp36m-manylinux1_x86_64.whl (5.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 5.8MB 5.6MB/s \n",
            "\u001b[?25hCollecting progressbar2<3.39,>=3.38 (from tensorlayer)\n",
            "  Downloading https://files.pythonhosted.org/packages/4f/6f/acb2dd76f2c77527584bd3a4c2509782bb35c481c610521fc3656de5a9e0/progressbar2-3.38.0-py2.py3-none-any.whl\n",
            "Collecting scipy<1.2,>=1.1 (from tensorlayer)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/0b/f163da98d3a01b3e0ef1cab8dd2123c34aee2bafbb1c5bffa354cc8a1730/scipy-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (31.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 31.2MB 1.3MB/s \n",
            "\u001b[?25hCollecting requests<2.20,>=2.19 (from tensorlayer)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/47/7e02164a2a3db50ed6d8a6ab1d6d60b69c4c3fdf57a284257925dfc12bda/requests-2.19.1-py2.py3-none-any.whl (91kB)\n",
            "\u001b[K    100% |████████████████████████████████| 92kB 22.5MB/s \n",
            "\u001b[?25hCollecting scikit-image<0.15,>=0.14 (from tensorlayer)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/79/cefff573a53ca3fb4c390739d19541b95f371e24d2990aed4cd8837971f0/scikit_image-0.14.0-cp36-cp36m-manylinux1_x86_64.whl (25.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 25.3MB 2.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<1.16,>=1.14 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (1.14.5)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from matplotlib<2.3,>=2.2->tensorlayer) (2018.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib<2.3,>=2.2->tensorlayer) (0.10.0)\n",
            "Collecting kiwisolver>=1.0.1 (from matplotlib<2.3,>=2.2->tensorlayer)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/a7/88719d132b18300b4369fbffa741841cfd36d1e637e1990f27929945b538/kiwisolver-1.0.1-cp36-cp36m-manylinux1_x86_64.whl (949kB)\n",
            "\u001b[K    100% |████████████████████████████████| 952kB 19.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib<2.3,>=2.2->tensorlayer) (2.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib<2.3,>=2.2->tensorlayer) (2.5.3)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib<2.3,>=2.2->tensorlayer) (1.11.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio<2.5,>=2.3->tensorlayer) (4.0.0)\n",
            "Collecting python-utils>=2.3.0 (from progressbar2<3.39,>=3.38->tensorlayer)\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/a0/19119d8b7c05be49baf6c593f11c432d571b70d805f2fe94c0585e55e4c8/python_utils-2.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<2.20,>=2.19->tensorlayer) (2018.8.24)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<2.20,>=2.19->tensorlayer) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.24,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<2.20,>=2.19->tensorlayer) (1.22)\n",
            "Requirement already satisfied: idna<2.8,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<2.20,>=2.19->tensorlayer) (2.6)\n",
            "Collecting dask[array]>=0.9.0 (from scikit-image<0.15,>=0.14->tensorlayer)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e7/f01b2e72b4c235ee23c7730424b7bb0b36f1e93c90c129a8a14d7984ba5f/dask-0.19.1-py2.py3-none-any.whl (655kB)\n",
            "\u001b[K    100% |████████████████████████████████| 665kB 22.1MB/s \n",
            "\u001b[?25hCollecting cloudpickle>=0.2.1 (from scikit-image<0.15,>=0.14->tensorlayer)\n",
            "  Downloading https://files.pythonhosted.org/packages/98/d6/a78a4589234cc6f47f29665c1225f30467db5fdaf4ca1fb52b0685bff108/cloudpickle-0.5.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: networkx>=1.8 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15,>=0.14->tensorlayer) (2.1)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15,>=0.14->tensorlayer) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib<2.3,>=2.2->tensorlayer) (39.1.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->imageio<2.5,>=2.3->tensorlayer) (0.46)\n",
            "Requirement already satisfied: toolz>=0.7.3; extra == \"array\" in /usr/local/lib/python3.6/dist-packages (from dask[array]>=0.9.0->scikit-image<0.15,>=0.14->tensorlayer) (0.9.0)\n",
            "Requirement already satisfied: decorator>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=1.8->scikit-image<0.15,>=0.14->tensorlayer) (4.3.0)\n",
            "Building wheels for collected packages: imageio\n",
            "  Running setup.py bdist_wheel for imageio ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/e0/43/31/605de9372ceaf657f152d3d5e82f42cf265d81db8bbe63cde1\n",
            "Successfully built imageio\n",
            "\u001b[31mscikit-image 0.14.0 has requirement pillow>=4.3.0, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tqdm, kiwisolver, matplotlib, imageio, lxml, python-utils, progressbar2, scipy, requests, dask, cloudpickle, scikit-image, tensorlayer\n",
            "  Found existing installation: tqdm 4.26.0\n",
            "    Uninstalling tqdm-4.26.0:\n",
            "      Successfully uninstalled tqdm-4.26.0\n",
            "  Found existing installation: matplotlib 2.1.2\n",
            "    Uninstalling matplotlib-2.1.2:\n",
            "      Successfully uninstalled matplotlib-2.1.2\n",
            "  Found existing installation: scipy 0.19.1\n",
            "    Uninstalling scipy-0.19.1:\n",
            "      Successfully uninstalled scipy-0.19.1\n",
            "  Found existing installation: requests 2.18.4\n",
            "    Uninstalling requests-2.18.4:\n",
            "      Successfully uninstalled requests-2.18.4\n",
            "  Found existing installation: scikit-image 0.13.1\n",
            "    Uninstalling scikit-image-0.13.1:\n",
            "      Successfully uninstalled scikit-image-0.13.1\n",
            "Successfully installed cloudpickle-0.5.6 dask-0.19.1 imageio-2.4.1 kiwisolver-1.0.1 lxml-4.2.5 matplotlib-2.2.3 progressbar2-3.38.0 python-utils-2.3.0 requests-2.19.1 scikit-image-0.14.0 scipy-1.1.0 tensorlayer-1.10.1 tqdm-4.25.0\n",
            "[TL] Load or Download cifar10 > data/cifar10\n",
            "[TL] Downloading cifar-10-python.tar.gz...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100% (20813 of 20813) |##################| Elapsed Time: 0:01:12 ETA:  00:00:00"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[TL] Succesfully downloaded cifar-10-python.tar.gz 170498071 bytes.\n",
            "[TL] Trying to extract tar file\n",
            "[TL] ... Success!\n",
            "[TL] InputLayer  model/input: (?, 24, 24, 3)\n",
            "[TL] Conv2d model/cnn1: n_filter: 64 filter_size: (5, 5) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNormLayer model/batch1: decay: 1.000000 epsilon: 0.000010 act: relu is_train: False\n",
            "[TL] MaxPool2d model/pool1: filter_size: (3, 3) strides: (2, 2) padding: SAME\n",
            "[TL] Conv2d model/cnn2: n_filter: 64 filter_size: (5, 5) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNormLayer model/batch2: decay: 1.000000 epsilon: 0.000010 act: relu is_train: False\n",
            "[TL] MaxPool2d model/pool2: filter_size: (3, 3) strides: (2, 2) padding: SAME\n",
            "[TL] FlattenLayer model/flatten: 2304\n",
            "[TL] DenseLayer  model/d1relu: 384 relu\n",
            "[TL] DenseLayer  model/d2relu: 192 relu\n",
            "[TL] DenseLayer  model/output: 10 No Activation\n",
            "[TL]   [*] geting variables with relu/W\n",
            "[TL]   got   0: model/d1relu/W:0   (2304, 384)\n",
            "[TL]   got   1: model/d2relu/W:0   (384, 192)\n",
            "[TL] InputLayer  model/input: (?, 24, 24, 3)\n",
            "[TL] Conv2d model/cnn1: n_filter: 64 filter_size: (5, 5) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNormLayer model/batch1: decay: 0.000000 epsilon: 0.000010 act: relu is_train: False\n",
            "[TL] MaxPool2d model/pool1: filter_size: (3, 3) strides: (2, 2) padding: SAME\n",
            "[TL] Conv2d model/cnn2: n_filter: 64 filter_size: (5, 5) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNormLayer model/batch2: decay: 0.000000 epsilon: 0.000010 act: relu is_train: False\n",
            "[TL] MaxPool2d model/pool2: filter_size: (3, 3) strides: (2, 2) padding: SAME\n",
            "[TL] FlattenLayer model/flatten: 2304\n",
            "[TL] DenseLayer  model/d1relu: 384 relu\n",
            "[TL] DenseLayer  model/d2relu: 192 relu\n",
            "[TL] DenseLayer  model/output: 10 No Activation\n",
            "[TL]   [*] geting variables with relu/W\n",
            "[TL]   got   0: model/d1relu/W:0   (2304, 384)\n",
            "[TL]   got   1: model/d2relu/W:0   (384, 192)\n",
            "[TL] WARNING: Function: `tensorlayer.layers.utils.initialize_global_variables` (in file: /usr/local/lib/python3.6/dist-packages/tensorlayer/layers/utils.py) is deprecated and will be removed after 2018-09-30.\n",
            "Instructions for updating: This API is deprecated in favor of `tf.global_variables_initializer`\n",
            "\n",
            "[TL]   param   0: model/cnn1/kernel:0  (5, 5, 3, 64)      float32_ref\n",
            "[TL]   param   1: model/batch1/beta:0  (64,)              float32_ref\n",
            "[TL]   param   2: model/batch1/gamma:0 (64,)              float32_ref\n",
            "[TL]   param   3: model/batch1/moving_mean:0 (64,)              float32_ref\n",
            "[TL]   param   4: model/batch1/moving_variance:0 (64,)              float32_ref\n",
            "[TL]   param   5: model/cnn2/kernel:0  (5, 5, 64, 64)     float32_ref\n",
            "[TL]   param   6: model/batch2/beta:0  (64,)              float32_ref\n",
            "[TL]   param   7: model/batch2/gamma:0 (64,)              float32_ref\n",
            "[TL]   param   8: model/batch2/moving_mean:0 (64,)              float32_ref\n",
            "[TL]   param   9: model/batch2/moving_variance:0 (64,)              float32_ref\n",
            "[TL]   param  10: model/d1relu/W:0     (2304, 384)        float32_ref\n",
            "[TL]   param  11: model/d1relu/b:0     (384,)             float32_ref\n",
            "[TL]   param  12: model/d2relu/W:0     (384, 192)         float32_ref\n",
            "[TL]   param  13: model/d2relu/b:0     (192,)             float32_ref\n",
            "[TL]   param  14: model/output/W:0     (192, 10)          float32_ref\n",
            "[TL]   param  15: model/output/b:0     (10,)              float32_ref\n",
            "[TL]   num of params: 1068682\n",
            "[TL]   layer   0: x:0                  (?, 24, 24, 3)     float32\n",
            "[TL]   layer   1: model/cnn1/Conv2D:0  (?, 24, 24, 64)    float32\n",
            "[TL]   layer   2: model/batch1/Relu:0  (?, 24, 24, 64)    float32\n",
            "[TL]   layer   3: model/pool1/MaxPool:0 (?, 12, 12, 64)    float32\n",
            "[TL]   layer   4: model/cnn2/Conv2D:0  (?, 12, 12, 64)    float32\n",
            "[TL]   layer   5: model/batch2/Relu:0  (?, 12, 12, 64)    float32\n",
            "[TL]   layer   6: model/pool2/MaxPool:0 (?, 6, 6, 64)      float32\n",
            "[TL]   layer   7: model/flatten:0      (?, 2304)          float32\n",
            "[TL]   layer   8: model/d1relu/Relu:0  (?, 384)           float32\n",
            "[TL]   layer   9: model/d2relu/Relu:0  (?, 192)           float32\n",
            "[TL]   layer  10: model/output/bias_add:0 (?, 10)            float32\n",
            "   learning_rate: 0.000100\n",
            "   batch_size: 128\n",
            "Epoch 1 of 50000 took 44.931546s\n",
            "   test loss: 2.912768\n",
            "   test acc: 0.451322\n",
            "Epoch 2 of 50000 took 44.034348s\n",
            "   test loss: 2.442669\n",
            "   test acc: 0.510717\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}