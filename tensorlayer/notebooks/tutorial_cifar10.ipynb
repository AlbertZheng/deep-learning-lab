{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tutorial_cifar10.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/AlbertZheng/deep-learning-lab/blob/master/tensorlayer/notebooks/tutorial_cifar10.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "SrrrsgNnj6J-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorlayer\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorlayer as tl\n",
        "from tensorlayer.layers import *\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.DEBUG)\n",
        "tl.logging.set_verbosity(tl.logging.DEBUG)\n",
        "\n",
        "# See https://www.tensorflow.org/tutorials/using_gpu#allowing_gpu_memory_growth\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "sess = tf.InteractiveSession(config=config)\n",
        "\n",
        "X_train, y_train, X_test, y_test = tl.files.load_cifar10_dataset(shape=(-1, 32, 32, 3), plotable=False)\n",
        "\n",
        "\n",
        "def model(x, y_, reuse):\n",
        "    W_init = tf.truncated_normal_initializer(stddev=5e-2)\n",
        "    W_init2 = tf.truncated_normal_initializer(stddev=0.04)\n",
        "    b_init2 = tf.constant_initializer(value=0.1)\n",
        "    with tf.variable_scope(\"model\", reuse=reuse):\n",
        "        net = InputLayer(x, name='input')\n",
        "        net = Conv2d(net, 64, (5, 5), (1, 1), act=tf.nn.relu, padding='SAME', W_init=W_init, name='cnn1')\n",
        "        net = MaxPool2d(net, (3, 3), (2, 2), padding='SAME', name='pool1')\n",
        "        net = LocalResponseNormLayer(net, depth_radius=4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm1')\n",
        "\n",
        "        net = Conv2d(net, 64, (5, 5), (1, 1), act=tf.nn.relu, padding='SAME', W_init=W_init, name='cnn2')\n",
        "        net = LocalResponseNormLayer(net, depth_radius=4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm2')\n",
        "        net = MaxPool2d(net, (3, 3), (2, 2), padding='SAME', name='pool2')\n",
        "\n",
        "        net = FlattenLayer(net, name='flatten')\n",
        "        net = DenseLayer(net, 384, act=tf.nn.relu, W_init=W_init2, b_init=b_init2, name='d1relu')\n",
        "        net = DenseLayer(net, 192, act=tf.nn.relu, W_init=W_init2, b_init=b_init2, name='d2relu')\n",
        "        net = DenseLayer(net, 10, act=None, W_init=W_init2, name='output')\n",
        "        y = net.outputs\n",
        "\n",
        "        ce = tl.cost.cross_entropy(y, y_, name='cost')\n",
        "        # L2 for the MLP, without this, the accuracy will be reduced by 15%.\n",
        "        L2 = 0\n",
        "        for p in tl.layers.get_variables_with_name('relu/W', True, True):\n",
        "            L2 += tf.contrib.layers.l2_regularizer(0.004)(p)\n",
        "        cost = ce + L2\n",
        "\n",
        "        correct_prediction = tf.equal(tf.argmax(y, 1), y_)\n",
        "        acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "        return net, cost, acc\n",
        "\n",
        "\n",
        "def model_batch_norm(x, y_, reuse, is_train):\n",
        "    \"\"\"Batch normalization should be placed before rectifier.\"\"\"\n",
        "    W_init = tf.truncated_normal_initializer(stddev=5e-2)\n",
        "    W_init2 = tf.truncated_normal_initializer(stddev=0.04)\n",
        "    b_init2 = tf.constant_initializer(value=0.1)\n",
        "    with tf.variable_scope(\"model\", reuse=reuse):\n",
        "        net = InputLayer(x, name='input')\n",
        "        net = Conv2d(net, 64, (5, 5), (1, 1), padding='SAME', W_init=W_init, b_init=None, name='cnn1')\n",
        "        net = BatchNormLayer(net, is_train, act=tf.nn.relu, name='batch1')\n",
        "        net = MaxPool2d(net, (3, 3), (2, 2), padding='SAME', name='pool1')\n",
        "\n",
        "        net = Conv2d(net, 64, (5, 5), (1, 1), padding='SAME', W_init=W_init, b_init=None, name='cnn2')\n",
        "        net = BatchNormLayer(net, is_train, act=tf.nn.relu, name='batch2')\n",
        "        net = MaxPool2d(net, (3, 3), (2, 2), padding='SAME', name='pool2')\n",
        "\n",
        "        net = FlattenLayer(net, name='flatten')  # output: (batch_size, 2304)\n",
        "        net = DenseLayer(net, 384, act=tf.nn.relu, W_init=W_init2, b_init=b_init2, name='d1relu')\n",
        "        net = DenseLayer(net, 192, act=tf.nn.relu, W_init=W_init2, b_init=b_init2, name='d2relu')\n",
        "        net = DenseLayer(net, 10, act=None, W_init=W_init2, name='output')\n",
        "        y = net.outputs\n",
        "\n",
        "        ce = tl.cost.cross_entropy(y, y_, name='cost')\n",
        "        # L2 for the MLP, without this, the accuracy will be reduced by 15%.\n",
        "        L2 = 0\n",
        "        for p in tl.layers.get_variables_with_name('relu/W', True, True):\n",
        "            L2 += tf.contrib.layers.l2_regularizer(0.004)(p)\n",
        "        cost = ce + L2\n",
        "\n",
        "        correct_prediction = tf.equal(tf.argmax(y, 1), y_)\n",
        "        acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "        return net, cost, acc\n",
        "\n",
        "\n",
        "def distort_fn(x, is_train=False):\n",
        "    \"\"\"\n",
        "    The images are processed as follows:\n",
        "    .. They are cropped to 24 x 24 pixels, centrally for evaluation or randomly for training.\n",
        "    .. They are approximately whitened to make the model insensitive to dynamic range.\n",
        "    For training, we additionally apply a series of random distortions to\n",
        "    artificially increase the data set size:\n",
        "    .. Randomly flip the image from left to right.\n",
        "    .. Randomly distort the image brightness.\n",
        "    \"\"\"\n",
        "    # print('begin',x.shape, np.min(x), np.max(x))\n",
        "    x = tl.prepro.crop(x, 24, 24, is_random=is_train)\n",
        "    # print('after crop',x.shape, np.min(x), np.max(x))\n",
        "    if is_train:\n",
        "        # x = tl.prepro.zoom(x, zoom_range=(0.9, 1.0), is_random=True)\n",
        "        # print('after zoom', x.shape, np.min(x), np.max(x))\n",
        "        x = tl.prepro.flip_axis(x, axis=1, is_random=True)\n",
        "        # print('after flip',x.shape, np.min(x), np.max(x))\n",
        "        x = tl.prepro.brightness(x, gamma=0.1, gain=1, is_random=True)\n",
        "        # print('after brightness',x.shape, np.min(x), np.max(x))\n",
        "        # tmp = np.max(x)\n",
        "        # x += np.random.uniform(-20, 20)\n",
        "        # x /= tmp\n",
        "    # normalize the image\n",
        "    x = (x - np.mean(x)) / max(np.std(x), 1e-5)  # avoid values divided by 0\n",
        "    # print('after norm', x.shape, np.min(x), np.max(x), np.mean(x))\n",
        "    return x\n",
        "\n",
        "\n",
        "x = tf.placeholder(dtype=tf.float32, shape=[None, 24, 24, 3], name='x')\n",
        "y_ = tf.placeholder(dtype=tf.int64, shape=[None], name='y_')\n",
        "\n",
        "# using local response normalization\n",
        "# network, cost, _ = model(x, y_, False)\n",
        "# _, cost_test, acc = model(x, y_, True)\n",
        "# you may want to try batch normalization\n",
        "network, cost, _ = model_batch_norm(x, y_, False, is_train=True)\n",
        "_, cost_test, acc = model_batch_norm(x, y_, True, is_train=False)\n",
        "\n",
        "# train\n",
        "n_epoch = 50000\n",
        "learning_rate = 0.0001\n",
        "print_freq = 1\n",
        "batch_size = 128\n",
        "\n",
        "train_params = network.all_params\n",
        "train_op = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-08,\n",
        "                                  use_locking=False).minimize(cost, var_list=train_params)\n",
        "\n",
        "tl.layers.initialize_global_variables(sess)\n",
        "\n",
        "network.print_params(False)\n",
        "network.print_layers()\n",
        "\n",
        "print('   learning_rate: %f' % learning_rate)\n",
        "print('   batch_size: %d' % batch_size)\n",
        "\n",
        "for epoch in range(n_epoch):\n",
        "    start_time = time.time()\n",
        "    for X_train_a, y_train_a in tl.iterate.minibatches(X_train, y_train, batch_size, shuffle=True):\n",
        "        X_train_a = tl.prepro.threading_data(X_train_a, fn=distort_fn, is_train=True)  # data augmentation for training\n",
        "        sess.run(train_op, feed_dict={x: X_train_a, y_: y_train_a})\n",
        "\n",
        "    if epoch + 1 == 1 or (epoch + 1) % print_freq == 0:\n",
        "        print(\"Epoch %d of %d took %fs\" % (epoch + 1, n_epoch, time.time() - start_time))\n",
        "        # train_loss, train_acc, n_batch = 0, 0, 0\n",
        "        # for X_train_a, y_train_a in tl.iterate.minibatches(\n",
        "        #                         X_train, y_train, batch_size, shuffle=True):\n",
        "        #     X_train_a = tl.prepro.threading_data(X_train_a, fn=distort_fn, is_train=False)  # central crop\n",
        "        #     err, ac = sess.run([cost_test, acc], feed_dict={x: X_train_a, y_: y_train_a})\n",
        "        #     train_loss += err; train_acc += ac; n_batch += 1\n",
        "        # print(\"   train loss: %f\" % (train_loss/ n_batch))\n",
        "        # print(\"   train acc: %f\" % (train_acc/ n_batch))\n",
        "        test_loss, test_acc, n_batch = 0, 0, 0\n",
        "        for X_test_a, y_test_a in tl.iterate.minibatches(X_test, y_test, batch_size, shuffle=False):\n",
        "            X_test_a = tl.prepro.threading_data(X_test_a, fn=distort_fn, is_train=False)  # central crop\n",
        "            err, ac = sess.run([cost_test, acc], feed_dict={x: X_test_a, y_: y_test_a})\n",
        "            test_loss += err\n",
        "            test_acc += ac\n",
        "            n_batch += 1\n",
        "        print(\"   test loss: %f\" % (test_loss / n_batch))\n",
        "        print(\"   test acc: %f\" % (test_acc / n_batch))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}