{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "[View in Colaboratory](https://colab.research.google.com/github/AlbertZheng/deep-learning-lab/blob/master/tensorlayer/notebooks/tutorial_cifar10_tfrecord.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4meRmKT-6oI",
    "colab_type": "text"
   },
   "source": [
    "# Reimplementation of the TensorFlow official CIFAR-10 CNN tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYvgLpz0s7lI",
    "colab_type": "text"
   },
   "source": [
    "## Install dependent packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4qOgBM9LsUEr",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1465.0
    },
    "outputId": "458d2ae8-d965-47dc-a44b-4b0ae9cf8c65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorlayer>=1.10\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c4/e2/736458723564163cfd87e3a9719a9fdece9011429bf556fb910d3691352e/tensorlayer-1.10.1-py2.py3-none-any.whl (313kB)\n",
      "\u001b[K    100% |████████████████████████████████| 317kB 7.9MB/s \n",
      "\u001b[?25hCollecting lxml<4.3,>=4.2 (from tensorlayer>=1.10)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/a4/9eea8035fc7c7670e5eab97f34ff2ef0ddd78a491bf96df5accedb0e63f5/lxml-4.2.5-cp36-cp36m-manylinux1_x86_64.whl (5.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 5.8MB 5.7MB/s \n",
      "\u001b[?25hCollecting requests<2.20,>=2.19 (from tensorlayer>=1.10)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/47/7e02164a2a3db50ed6d8a6ab1d6d60b69c4c3fdf57a284257925dfc12bda/requests-2.19.1-py2.py3-none-any.whl (91kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 23.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: scikit-learn<0.20,>=0.19 in /usr/local/lib/python3.6/dist-packages (from tensorlayer>=1.10) (0.19.2)\n",
      "Collecting scikit-image<0.15,>=0.14 (from tensorlayer>=1.10)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/79/cefff573a53ca3fb4c390739d19541b95f371e24d2990aed4cd8837971f0/scikit_image-0.14.0-cp36-cp36m-manylinux1_x86_64.whl (25.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 25.3MB 697kB/s \n",
      "\u001b[?25hRequirement already satisfied: wrapt<1.11,>=1.10 in /usr/local/lib/python3.6/dist-packages (from tensorlayer>=1.10) (1.10.11)\n",
      "Requirement already satisfied: numpy<1.16,>=1.14 in /usr/local/lib/python3.6/dist-packages (from tensorlayer>=1.10) (1.14.5)\n",
      "Collecting scipy<1.2,>=1.1 (from tensorlayer>=1.10)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/0b/f163da98d3a01b3e0ef1cab8dd2123c34aee2bafbb1c5bffa354cc8a1730/scipy-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (31.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 31.2MB 1.4MB/s \n",
      "\u001b[?25hCollecting tqdm<4.26,>=4.23 (from tensorlayer>=1.10)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c7/e0/52b2faaef4fd87f86eb8a8f1afa2cd6eb11146822033e29c04ac48ada32c/tqdm-4.25.0-py2.py3-none-any.whl (43kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 20.1MB/s \n",
      "\u001b[?25hCollecting progressbar2<3.39,>=3.38 (from tensorlayer>=1.10)\n",
      "  Downloading https://files.pythonhosted.org/packages/4f/6f/acb2dd76f2c77527584bd3a4c2509782bb35c481c610521fc3656de5a9e0/progressbar2-3.38.0-py2.py3-none-any.whl\n",
      "Collecting imageio<2.5,>=2.3 (from tensorlayer>=1.10)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/b4/cbb592964dfd71a9de6a5b08f882fd334fb99ae09ddc82081dbb2f718c81/imageio-2.4.1.tar.gz (3.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.3MB 975kB/s \n",
      "\u001b[?25hCollecting matplotlib<2.3,>=2.2 (from tensorlayer>=1.10)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/59/f235ab21bbe7b7c6570c4abf17ffb893071f4fa3b9cf557b09b60359ad9a/matplotlib-2.2.3-cp36-cp36m-manylinux1_x86_64.whl (12.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 12.6MB 2.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<2.20,>=2.19->tensorlayer>=1.10) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.24,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<2.20,>=2.19->tensorlayer>=1.10) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<2.20,>=2.19->tensorlayer>=1.10) (2018.8.24)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<2.20,>=2.19->tensorlayer>=1.10) (2.6)\n",
      "Collecting pillow>=4.3.0 (from scikit-image<0.15,>=0.14->tensorlayer>=1.10)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/24/f53ff6b61b3d728b90934bddb4f03f8ab584a7f49299bf3bde56e2952612/Pillow-5.2.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 2.0MB 10.8MB/s \n",
      "\u001b[?25hCollecting cloudpickle>=0.2.1 (from scikit-image<0.15,>=0.14->tensorlayer>=1.10)\n",
      "  Downloading https://files.pythonhosted.org/packages/98/d6/a78a4589234cc6f47f29665c1225f30467db5fdaf4ca1fb52b0685bff108/cloudpickle-0.5.6-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15,>=0.14->tensorlayer>=1.10) (1.11.0)\n",
      "Collecting dask[array]>=0.9.0 (from scikit-image<0.15,>=0.14->tensorlayer>=1.10)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/cc/d8b279ad3512e682069263de0494eeb9907ebe6b98bef01424be36421e13/dask-0.19.2-py2.py3-none-any.whl (657kB)\n",
      "\u001b[K    100% |████████████████████████████████| 665kB 22.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15,>=0.14->tensorlayer>=1.10) (1.0.0)\n",
      "Requirement already satisfied: networkx>=1.8 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15,>=0.14->tensorlayer>=1.10) (2.2)\n",
      "Collecting python-utils>=2.3.0 (from progressbar2<3.39,>=3.38->tensorlayer>=1.10)\n",
      "  Downloading https://files.pythonhosted.org/packages/eb/a0/19119d8b7c05be49baf6c593f11c432d571b70d805f2fe94c0585e55e4c8/python_utils-2.3.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib<2.3,>=2.2->tensorlayer>=1.10) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib<2.3,>=2.2->tensorlayer>=1.10) (2.5.3)\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib<2.3,>=2.2->tensorlayer>=1.10)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/a7/88719d132b18300b4369fbffa741841cfd36d1e637e1990f27929945b538/kiwisolver-1.0.1-cp36-cp36m-manylinux1_x86_64.whl (949kB)\n",
      "\u001b[K    100% |████████████████████████████████| 952kB 18.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib<2.3,>=2.2->tensorlayer>=1.10) (2.2.1)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from matplotlib<2.3,>=2.2->tensorlayer>=1.10) (2018.5)\n",
      "Requirement already satisfied: toolz>=0.7.3; extra == \"array\" in /usr/local/lib/python3.6/dist-packages (from dask[array]>=0.9.0->scikit-image<0.15,>=0.14->tensorlayer>=1.10) (0.9.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=1.8->scikit-image<0.15,>=0.14->tensorlayer>=1.10) (4.3.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib<2.3,>=2.2->tensorlayer>=1.10) (39.1.0)\n",
      "Building wheels for collected packages: imageio\n",
      "  Running setup.py bdist_wheel for imageio ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/e0/43/31/605de9372ceaf657f152d3d5e82f42cf265d81db8bbe63cde1\n",
      "Successfully built imageio\n",
      "Installing collected packages: lxml, requests, scipy, pillow, cloudpickle, dask, kiwisolver, matplotlib, scikit-image, tqdm, python-utils, progressbar2, imageio, tensorlayer\n",
      "  Found existing installation: requests 2.18.4\n",
      "    Uninstalling requests-2.18.4:\n",
      "      Successfully uninstalled requests-2.18.4\n",
      "  Found existing installation: scipy 0.19.1\n",
      "    Uninstalling scipy-0.19.1:\n",
      "      Successfully uninstalled scipy-0.19.1\n",
      "  Found existing installation: Pillow 4.0.0\n",
      "    Uninstalling Pillow-4.0.0:\n",
      "      Successfully uninstalled Pillow-4.0.0\n",
      "  Found existing installation: matplotlib 2.1.2\n",
      "    Uninstalling matplotlib-2.1.2:\n",
      "      Successfully uninstalled matplotlib-2.1.2\n",
      "  Found existing installation: scikit-image 0.13.1\n",
      "    Uninstalling scikit-image-0.13.1:\n",
      "      Successfully uninstalled scikit-image-0.13.1\n",
      "  Found existing installation: tqdm 4.26.0\n",
      "    Uninstalling tqdm-4.26.0:\n",
      "      Successfully uninstalled tqdm-4.26.0\n",
      "Successfully installed cloudpickle-0.5.6 dask-0.19.2 imageio-2.4.1 kiwisolver-1.0.1 lxml-4.2.5 matplotlib-2.2.3 pillow-5.2.0 progressbar2-3.38.0 python-utils-2.3.0 requests-2.19.1 scikit-image-0.14.0 scipy-1.1.0 tensorlayer-1.10.1 tqdm-4.25.0\n",
      "tensorboard              1.10.0   \n",
      "tensorflow               1.10.1   \n",
      "tensorflow-hub           0.1.1    \n",
      "tensorlayer              1.10.1   \n"
     ]
    }
   ],
   "source": [
    "!pip install \"tensorlayer>=1.10\"\n",
    "!pip list|grep tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXCuT0b_tMmG",
    "colab_type": "text"
   },
   "source": [
    "## Check if GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "k6xg3mTYs64Y",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    },
    "outputId": "d8b46540-3304-4e1e-f600-b0502c28ccb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Found GPU at: /device:GPU:0 ###\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "from tensorlayer.layers import *\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.DEBUG)\n",
    "tl.logging.set_verbosity(tl.logging.DEBUG)\n",
    "\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('*** GPU device not found ***')\n",
    "print('### Found GPU at: {} ###'.format(device_name))\n",
    "\n",
    "# See https://www.tensorflow.org/tutorials/using_gpu#allowing_gpu_memory_growth\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ej0VrZhRuE4H",
    "colab_type": "text"
   },
   "source": [
    "## Download CIFAR10 dataset, and convert to TFRecord format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Ek-6WH4nuIhx",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 945.0
    },
    "outputId": "2f873dfc-8861-497a-a40e-334bdfafefd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TL] Load or Download cifar10 > data/cifar10\n",
      "[TL] Downloading cifar-10-python.tar.gz...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (20813 of 20813) |##################| Elapsed Time: 0:00:48 ETA:  00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TL] Succesfully downloaded cifar-10-python.tar.gz 170498071 bytes.\n",
      "[TL] Trying to extract tar file\n",
      "[TL] ... Success!\n",
      "X_train.shape  (50000, 32, 32, 3)\ty_train.shape  (50000,)\n",
      "X_train.dtype float32 , y_train.dtype int32\n",
      "X_test.shape  (10000, 32, 32, 3)\ty_test.shape  (10000,)\n",
      "X_test.dtype float32 , y_test.dtype int32\n",
      "Current directory: /content \n",
      "Converting data into train.cifar10 ...\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFZCAYAAAARqQ0OAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f398fe3c400>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /content \n",
      "Converting data into test.cifar10 ...\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFZCAYAAAARqQ0OAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f398fe44b00>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = tl.files.load_cifar10_dataset(\n",
    "    shape=(-1, 32, 32, 3), plotable=False)\n",
    "\n",
    "\n",
    "def print_dataset_shape(X_name, X, y_name, y):\n",
    "    print(X_name + '.shape ', X.shape, end='\\t')\n",
    "    print(y_name + '.shape ', y.shape)\n",
    "    print('%s.dtype %s , %s.dtype %s' % (X_name, X.dtype, y_name, y.dtype))\n",
    "\n",
    "\n",
    "print_dataset_shape('X_train', X_train, 'y_train', y_train)\n",
    "# (50000, 32, 32, 3)\n",
    "# (50000,)\n",
    "# X_train.dtype float32 , y_train.dtype int32\n",
    "\n",
    "print_dataset_shape('X_test', X_test, 'y_test', y_test)\n",
    "# (10000, 32, 32, 3)\n",
    "# (10000,)\n",
    "# X_test.dtype float32 , y_test.dtype int32\n",
    "\n",
    "\n",
    "def data_to_tfrecord(images, labels, filename):\n",
    "    \"\"\"Save data into TFRecord.\"\"\"\n",
    "    if os.path.isfile(filename):\n",
    "        print(\"%s exists\" % filename)\n",
    "        return\n",
    "\n",
    "    print(\"Current directory: %s \" % os.getcwd())\n",
    "    print(\"Converting data into %s ...\" % filename)\n",
    "\n",
    "    writer = tf.python_io.TFRecordWriter(filename)\n",
    "    for index, img in enumerate(images):\n",
    "        label = int(labels[index])\n",
    "        img_raw = img.tobytes()\n",
    "\n",
    "        # Visualize a image\n",
    "        if index == 0:\n",
    "            tl.visualize.frame(np.asarray(img, dtype=np.uint8), second=1,\n",
    "                               saveable=False, name='label: ' + str(label), fig_idx=1236)\n",
    "\n",
    "        # Convert the bytes back to image as follow:\n",
    "        # image = Image.frombytes('RGB', (32, 32), img_raw)\n",
    "        # image = np.fromstring(img_raw, np.float32)\n",
    "        # image = image.reshape([32, 32, 3])\n",
    "        # tl.visualize.frame(np.asarray(image, dtype=np.uint8), second=1, saveable=False, name='frame', fig_idx=1236)\n",
    "\n",
    "        example = tf.train.Example(\n",
    "            features=tf.train.Features(\n",
    "                feature={\n",
    "                    \"label\": tf.train.Feature(int64_list=tf.train.Int64List(value=[label])),\n",
    "                    'img_raw': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_raw])),\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "        writer.write(example.SerializeToString())  # Serialize To String\n",
    "    writer.close()\n",
    "\n",
    "    \n",
    "# Save data into TFRecord files\n",
    "data_to_tfrecord(images=X_train, labels=y_train, filename=\"train.cifar10\")\n",
    "data_to_tfrecord(images=X_test, labels=y_test, filename=\"test.cifar10\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hb9uyk8zsl73",
    "colab_type": "text"
   },
   "source": [
    "## Funcation: return a tensor tuple (image, label) for reading and decoding a single example from TFRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "zoyyRqlHtbtV",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def read_and_decode(filename, is_train=None):\n",
    "    \"\"\"Return tensor to read from TFRecord.\"\"\"\n",
    "    filename_queue = tf.train.string_input_producer([filename])\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example, features={\n",
    "            'label': tf.FixedLenFeature([], tf.int64),\n",
    "            'img_raw': tf.FixedLenFeature([], tf.string),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # You can do more image distortion here for training data\n",
    "    img = tf.decode_raw(features['img_raw'], tf.float32)\n",
    "    img = tf.reshape(img, [32, 32, 3])\n",
    "    # img = tf.cast(img, tf.float32) #* (1. / 255) - 0.5\n",
    "    if is_train == True:\n",
    "        # 1. Randomly crop a [height, width] section of the image.\n",
    "        img = tf.random_crop(img, [24, 24, 3])\n",
    "\n",
    "        # 2. Randomly flip the image horizontally.\n",
    "        img = tf.image.random_flip_left_right(img)\n",
    "\n",
    "        # 3. Randomly change brightness.\n",
    "        img = tf.image.random_brightness(img, max_delta=63)\n",
    "\n",
    "        # 4. Randomly change contrast.\n",
    "        img = tf.image.random_contrast(img, lower=0.2, upper=1.8)\n",
    "\n",
    "        # 5. Subtract off the mean and divide by the variance of the pixels.\n",
    "        img = tf.image.per_image_standardization(img)\n",
    "\n",
    "    elif is_train == False:\n",
    "        # 1. Crop the central [height, width] of the image.\n",
    "        img = tf.image.resize_image_with_crop_or_pad(img, 24, 24)\n",
    "\n",
    "        # 2. Subtract off the mean and divide by the variance of the pixels.\n",
    "        img = tf.image.per_image_standardization(img)\n",
    "\n",
    "    elif is_train == None:\n",
    "        img = img\n",
    "\n",
    "    label = tf.cast(features['label'], tf.int32)\n",
    "\n",
    "    return img, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnurt7UwtmB2",
    "colab_type": "text"
   },
   "source": [
    "## Funcation: return the network model, cost and accuracy tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "h-nCqRQftlK8",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "\"\"\" Model that using LRN (local response normalization) \"\"\"\n",
    "def model(x_crop, y_, reuse):\n",
    "    \"\"\"For more simplified CNN APIs, check tensorlayer.org.\"\"\"\n",
    "    W_init = tf.truncated_normal_initializer(stddev=5e-2)\n",
    "    W_init2 = tf.truncated_normal_initializer(stddev=0.04)\n",
    "    b_init2 = tf.constant_initializer(value=0.1)\n",
    "\n",
    "    with tf.variable_scope(\"model\", reuse=reuse):\n",
    "        net = InputLayer(x_crop, name='input')\n",
    "        net = Conv2d(net, 64, (5, 5), (1, 1), act=tf.nn.relu,\n",
    "                     padding='SAME', W_init=W_init, name='cnn1')\n",
    "        net = MaxPool2d(net, (3, 3), (2, 2), padding='SAME', name='pool1')\n",
    "        net = LocalResponseNormLayer(\n",
    "            net, depth_radius=4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm1')\n",
    "\n",
    "        net = Conv2d(net, 64, (5, 5), (1, 1), act=tf.nn.relu,\n",
    "                     padding='SAME', W_init=W_init, name='cnn2')\n",
    "        net = LocalResponseNormLayer(\n",
    "            net, depth_radius=4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm2')\n",
    "        net = MaxPool2d(net, (3, 3), (2, 2), padding='SAME', name='pool2')\n",
    "\n",
    "        net = FlattenLayer(net, name='flatten')\n",
    "        net = DenseLayer(net, 384, act=tf.nn.relu,\n",
    "                         W_init=W_init2, b_init=b_init2, name='d1relu')\n",
    "        net = DenseLayer(net, 192, act=tf.nn.relu,\n",
    "                         W_init=W_init2, b_init=b_init2, name='d2relu')\n",
    "        net = DenseLayer(net, n_units=10, act=None,\n",
    "                         W_init=W_init2, name='output')\n",
    "        y = net.outputs\n",
    "\n",
    "        ce = tl.cost.cross_entropy(y, y_, name='cost')\n",
    "\n",
    "        \"\"\" 需给后面的全连接层引入L2 normalization，惩罚模型的复杂度，避免overfitting \"\"\"\n",
    "        # L2 for the MLP, without this, the accuracy will be reduced by 15%.\n",
    "        L2 = 0\n",
    "        for p in tl.layers.get_variables_with_name('relu/W', True, True):\n",
    "            L2 += tf.contrib.layers.l2_regularizer(0.004)(p)\n",
    "        # 加上L2模型复杂度惩罚项后，得到最终真正的cost\n",
    "        cost = ce + L2\n",
    "\n",
    "        # correct_prediction = tf.equal(tf.argmax(tf.nn.softmax(y), 1), y_)\n",
    "        correct_prediction = tf.equal(tf.cast(tf.argmax(y, 1), tf.int32), y_)\n",
    "        acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        return net, cost, acc\n",
    "\n",
    "\n",
    "\"\"\" Model that using batch normalization \"\"\"    \n",
    "def model_batch_norm(x_crop, y_, reuse, is_train):\n",
    "    \"\"\"Batch normalization should be placed before rectifier.\"\"\"\n",
    "    W_init = tf.truncated_normal_initializer(stddev=5e-2)\n",
    "    W_init2 = tf.truncated_normal_initializer(stddev=0.04)\n",
    "    b_init2 = tf.constant_initializer(value=0.1)\n",
    "\n",
    "    with tf.variable_scope(\"model\", reuse=reuse):\n",
    "        net = InputLayer(x_crop, name='input')\n",
    "        net = Conv2d(net, 64, (5, 5), (1, 1), padding='SAME',\n",
    "                     W_init=W_init, b_init=None, name='cnn1')\n",
    "        net = BatchNormLayer(net, is_train, act=tf.nn.relu, name='batch1')\n",
    "        net = MaxPool2d(net, (3, 3), (2, 2), padding='SAME', name='pool1')\n",
    "\n",
    "        net = Conv2d(net, 64, (5, 5), (1, 1), padding='SAME',\n",
    "                     W_init=W_init, b_init=None, name='cnn2')\n",
    "        net = BatchNormLayer(net, is_train, act=tf.nn.relu, name='batch2')\n",
    "        net = MaxPool2d(net, (3, 3), (2, 2), padding='SAME', name='pool2')\n",
    "\n",
    "        net = FlattenLayer(net, name='flatten')\n",
    "        net = DenseLayer(net, 384, act=tf.nn.relu,\n",
    "                         W_init=W_init2, b_init=b_init2, name='d1relu')\n",
    "        net = DenseLayer(net, 192, act=tf.nn.relu,\n",
    "                         W_init=W_init2, b_init=b_init2, name='d2relu')\n",
    "        net = DenseLayer(net, n_units=10, act=None,\n",
    "                         W_init=W_init2, name='output')\n",
    "        y = net.outputs\n",
    "\n",
    "        ce = tl.cost.cross_entropy(y, y_, name='cost')\n",
    "\n",
    "        \"\"\" 需给后面的全连接层引入L2 normalization，惩罚模型的复杂度，避免overfitting \"\"\"\n",
    "        # L2 for the MLP, without this, the accuracy will be reduced by 15%.\n",
    "        L2 = 0\n",
    "        for p in tl.layers.get_variables_with_name('relu/W', True, True):\n",
    "            L2 += tf.contrib.layers.l2_regularizer(0.004)(p)\n",
    "        # 加上L2模型复杂度惩罚项后，得到最终真正的cost\n",
    "        cost = ce + L2\n",
    "\n",
    "        correct_prediction = tf.equal(tf.cast(tf.argmax(y, 1), tf.int32), y_)\n",
    "        acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        return net, cost, acc\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hvs-GYVshZfy",
    "colab_type": "text"
   },
   "source": [
    "## Build the model, and train it, test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "k1R8bM9-hY8Y",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70672.0
    },
    "outputId": "ebc5bb5c-fce5-4a9a-d857-41b26ba8c392"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape  (24, 24, 3)\ty_train.shape  ()\n",
      "x_train.dtype <dtype: 'float32'> , y_train.dtype <dtype: 'int32'>\n",
      "Tensor(\"per_image_standardization:0\", shape=(24, 24, 3), dtype=float32, device=/device:CPU:0)\n",
      "Tensor(\"Cast:0\", shape=(), dtype=int32, device=/device:CPU:0)\n",
      "x_test.shape  (24, 24, 3)\ty_test.shape  ()\n",
      "x_test.dtype <dtype: 'float32'> , y_test.dtype <dtype: 'int32'>\n",
      "Tensor(\"per_image_standardization_1:0\", shape=(24, 24, 3), dtype=float32, device=/device:CPU:0)\n",
      "Tensor(\"Cast_1:0\", shape=(), dtype=int32, device=/device:CPU:0)\n",
      "x_train_batch.shape  (128, 24, 24, 3)\ty_train_batch.shape  (128,)\n",
      "x_train_batch.dtype <dtype: 'float32'> , y_train_batch.dtype <dtype: 'int32'>\n",
      "Tensor(\"shuffle_batch:0\", shape=(128, 24, 24, 3), dtype=float32, device=/device:CPU:0)\n",
      "Tensor(\"shuffle_batch:1\", shape=(128,), dtype=int32, device=/device:CPU:0)\n",
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
      "img_batch   : (128, 24, 24, 3)\n",
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
      "label_batch : (128,)\n",
      "x_test_batch.shape  (128, 24, 24, 3)\ty_test_batch.shape  (128,)\n",
      "x_test_batch.dtype <dtype: 'float32'> , y_test_batch.dtype <dtype: 'int32'>\n",
      "Tensor(\"batch:0\", shape=(128, 24, 24, 3), dtype=float32, device=/device:CPU:0)\n",
      "Tensor(\"batch:1\", shape=(128,), dtype=int32, device=/device:CPU:0)\n",
      "[TL] InputLayer  model/input: (128, 24, 24, 3)\n",
      "[TL] Conv2d model/cnn1: n_filter: 64 filter_size: (5, 5) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer model/batch1: decay: 1.000000 epsilon: 0.000010 act: relu is_train: False\n",
      "[TL] MaxPool2d model/pool1: filter_size: (3, 3) strides: (2, 2) padding: SAME\n",
      "[TL] Conv2d model/cnn2: n_filter: 64 filter_size: (5, 5) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer model/batch2: decay: 1.000000 epsilon: 0.000010 act: relu is_train: False\n",
      "[TL] MaxPool2d model/pool2: filter_size: (3, 3) strides: (2, 2) padding: SAME\n",
      "[TL] FlattenLayer model/flatten: 2304\n",
      "[TL] DenseLayer  model/d1relu: 384 relu\n",
      "[TL] DenseLayer  model/d2relu: 192 relu\n",
      "[TL] DenseLayer  model/output: 10 No Activation\n",
      "[TL]   [*] geting variables with relu/W\n",
      "[TL]   got   0: model/d1relu/W:0   (2304, 384)\n",
      "[TL]   got   1: model/d2relu/W:0   (384, 192)\n",
      "[TL] InputLayer  model/input: (128, 24, 24, 3)\n",
      "[TL] Conv2d model/cnn1: n_filter: 64 filter_size: (5, 5) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer model/batch1: decay: 0.000000 epsilon: 0.000010 act: relu is_train: False\n",
      "[TL] MaxPool2d model/pool1: filter_size: (3, 3) strides: (2, 2) padding: SAME\n",
      "[TL] Conv2d model/cnn2: n_filter: 64 filter_size: (5, 5) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer model/batch2: decay: 0.000000 epsilon: 0.000010 act: relu is_train: False\n",
      "[TL] MaxPool2d model/pool2: filter_size: (3, 3) strides: (2, 2) padding: SAME\n",
      "[TL] FlattenLayer model/flatten: 2304\n",
      "[TL] DenseLayer  model/d1relu: 384 relu\n",
      "[TL] DenseLayer  model/d2relu: 192 relu\n",
      "[TL] DenseLayer  model/output: 10 No Activation\n",
      "[TL]   [*] geting variables with relu/W\n",
      "[TL]   got   0: model/d1relu/W:0   (2304, 384)\n",
      "[TL]   got   1: model/d2relu/W:0   (384, 192)\n",
      "[TL] WARNING: Function: `tensorlayer.layers.utils.initialize_global_variables` (in file: /usr/local/lib/python3.6/dist-packages/tensorlayer/layers/utils.py) is deprecated and will be removed after 2018-09-30.\n",
      "Instructions for updating: This API is deprecated in favor of `tf.global_variables_initializer`\n",
      "\n",
      "[TL]   param   0: model/cnn1/kernel:0  (5, 5, 3, 64)      float32_ref\n",
      "[TL]   param   1: model/batch1/beta:0  (64,)              float32_ref\n",
      "[TL]   param   2: model/batch1/gamma:0 (64,)              float32_ref\n",
      "[TL]   param   3: model/batch1/moving_mean:0 (64,)              float32_ref\n",
      "[TL]   param   4: model/batch1/moving_variance:0 (64,)              float32_ref\n",
      "[TL]   param   5: model/cnn2/kernel:0  (5, 5, 64, 64)     float32_ref\n",
      "[TL]   param   6: model/batch2/beta:0  (64,)              float32_ref\n",
      "[TL]   param   7: model/batch2/gamma:0 (64,)              float32_ref\n",
      "[TL]   param   8: model/batch2/moving_mean:0 (64,)              float32_ref\n",
      "[TL]   param   9: model/batch2/moving_variance:0 (64,)              float32_ref\n",
      "[TL]   param  10: model/d1relu/W:0     (2304, 384)        float32_ref\n",
      "[TL]   param  11: model/d1relu/b:0     (384,)             float32_ref\n",
      "[TL]   param  12: model/d2relu/W:0     (384, 192)         float32_ref\n",
      "[TL]   param  13: model/d2relu/b:0     (192,)             float32_ref\n",
      "[TL]   param  14: model/output/W:0     (192, 10)          float32_ref\n",
      "[TL]   param  15: model/output/b:0     (10,)              float32_ref\n",
      "[TL]   num of params: 1068682\n",
      "[TL]   layer   0: shuffle_batch:0      (128, 24, 24, 3)    float32\n",
      "[TL]   layer   1: model/cnn1/Conv2D:0  (128, 24, 24, 64)    float32\n",
      "[TL]   layer   2: model/batch1/Relu:0  (128, 24, 24, 64)    float32\n",
      "[TL]   layer   3: model/pool1/MaxPool:0 (128, 12, 12, 64)    float32\n",
      "[TL]   layer   4: model/cnn2/Conv2D:0  (128, 12, 12, 64)    float32\n",
      "[TL]   layer   5: model/batch2/Relu:0  (128, 12, 12, 64)    float32\n",
      "[TL]   layer   6: model/pool2/MaxPool:0 (128, 6, 6, 64)    float32\n",
      "[TL]   layer   7: model/flatten:0      (128, 2304)        float32\n",
      "[TL]   layer   8: model/d1relu/Relu:0  (128, 384)         float32\n",
      "[TL]   layer   9: model/d2relu/Relu:0  (128, 192)         float32\n",
      "[TL]   layer  10: model/output/bias_add:0 (128, 10)          float32\n",
      "   learning_rate: 0.000100\n",
      "   batch_size: 128\n",
      "   n_epoch: 50000, step in an epoch: 390, total n_step: 19500000\n",
      "Epoch 0 : Step 0-390 of 19500000 took 14.290147s\n",
      "   train loss: 3.566142\n",
      "   train acc: 0.339643\n",
      "   test loss: 2.898249\n",
      "   test acc: 0.461038\n",
      "Epoch 1 : Step 390-780 of 19500000 took 7.821692s\n",
      "   train loss: 2.745758\n",
      "   train acc: 0.443950\n",
      "   test loss: 2.442862\n",
      "   test acc: 0.500701\n",
      "Epoch 2 : Step 780-1170 of 19500000 took 7.833934s\n",
      "   train loss: 2.377191\n",
      "   train acc: 0.495853\n",
      "   test loss: 2.135397\n",
      "   test acc: 0.554688\n",
      "Epoch 3 : Step 1170-1560 of 19500000 took 7.776775s\n",
      "   train loss: 2.159861\n",
      "   train acc: 0.520733\n",
      "   test loss: 1.941702\n",
      "   test acc: 0.580529\n",
      "Epoch 4 : Step 1560-1950 of 19500000 took 7.802487s\n",
      "   train loss: 1.995858\n",
      "   train acc: 0.541346\n",
      "   test loss: 1.786162\n",
      "   test acc: 0.600260\n",
      "Epoch 5 : Step 1950-2340 of 19500000 took 7.774410s\n",
      "   train loss: 1.867088\n",
      "   train acc: 0.558474\n",
      "   test loss: 1.688634\n",
      "   test acc: 0.612380\n",
      "Epoch 6 : Step 2340-2730 of 19500000 took 7.800635s\n",
      "   train loss: 1.763838\n",
      "   train acc: 0.569511\n",
      "   test loss: 1.610781\n",
      "   test acc: 0.619992\n",
      "Epoch 7 : Step 2730-3120 of 19500000 took 7.880707s\n",
      "   train loss: 1.675510\n",
      "   train acc: 0.582452\n",
      "   test loss: 1.501025\n",
      "   test acc: 0.638522\n",
      "Epoch 8 : Step 3120-3510 of 19500000 took 7.815720s\n",
      "   train loss: 1.598136\n",
      "   train acc: 0.593930\n",
      "   test loss: 1.452884\n",
      "   test acc: 0.642027\n",
      "Epoch 9 : Step 3510-3900 of 19500000 took 7.829285s\n",
      "   train loss: 1.537278\n",
      "   train acc: 0.601462\n",
      "   test loss: 1.409689\n",
      "   test acc: 0.634315\n",
      "Epoch 10 : Step 3900-4290 of 19500000 took 7.799988s\n",
      "   train loss: 1.482128\n",
      "   train acc: 0.608794\n",
      "   test loss: 1.362455\n",
      "   test acc: 0.642027\n",
      "Epoch 11 : Step 4290-4680 of 19500000 took 7.846087s\n",
      "   train loss: 1.432179\n",
      "   train acc: 0.619451\n",
      "   test loss: 1.272290\n",
      "   test acc: 0.671575\n",
      "Epoch 12 : Step 4680-5070 of 19500000 took 7.856037s\n",
      "   train loss: 1.384505\n",
      "   train acc: 0.624399\n",
      "   test loss: 1.237142\n",
      "   test acc: 0.676683\n",
      "Epoch 13 : Step 5070-5460 of 19500000 took 7.818373s\n",
      "   train loss: 1.347213\n",
      "   train acc: 0.632111\n",
      "   test loss: 1.219299\n",
      "   test acc: 0.672776\n",
      "Epoch 14 : Step 5460-5850 of 19500000 took 7.858019s\n",
      "   train loss: 1.301871\n",
      "   train acc: 0.639543\n",
      "   test loss: 1.201176\n",
      "   test acc: 0.672175\n",
      "Epoch 15 : Step 5850-6240 of 19500000 took 7.919652s\n",
      "   train loss: 1.272937\n",
      "   train acc: 0.646675\n",
      "   test loss: 1.137115\n",
      "   test acc: 0.695212\n",
      "Epoch 16 : Step 6240-6630 of 19500000 took 7.789025s\n",
      "   train loss: 1.243519\n",
      "   train acc: 0.649740\n",
      "   test loss: 1.115012\n",
      "   test acc: 0.693810\n",
      "Epoch 17 : Step 6630-7020 of 19500000 took 7.847928s\n",
      "   train loss: 1.213315\n",
      "   train acc: 0.657472\n",
      "   test loss: 1.103263\n",
      "   test acc: 0.696514\n",
      "Epoch 18 : Step 7020-7410 of 19500000 took 7.796750s\n",
      "   train loss: 1.184365\n",
      "   train acc: 0.663061\n",
      "   test loss: 1.069697\n",
      "   test acc: 0.702925\n",
      "Epoch 19 : Step 7410-7800 of 19500000 took 7.809968s\n",
      "   train loss: 1.162205\n",
      "   train acc: 0.669010\n",
      "   test loss: 1.091893\n",
      "   test acc: 0.687500\n",
      "Epoch 20 : Step 7800-8190 of 19500000 took 7.840621s\n",
      "   train loss: 1.143938\n",
      "   train acc: 0.669912\n",
      "   test loss: 1.048392\n",
      "   test acc: 0.707131\n",
      "Epoch 21 : Step 8190-8580 of 19500000 took 7.848229s\n",
      "   train loss: 1.121045\n",
      "   train acc: 0.676302\n",
      "   test loss: 1.024176\n",
      "   test acc: 0.711338\n",
      "Epoch 22 : Step 8580-8970 of 19500000 took 7.899317s\n",
      "   train loss: 1.101710\n",
      "   train acc: 0.681591\n",
      "   test loss: 0.990983\n",
      "   test acc: 0.723257\n",
      "Epoch 23 : Step 8970-9360 of 19500000 took 7.840654s\n",
      "   train loss: 1.082644\n",
      "   train acc: 0.684696\n",
      "   test loss: 0.987159\n",
      "   test acc: 0.719351\n",
      "Epoch 24 : Step 9360-9750 of 19500000 took 7.858424s\n",
      "   train loss: 1.070516\n",
      "   train acc: 0.687520\n",
      "   test loss: 0.962386\n",
      "   test acc: 0.729067\n",
      "Epoch 25 : Step 9750-10140 of 19500000 took 7.817484s\n",
      "   train loss: 1.051809\n",
      "   train acc: 0.693169\n",
      "   test loss: 0.962063\n",
      "   test acc: 0.725160\n",
      "Epoch 26 : Step 10140-10530 of 19500000 took 7.795354s\n",
      "   train loss: 1.039307\n",
      "   train acc: 0.695092\n",
      "   test loss: 0.937535\n",
      "   test acc: 0.730469\n",
      "Epoch 27 : Step 10530-10920 of 19500000 took 7.839386s\n",
      "   train loss: 1.025066\n",
      "   train acc: 0.699579\n",
      "   test loss: 0.922035\n",
      "   test acc: 0.737480\n",
      "Epoch 28 : Step 10920-11310 of 19500000 took 7.869165s\n",
      "   train loss: 1.015586\n",
      "   train acc: 0.700621\n",
      "   test loss: 0.921419\n",
      "   test acc: 0.735276\n",
      "Epoch 29 : Step 11310-11700 of 19500000 took 7.862733s\n",
      "   train loss: 0.998493\n",
      "   train acc: 0.708313\n",
      "   test loss: 0.931739\n",
      "   test acc: 0.729768\n",
      "Epoch 30 : Step 11700-12090 of 19500000 took 7.960122s\n",
      "   train loss: 0.988894\n",
      "   train acc: 0.708694\n",
      "   test loss: 0.912688\n",
      "   test acc: 0.735377\n",
      "Epoch 31 : Step 12090-12480 of 19500000 took 7.809804s\n",
      "   train loss: 0.973628\n",
      "   train acc: 0.712640\n",
      "   test loss: 0.885022\n",
      "   test acc: 0.745593\n",
      "Epoch 32 : Step 12480-12870 of 19500000 took 7.797542s\n",
      "   train loss: 0.968711\n",
      "   train acc: 0.712640\n",
      "   test loss: 0.879153\n",
      "   test acc: 0.748998\n",
      "Epoch 33 : Step 12870-13260 of 19500000 took 7.804128s\n",
      "   train loss: 0.957309\n",
      "   train acc: 0.716386\n",
      "   test loss: 0.872512\n",
      "   test acc: 0.748798\n",
      "Epoch 34 : Step 13260-13650 of 19500000 took 7.888421s\n",
      "   train loss: 0.952386\n",
      "   train acc: 0.718229\n",
      "   test loss: 0.887627\n",
      "   test acc: 0.736478\n",
      "Epoch 35 : Step 13650-14040 of 19500000 took 7.891972s\n",
      "   train loss: 0.938473\n",
      "   train acc: 0.721975\n",
      "   test loss: 0.869776\n",
      "   test acc: 0.749800\n",
      "Epoch 36 : Step 14040-14430 of 19500000 took 7.807022s\n",
      "   train loss: 0.932393\n",
      "   train acc: 0.723337\n",
      "   test loss: 0.867208\n",
      "   test acc: 0.750000\n",
      "Epoch 37 : Step 14430-14820 of 19500000 took 7.782058s\n",
      "   train loss: 0.927075\n",
      "   train acc: 0.725300\n",
      "   test loss: 0.855998\n",
      "   test acc: 0.750200\n",
      "Epoch 38 : Step 14820-15210 of 19500000 took 7.787321s\n",
      "   train loss: 0.917922\n",
      "   train acc: 0.728466\n",
      "   test loss: 0.858650\n",
      "   test acc: 0.746895\n",
      "Epoch 39 : Step 15210-15600 of 19500000 took 7.801118s\n",
      "   train loss: 0.912155\n",
      "   train acc: 0.726803\n",
      "   test loss: 0.824995\n",
      "   test acc: 0.760317\n",
      "Epoch 40 : Step 15600-15990 of 19500000 took 7.845811s\n",
      "   train loss: 0.897236\n",
      "   train acc: 0.734575\n",
      "   test loss: 0.829113\n",
      "   test acc: 0.756911\n",
      "Epoch 41 : Step 15990-16380 of 19500000 took 7.902796s\n",
      "   train loss: 0.898162\n",
      "   train acc: 0.730950\n",
      "   test loss: 0.831675\n",
      "   test acc: 0.753706\n",
      "Epoch 42 : Step 16380-16770 of 19500000 took 7.902551s\n",
      "   train loss: 0.889076\n",
      "   train acc: 0.735677\n",
      "   test loss: 0.811715\n",
      "   test acc: 0.763922\n",
      "Epoch 43 : Step 16770-17160 of 19500000 took 7.822614s\n",
      "   train loss: 0.876954\n",
      "   train acc: 0.739784\n",
      "   test loss: 0.804257\n",
      "   test acc: 0.767929\n",
      "Epoch 44 : Step 17160-17550 of 19500000 took 7.848814s\n",
      "   train loss: 0.870998\n",
      "   train acc: 0.740645\n",
      "   test loss: 0.818822\n",
      "   test acc: 0.761719\n",
      "Epoch 45 : Step 17550-17940 of 19500000 took 7.836352s\n",
      "   train loss: 0.868993\n",
      "   train acc: 0.740385\n",
      "   test loss: 0.801908\n",
      "   test acc: 0.768730\n",
      "Epoch 46 : Step 17940-18330 of 19500000 took 7.830367s\n",
      "   train loss: 0.869126\n",
      "   train acc: 0.740725\n",
      "   test loss: 0.796739\n",
      "   test acc: 0.769231\n",
      "Epoch 47 : Step 18330-18720 of 19500000 took 7.918839s\n",
      "   train loss: 0.852253\n",
      "   train acc: 0.746034\n",
      "   test loss: 0.803346\n",
      "   test acc: 0.766727\n",
      "Epoch 48 : Step 18720-19110 of 19500000 took 7.867721s\n",
      "   train loss: 0.849047\n",
      "   train acc: 0.748217\n",
      "   test loss: 0.789954\n",
      "   test acc: 0.770733\n",
      "Epoch 49 : Step 19110-19500 of 19500000 took 7.857518s\n",
      "   train loss: 0.849180\n",
      "   train acc: 0.748417\n",
      "   test loss: 0.792386\n",
      "   test acc: 0.768530\n",
      "Save model !!!!!!!!!!\n",
      "[TL] [*] Saving TL params into model.npz\n",
      "[TL] [*] Saved\n",
      "Epoch 50 : Step 19500-19890 of 19500000 took 7.813400s\n",
      "   train loss: 0.839022\n",
      "   train acc: 0.750441\n",
      "   test loss: 0.786643\n",
      "   test acc: 0.769832\n",
      "Epoch 51 : Step 19890-20280 of 19500000 took 7.821837s\n",
      "   train loss: 0.834111\n",
      "   train acc: 0.750921\n",
      "   test loss: 0.779235\n",
      "   test acc: 0.773237\n",
      "Epoch 52 : Step 20280-20670 of 19500000 took 7.850198s\n",
      "   train loss: 0.830469\n",
      "   train acc: 0.752344\n",
      "   test loss: 0.774128\n",
      "   test acc: 0.775942\n",
      "Epoch 53 : Step 20670-21060 of 19500000 took 7.813007s\n",
      "   train loss: 0.826487\n",
      "   train acc: 0.754347\n",
      "   test loss: 0.763851\n",
      "   test acc: 0.781350\n",
      "Epoch 54 : Step 21060-21450 of 19500000 took 7.902426s\n",
      "   train loss: 0.818088\n",
      "   train acc: 0.756631\n",
      "   test loss: 0.784136\n",
      "   test acc: 0.772736\n",
      "Epoch 55 : Step 21450-21840 of 19500000 took 7.868757s\n",
      "   train loss: 0.815906\n",
      "   train acc: 0.759756\n",
      "   test loss: 0.757886\n",
      "   test acc: 0.780950\n",
      "Epoch 56 : Step 21840-22230 of 19500000 took 7.791600s\n",
      "   train loss: 0.809121\n",
      "   train acc: 0.760978\n",
      "   test loss: 0.774352\n",
      "   test acc: 0.771735\n",
      "Epoch 57 : Step 22230-22620 of 19500000 took 7.873916s\n",
      "   train loss: 0.808365\n",
      "   train acc: 0.760216\n",
      "   test loss: 0.762877\n",
      "   test acc: 0.777744\n",
      "Epoch 58 : Step 22620-23010 of 19500000 took 7.817211s\n",
      "   train loss: 0.806087\n",
      "   train acc: 0.761178\n",
      "   test loss: 0.752129\n",
      "   test acc: 0.778245\n",
      "Epoch 59 : Step 23010-23400 of 19500000 took 7.815448s\n",
      "   train loss: 0.800492\n",
      "   train acc: 0.763141\n",
      "   test loss: 0.764334\n",
      "   test acc: 0.776242\n",
      "Epoch 60 : Step 23400-23790 of 19500000 took 7.837489s\n",
      "   train loss: 0.798296\n",
      "   train acc: 0.762941\n",
      "   test loss: 0.750520\n",
      "   test acc: 0.781550\n",
      "Epoch 61 : Step 23790-24180 of 19500000 took 7.816495s\n",
      "   train loss: 0.794605\n",
      "   train acc: 0.765905\n",
      "   test loss: 0.746681\n",
      "   test acc: 0.784455\n",
      "Epoch 62 : Step 24180-24570 of 19500000 took 7.856062s\n",
      "   train loss: 0.793234\n",
      "   train acc: 0.763982\n",
      "   test loss: 0.754144\n",
      "   test acc: 0.780148\n",
      "Epoch 63 : Step 24570-24960 of 19500000 took 7.934903s\n",
      "   train loss: 0.790444\n",
      "   train acc: 0.767488\n",
      "   test loss: 0.759418\n",
      "   test acc: 0.781350\n",
      "Epoch 64 : Step 24960-25350 of 19500000 took 7.833100s\n",
      "   train loss: 0.776419\n",
      "   train acc: 0.771334\n",
      "   test loss: 0.753610\n",
      "   test acc: 0.782151\n",
      "Epoch 65 : Step 25350-25740 of 19500000 took 7.847472s\n",
      "   train loss: 0.779366\n",
      "   train acc: 0.769732\n",
      "   test loss: 0.761512\n",
      "   test acc: 0.779347\n",
      "Epoch 66 : Step 25740-26130 of 19500000 took 7.843718s\n",
      "   train loss: 0.772161\n",
      "   train acc: 0.771174\n",
      "   test loss: 0.745698\n",
      "   test acc: 0.786759\n",
      "Epoch 67 : Step 26130-26520 of 19500000 took 7.833020s\n",
      "   train loss: 0.764794\n",
      "   train acc: 0.776302\n",
      "   test loss: 0.741795\n",
      "   test acc: 0.781951\n",
      "Epoch 68 : Step 26520-26910 of 19500000 took 7.807211s\n",
      "   train loss: 0.764692\n",
      "   train acc: 0.775841\n",
      "   test loss: 0.740744\n",
      "   test acc: 0.784054\n",
      "Epoch 69 : Step 26910-27300 of 19500000 took 7.854607s\n",
      "   train loss: 0.764913\n",
      "   train acc: 0.777244\n",
      "   test loss: 0.752211\n",
      "   test acc: 0.782352\n",
      "Epoch 70 : Step 27300-27690 of 19500000 took 7.777186s\n",
      "   train loss: 0.761798\n",
      "   train acc: 0.775441\n",
      "   test loss: 0.740405\n",
      "   test acc: 0.784555\n",
      "Epoch 71 : Step 27690-28080 of 19500000 took 7.826063s\n",
      "   train loss: 0.754947\n",
      "   train acc: 0.778105\n",
      "   test loss: 0.730038\n",
      "   test acc: 0.789062\n",
      "Epoch 72 : Step 28080-28470 of 19500000 took 7.824033s\n",
      "   train loss: 0.753063\n",
      "   train acc: 0.779307\n",
      "   test loss: 0.750191\n",
      "   test acc: 0.776242\n",
      "Epoch 73 : Step 28470-28860 of 19500000 took 7.874467s\n",
      "   train loss: 0.752091\n",
      "   train acc: 0.780829\n",
      "   test loss: 0.740880\n",
      "   test acc: 0.785757\n",
      "Epoch 74 : Step 28860-29250 of 19500000 took 7.882562s\n",
      "   train loss: 0.749646\n",
      "   train acc: 0.781030\n",
      "   test loss: 0.720595\n",
      "   test acc: 0.790765\n",
      "Epoch 75 : Step 29250-29640 of 19500000 took 7.857342s\n",
      "   train loss: 0.746018\n",
      "   train acc: 0.781851\n",
      "   test loss: 0.723245\n",
      "   test acc: 0.790064\n",
      "Epoch 76 : Step 29640-30030 of 19500000 took 7.835821s\n",
      "   train loss: 0.739861\n",
      "   train acc: 0.784034\n",
      "   test loss: 0.733165\n",
      "   test acc: 0.786859\n",
      "Epoch 77 : Step 30030-30420 of 19500000 took 7.795921s\n",
      "   train loss: 0.738130\n",
      "   train acc: 0.784175\n",
      "   test loss: 0.728361\n",
      "   test acc: 0.788261\n",
      "Epoch 78 : Step 30420-30810 of 19500000 took 7.838533s\n",
      "   train loss: 0.735834\n",
      "   train acc: 0.784415\n",
      "   test loss: 0.737345\n",
      "   test acc: 0.789864\n",
      "Epoch 79 : Step 30810-31200 of 19500000 took 7.853929s\n",
      "   train loss: 0.733987\n",
      "   train acc: 0.784936\n",
      "   test loss: 0.718521\n",
      "   test acc: 0.794171\n",
      "Epoch 80 : Step 31200-31590 of 19500000 took 7.848983s\n",
      "   train loss: 0.730048\n",
      "   train acc: 0.787059\n",
      "   test loss: 0.725650\n",
      "   test acc: 0.789062\n",
      "Epoch 81 : Step 31590-31980 of 19500000 took 7.890296s\n",
      "   train loss: 0.724381\n",
      "   train acc: 0.790405\n",
      "   test loss: 0.712968\n",
      "   test acc: 0.793970\n",
      "Epoch 82 : Step 31980-32370 of 19500000 took 7.926126s\n",
      "   train loss: 0.719612\n",
      "   train acc: 0.790445\n",
      "   test loss: 0.720883\n",
      "   test acc: 0.789964\n",
      "Epoch 83 : Step 32370-32760 of 19500000 took 7.825338s\n",
      "   train loss: 0.719195\n",
      "   train acc: 0.793189\n",
      "   test loss: 0.761956\n",
      "   test acc: 0.773337\n",
      "Epoch 84 : Step 32760-33150 of 19500000 took 7.887960s\n",
      "   train loss: 0.719478\n",
      "   train acc: 0.792508\n",
      "   test loss: 0.749290\n",
      "   test acc: 0.783954\n",
      "Epoch 85 : Step 33150-33540 of 19500000 took 7.774018s\n",
      "   train loss: 0.714838\n",
      "   train acc: 0.793369\n",
      "   test loss: 0.699487\n",
      "   test acc: 0.793970\n",
      "Epoch 86 : Step 33540-33930 of 19500000 took 7.818958s\n",
      "   train loss: 0.713223\n",
      "   train acc: 0.792468\n",
      "   test loss: 0.726976\n",
      "   test acc: 0.792568\n",
      "Epoch 87 : Step 33930-34320 of 19500000 took 7.881605s\n",
      "   train loss: 0.712683\n",
      "   train acc: 0.792748\n",
      "   test loss: 0.716751\n",
      "   test acc: 0.794171\n",
      "Epoch 88 : Step 34320-34710 of 19500000 took 7.822587s\n",
      "   train loss: 0.708168\n",
      "   train acc: 0.794952\n",
      "   test loss: 0.700936\n",
      "   test acc: 0.794671\n",
      "Epoch 89 : Step 34710-35100 of 19500000 took 7.867370s\n",
      "   train loss: 0.703267\n",
      "   train acc: 0.796394\n",
      "   test loss: 0.693044\n",
      "   test acc: 0.800080\n",
      "Epoch 90 : Step 35100-35490 of 19500000 took 7.793344s\n",
      "   train loss: 0.707769\n",
      "   train acc: 0.794231\n",
      "   test loss: 0.708878\n",
      "   test acc: 0.793670\n",
      "Epoch 91 : Step 35490-35880 of 19500000 took 7.812419s\n",
      "   train loss: 0.701993\n",
      "   train acc: 0.798277\n",
      "   test loss: 0.721056\n",
      "   test acc: 0.791266\n",
      "Epoch 92 : Step 35880-36270 of 19500000 took 7.784708s\n",
      "   train loss: 0.700017\n",
      "   train acc: 0.796534\n",
      "   test loss: 0.700891\n",
      "   test acc: 0.798878\n",
      "Epoch 93 : Step 36270-36660 of 19500000 took 7.777465s\n",
      "   train loss: 0.692360\n",
      "   train acc: 0.800681\n",
      "   test loss: 0.693528\n",
      "   test acc: 0.802384\n",
      "Epoch 94 : Step 36660-37050 of 19500000 took 7.826829s\n",
      "   train loss: 0.694918\n",
      "   train acc: 0.801302\n",
      "   test loss: 0.723381\n",
      "   test acc: 0.788662\n",
      "Epoch 95 : Step 37050-37440 of 19500000 took 7.836845s\n",
      "   train loss: 0.691089\n",
      "   train acc: 0.799058\n",
      "   test loss: 0.719424\n",
      "   test acc: 0.792268\n",
      "Epoch 96 : Step 37440-37830 of 19500000 took 7.867600s\n",
      "   train loss: 0.685878\n",
      "   train acc: 0.802644\n",
      "   test loss: 0.721182\n",
      "   test acc: 0.791266\n",
      "Epoch 97 : Step 37830-38220 of 19500000 took 7.826293s\n",
      "   train loss: 0.683200\n",
      "   train acc: 0.805188\n",
      "   test loss: 0.693534\n",
      "   test acc: 0.806490\n",
      "Epoch 98 : Step 38220-38610 of 19500000 took 7.835367s\n",
      "   train loss: 0.688442\n",
      "   train acc: 0.801562\n",
      "   test loss: 0.694977\n",
      "   test acc: 0.801182\n",
      "Epoch 99 : Step 38610-39000 of 19500000 took 7.813482s\n",
      "   train loss: 0.677048\n",
      "   train acc: 0.807412\n",
      "   test loss: 0.725889\n",
      "   test acc: 0.790765\n",
      "Save model !!!!!!!!!!\n",
      "[TL] [*] Saving TL params into model.npz\n",
      "[TL] [*] Saved\n",
      "Epoch 100 : Step 39000-39390 of 19500000 took 7.775095s\n",
      "   train loss: 0.676549\n",
      "   train acc: 0.805349\n",
      "   test loss: 0.690714\n",
      "   test acc: 0.801583\n",
      "Epoch 101 : Step 39390-39780 of 19500000 took 7.836342s\n",
      "   train loss: 0.678740\n",
      "   train acc: 0.805108\n",
      "   test loss: 0.701538\n",
      "   test acc: 0.798177\n",
      "Epoch 102 : Step 39780-40170 of 19500000 took 7.804421s\n",
      "   train loss: 0.669873\n",
      "   train acc: 0.808574\n",
      "   test loss: 0.703922\n",
      "   test acc: 0.800681\n",
      "Epoch 103 : Step 40170-40560 of 19500000 took 7.830713s\n",
      "   train loss: 0.677317\n",
      "   train acc: 0.805549\n",
      "   test loss: 0.685463\n",
      "   test acc: 0.806891\n",
      "Epoch 104 : Step 40560-40950 of 19500000 took 7.767043s\n",
      "   train loss: 0.672032\n",
      "   train acc: 0.807953\n",
      "   test loss: 0.680950\n",
      "   test acc: 0.805389\n",
      "Epoch 105 : Step 40950-41340 of 19500000 took 7.782336s\n",
      "   train loss: 0.667802\n",
      "   train acc: 0.810877\n",
      "   test loss: 0.704951\n",
      "   test acc: 0.798678\n",
      "Epoch 106 : Step 41340-41730 of 19500000 took 7.836197s\n",
      "   train loss: 0.668861\n",
      "   train acc: 0.809756\n",
      "   test loss: 0.687239\n",
      "   test acc: 0.803486\n",
      "Epoch 107 : Step 41730-42120 of 19500000 took 7.786877s\n",
      "   train loss: 0.666908\n",
      "   train acc: 0.810717\n",
      "   test loss: 0.713703\n",
      "   test acc: 0.798678\n",
      "Epoch 108 : Step 42120-42510 of 19500000 took 7.848858s\n",
      "   train loss: 0.664063\n",
      "   train acc: 0.810637\n",
      "   test loss: 0.690070\n",
      "   test acc: 0.803385\n",
      "Epoch 109 : Step 42510-42900 of 19500000 took 7.836386s\n",
      "   train loss: 0.661183\n",
      "   train acc: 0.810357\n",
      "   test loss: 0.699797\n",
      "   test acc: 0.798478\n",
      "Epoch 110 : Step 42900-43290 of 19500000 took 7.780879s\n",
      "   train loss: 0.662690\n",
      "   train acc: 0.810317\n",
      "   test loss: 0.702773\n",
      "   test acc: 0.798878\n",
      "Epoch 111 : Step 43290-43680 of 19500000 took 7.785024s\n",
      "   train loss: 0.661136\n",
      "   train acc: 0.811518\n",
      "   test loss: 0.696579\n",
      "   test acc: 0.801883\n",
      "Epoch 112 : Step 43680-44070 of 19500000 took 7.771912s\n",
      "   train loss: 0.656828\n",
      "   train acc: 0.814243\n",
      "   test loss: 0.691853\n",
      "   test acc: 0.804187\n",
      "Epoch 113 : Step 44070-44460 of 19500000 took 7.809291s\n",
      "   train loss: 0.651667\n",
      "   train acc: 0.815585\n",
      "   test loss: 0.699545\n",
      "   test acc: 0.801683\n",
      "Epoch 114 : Step 44460-44850 of 19500000 took 7.835029s\n",
      "   train loss: 0.658264\n",
      "   train acc: 0.812520\n",
      "   test loss: 0.693534\n",
      "   test acc: 0.798678\n",
      "Epoch 115 : Step 44850-45240 of 19500000 took 7.823953s\n",
      "   train loss: 0.651269\n",
      "   train acc: 0.815104\n",
      "   test loss: 0.684199\n",
      "   test acc: 0.807292\n",
      "Epoch 116 : Step 45240-45630 of 19500000 took 7.917714s\n",
      "   train loss: 0.649722\n",
      "   train acc: 0.815284\n",
      "   test loss: 0.678033\n",
      "   test acc: 0.809395\n",
      "Epoch 117 : Step 45630-46020 of 19500000 took 7.880069s\n",
      "   train loss: 0.645340\n",
      "   train acc: 0.818790\n",
      "   test loss: 0.684922\n",
      "   test acc: 0.806591\n",
      "Epoch 118 : Step 46020-46410 of 19500000 took 7.924464s\n",
      "   train loss: 0.643943\n",
      "   train acc: 0.818630\n",
      "   test loss: 0.690597\n",
      "   test acc: 0.805288\n",
      "Epoch 119 : Step 46410-46800 of 19500000 took 7.944484s\n",
      "   train loss: 0.649567\n",
      "   train acc: 0.815024\n",
      "   test loss: 0.695039\n",
      "   test acc: 0.802985\n",
      "Epoch 120 : Step 46800-47190 of 19500000 took 7.904624s\n",
      "   train loss: 0.646748\n",
      "   train acc: 0.816206\n",
      "   test loss: 0.691667\n",
      "   test acc: 0.801683\n",
      "Epoch 121 : Step 47190-47580 of 19500000 took 7.980242s\n",
      "   train loss: 0.645402\n",
      "   train acc: 0.817208\n",
      "   test loss: 0.694495\n",
      "   test acc: 0.805188\n",
      "Epoch 122 : Step 47580-47970 of 19500000 took 7.886037s\n",
      "   train loss: 0.642396\n",
      "   train acc: 0.817147\n",
      "   test loss: 0.671889\n",
      "   test acc: 0.809796\n",
      "Epoch 123 : Step 47970-48360 of 19500000 took 7.907306s\n",
      "   train loss: 0.640619\n",
      "   train acc: 0.820252\n",
      "   test loss: 0.689194\n",
      "   test acc: 0.803385\n",
      "Epoch 124 : Step 48360-48750 of 19500000 took 7.888659s\n",
      "   train loss: 0.632671\n",
      "   train acc: 0.823598\n",
      "   test loss: 0.688156\n",
      "   test acc: 0.803385\n",
      "Epoch 125 : Step 48750-49140 of 19500000 took 7.901644s\n",
      "   train loss: 0.635245\n",
      "   train acc: 0.822155\n",
      "   test loss: 0.676555\n",
      "   test acc: 0.808594\n",
      "Epoch 126 : Step 49140-49530 of 19500000 took 7.892745s\n",
      "   train loss: 0.638618\n",
      "   train acc: 0.821154\n",
      "   test loss: 0.686519\n",
      "   test acc: 0.808393\n",
      "Epoch 127 : Step 49530-49920 of 19500000 took 7.910955s\n",
      "   train loss: 0.630730\n",
      "   train acc: 0.822937\n",
      "   test loss: 0.681649\n",
      "   test acc: 0.808293\n",
      "Epoch 128 : Step 49920-50310 of 19500000 took 7.915644s\n",
      "   train loss: 0.635720\n",
      "   train acc: 0.820933\n",
      "   test loss: 0.682243\n",
      "   test acc: 0.807392\n",
      "Epoch 129 : Step 50310-50700 of 19500000 took 7.887466s\n",
      "   train loss: 0.628083\n",
      "   train acc: 0.823197\n",
      "   test loss: 0.684998\n",
      "   test acc: 0.807993\n",
      "Epoch 130 : Step 50700-51090 of 19500000 took 7.935328s\n",
      "   train loss: 0.625017\n",
      "   train acc: 0.825861\n",
      "   test loss: 0.690126\n",
      "   test acc: 0.807292\n",
      "Epoch 131 : Step 51090-51480 of 19500000 took 7.834583s\n",
      "   train loss: 0.632170\n",
      "   train acc: 0.822877\n",
      "   test loss: 0.664818\n",
      "   test acc: 0.815605\n",
      "Epoch 132 : Step 51480-51870 of 19500000 took 7.905856s\n",
      "   train loss: 0.628081\n",
      "   train acc: 0.823538\n",
      "   test loss: 0.689756\n",
      "   test acc: 0.804187\n",
      "Epoch 133 : Step 51870-52260 of 19500000 took 7.811324s\n",
      "   train loss: 0.620391\n",
      "   train acc: 0.826783\n",
      "   test loss: 0.680772\n",
      "   test acc: 0.807592\n",
      "Epoch 134 : Step 52260-52650 of 19500000 took 7.895109s\n",
      "   train loss: 0.624256\n",
      "   train acc: 0.826122\n",
      "   test loss: 0.699581\n",
      "   test acc: 0.801482\n",
      "Epoch 135 : Step 52650-53040 of 19500000 took 7.819033s\n",
      "   train loss: 0.619318\n",
      "   train acc: 0.826222\n",
      "   test loss: 0.662237\n",
      "   test acc: 0.811098\n",
      "Epoch 136 : Step 53040-53430 of 19500000 took 7.829144s\n",
      "   train loss: 0.619239\n",
      "   train acc: 0.828125\n",
      "   test loss: 0.667474\n",
      "   test acc: 0.814804\n",
      "Epoch 137 : Step 53430-53820 of 19500000 took 7.836113s\n",
      "   train loss: 0.611918\n",
      "   train acc: 0.829487\n",
      "   test loss: 0.662744\n",
      "   test acc: 0.814403\n",
      "Epoch 138 : Step 53820-54210 of 19500000 took 7.747221s\n",
      "   train loss: 0.617567\n",
      "   train acc: 0.826542\n",
      "   test loss: 0.692027\n",
      "   test acc: 0.803486\n",
      "Epoch 139 : Step 54210-54600 of 19500000 took 7.818828s\n",
      "   train loss: 0.608735\n",
      "   train acc: 0.828646\n",
      "   test loss: 0.665989\n",
      "   test acc: 0.810196\n",
      "Epoch 140 : Step 54600-54990 of 19500000 took 7.778113s\n",
      "   train loss: 0.613028\n",
      "   train acc: 0.828285\n",
      "   test loss: 0.664215\n",
      "   test acc: 0.815004\n",
      "Epoch 141 : Step 54990-55380 of 19500000 took 7.816059s\n",
      "   train loss: 0.612933\n",
      "   train acc: 0.828906\n",
      "   test loss: 0.681496\n",
      "   test acc: 0.808393\n",
      "Epoch 142 : Step 55380-55770 of 19500000 took 7.803243s\n",
      "   train loss: 0.610750\n",
      "   train acc: 0.831210\n",
      "   test loss: 0.677714\n",
      "   test acc: 0.809395\n",
      "Epoch 143 : Step 55770-56160 of 19500000 took 7.771248s\n",
      "   train loss: 0.613490\n",
      "   train acc: 0.829587\n",
      "   test loss: 0.668492\n",
      "   test acc: 0.812300\n",
      "Epoch 144 : Step 56160-56550 of 19500000 took 7.811123s\n",
      "   train loss: 0.607184\n",
      "   train acc: 0.832131\n",
      "   test loss: 0.673099\n",
      "   test acc: 0.812800\n",
      "Epoch 145 : Step 56550-56940 of 19500000 took 7.820225s\n",
      "   train loss: 0.606432\n",
      "   train acc: 0.831911\n",
      "   test loss: 0.660990\n",
      "   test acc: 0.817408\n",
      "Epoch 146 : Step 56940-57330 of 19500000 took 7.823040s\n",
      "   train loss: 0.606767\n",
      "   train acc: 0.830288\n",
      "   test loss: 0.676180\n",
      "   test acc: 0.810296\n",
      "Epoch 147 : Step 57330-57720 of 19500000 took 7.786799s\n",
      "   train loss: 0.604821\n",
      "   train acc: 0.830308\n",
      "   test loss: 0.680788\n",
      "   test acc: 0.811298\n",
      "Epoch 148 : Step 57720-58110 of 19500000 took 7.838810s\n",
      "   train loss: 0.604419\n",
      "   train acc: 0.834175\n",
      "   test loss: 0.668889\n",
      "   test acc: 0.814203\n",
      "Epoch 149 : Step 58110-58500 of 19500000 took 7.871610s\n",
      "   train loss: 0.606420\n",
      "   train acc: 0.830789\n",
      "   test loss: 0.676458\n",
      "   test acc: 0.808293\n",
      "Save model !!!!!!!!!!\n",
      "[TL] [*] Saving TL params into model.npz\n",
      "[TL] [*] Saved\n",
      "Epoch 150 : Step 58500-58890 of 19500000 took 7.811251s\n",
      "   train loss: 0.600845\n",
      "   train acc: 0.834375\n",
      "   test loss: 0.678794\n",
      "   test acc: 0.813702\n",
      "Epoch 151 : Step 58890-59280 of 19500000 took 7.771862s\n",
      "   train loss: 0.603383\n",
      "   train acc: 0.830950\n",
      "   test loss: 0.666988\n",
      "   test acc: 0.812500\n",
      "Epoch 152 : Step 59280-59670 of 19500000 took 7.812979s\n",
      "   train loss: 0.597174\n",
      "   train acc: 0.835757\n",
      "   test loss: 0.669410\n",
      "   test acc: 0.814804\n",
      "Epoch 153 : Step 59670-60060 of 19500000 took 7.832513s\n",
      "   train loss: 0.595651\n",
      "   train acc: 0.835337\n",
      "   test loss: 0.685941\n",
      "   test acc: 0.810296\n",
      "Epoch 154 : Step 60060-60450 of 19500000 took 7.815905s\n",
      "   train loss: 0.598650\n",
      "   train acc: 0.834575\n",
      "   test loss: 0.690553\n",
      "   test acc: 0.806891\n",
      "Epoch 155 : Step 60450-60840 of 19500000 took 8.180665s\n",
      "   train loss: 0.590459\n",
      "   train acc: 0.838982\n",
      "   test loss: 0.657602\n",
      "   test acc: 0.817508\n",
      "Epoch 156 : Step 60840-61230 of 19500000 took 7.832942s\n",
      "   train loss: 0.593346\n",
      "   train acc: 0.835377\n",
      "   test loss: 0.669294\n",
      "   test acc: 0.812800\n",
      "Epoch 157 : Step 61230-61620 of 19500000 took 7.775618s\n",
      "   train loss: 0.599007\n",
      "   train acc: 0.835276\n",
      "   test loss: 0.656734\n",
      "   test acc: 0.820713\n",
      "Epoch 158 : Step 61620-62010 of 19500000 took 7.771168s\n",
      "   train loss: 0.592468\n",
      "   train acc: 0.837861\n",
      "   test loss: 0.682946\n",
      "   test acc: 0.814804\n",
      "Epoch 159 : Step 62010-62400 of 19500000 took 7.785909s\n",
      "   train loss: 0.591994\n",
      "   train acc: 0.836779\n",
      "   test loss: 0.683773\n",
      "   test acc: 0.808093\n",
      "Epoch 160 : Step 62400-62790 of 19500000 took 7.796050s\n",
      "   train loss: 0.592814\n",
      "   train acc: 0.836198\n",
      "   test loss: 0.704548\n",
      "   test acc: 0.802484\n",
      "Epoch 161 : Step 62790-63180 of 19500000 took 7.819417s\n",
      "   train loss: 0.590470\n",
      "   train acc: 0.834635\n",
      "   test loss: 0.673591\n",
      "   test acc: 0.814002\n",
      "Epoch 162 : Step 63180-63570 of 19500000 took 7.810699s\n",
      "   train loss: 0.588861\n",
      "   train acc: 0.837139\n",
      "   test loss: 0.673020\n",
      "   test acc: 0.811899\n",
      "Epoch 163 : Step 63570-63960 of 19500000 took 7.890919s\n",
      "   train loss: 0.581029\n",
      "   train acc: 0.841827\n",
      "   test loss: 0.684134\n",
      "   test acc: 0.810998\n",
      "Epoch 164 : Step 63960-64350 of 19500000 took 7.846304s\n",
      "   train loss: 0.582274\n",
      "   train acc: 0.842528\n",
      "   test loss: 0.661526\n",
      "   test acc: 0.817007\n",
      "Epoch 165 : Step 64350-64740 of 19500000 took 7.770569s\n",
      "   train loss: 0.581562\n",
      "   train acc: 0.841867\n",
      "   test loss: 0.656063\n",
      "   test acc: 0.819611\n",
      "Epoch 166 : Step 64740-65130 of 19500000 took 7.780710s\n",
      "   train loss: 0.583071\n",
      "   train acc: 0.839663\n",
      "   test loss: 0.666142\n",
      "   test acc: 0.814704\n",
      "Epoch 167 : Step 65130-65520 of 19500000 took 7.779106s\n",
      "   train loss: 0.580768\n",
      "   train acc: 0.840545\n",
      "   test loss: 0.684605\n",
      "   test acc: 0.808894\n",
      "Epoch 168 : Step 65520-65910 of 19500000 took 7.821757s\n",
      "   train loss: 0.579460\n",
      "   train acc: 0.842528\n",
      "   test loss: 0.670242\n",
      "   test acc: 0.815605\n",
      "Epoch 169 : Step 65910-66300 of 19500000 took 7.797671s\n",
      "   train loss: 0.585356\n",
      "   train acc: 0.838542\n",
      "   test loss: 0.684786\n",
      "   test acc: 0.809395\n",
      "Epoch 170 : Step 66300-66690 of 19500000 took 7.789423s\n",
      "   train loss: 0.580020\n",
      "   train acc: 0.841106\n",
      "   test loss: 0.672260\n",
      "   test acc: 0.811098\n",
      "Epoch 171 : Step 66690-67080 of 19500000 took 7.806930s\n",
      "   train loss: 0.578697\n",
      "   train acc: 0.841847\n",
      "   test loss: 0.672732\n",
      "   test acc: 0.812200\n",
      "Epoch 172 : Step 67080-67470 of 19500000 took 7.754428s\n",
      "   train loss: 0.582680\n",
      "   train acc: 0.840585\n",
      "   test loss: 0.664147\n",
      "   test acc: 0.815705\n",
      "Epoch 173 : Step 67470-67860 of 19500000 took 7.792114s\n",
      "   train loss: 0.579003\n",
      "   train acc: 0.841927\n",
      "   test loss: 0.691145\n",
      "   test acc: 0.808193\n",
      "Epoch 174 : Step 67860-68250 of 19500000 took 7.788209s\n",
      "   train loss: 0.569935\n",
      "   train acc: 0.846154\n",
      "   test loss: 0.662381\n",
      "   test acc: 0.819010\n",
      "Epoch 175 : Step 68250-68640 of 19500000 took 7.829050s\n",
      "   train loss: 0.574349\n",
      "   train acc: 0.843570\n",
      "   test loss: 0.671451\n",
      "   test acc: 0.813802\n",
      "Epoch 176 : Step 68640-69030 of 19500000 took 7.938439s\n",
      "   train loss: 0.573248\n",
      "   train acc: 0.844211\n",
      "   test loss: 0.653060\n",
      "   test acc: 0.819511\n",
      "Epoch 177 : Step 69030-69420 of 19500000 took 7.829314s\n",
      "   train loss: 0.572847\n",
      "   train acc: 0.842688\n",
      "   test loss: 0.675001\n",
      "   test acc: 0.813101\n",
      "Epoch 178 : Step 69420-69810 of 19500000 took 7.807370s\n",
      "   train loss: 0.569737\n",
      "   train acc: 0.844972\n",
      "   test loss: 0.671675\n",
      "   test acc: 0.817208\n",
      "Epoch 179 : Step 69810-70200 of 19500000 took 7.805981s\n",
      "   train loss: 0.565894\n",
      "   train acc: 0.846354\n",
      "   test loss: 0.664791\n",
      "   test acc: 0.818009\n",
      "Epoch 180 : Step 70200-70590 of 19500000 took 7.847653s\n",
      "   train loss: 0.567320\n",
      "   train acc: 0.845072\n",
      "   test loss: 0.666423\n",
      "   test acc: 0.817508\n",
      "Epoch 181 : Step 70590-70980 of 19500000 took 7.821176s\n",
      "   train loss: 0.573388\n",
      "   train acc: 0.844251\n",
      "   test loss: 0.678685\n",
      "   test acc: 0.811498\n",
      "Epoch 182 : Step 70980-71370 of 19500000 took 7.826407s\n",
      "   train loss: 0.569377\n",
      "   train acc: 0.845413\n",
      "   test loss: 0.654075\n",
      "   test acc: 0.817107\n",
      "Epoch 183 : Step 71370-71760 of 19500000 took 7.851842s\n",
      "   train loss: 0.566647\n",
      "   train acc: 0.846474\n",
      "   test loss: 0.658349\n",
      "   test acc: 0.818109\n",
      "Epoch 184 : Step 71760-72150 of 19500000 took 7.818488s\n",
      "   train loss: 0.563018\n",
      "   train acc: 0.848938\n",
      "   test loss: 0.687667\n",
      "   test acc: 0.808293\n",
      "Epoch 185 : Step 72150-72540 of 19500000 took 7.820984s\n",
      "   train loss: 0.563835\n",
      "   train acc: 0.847536\n",
      "   test loss: 0.665986\n",
      "   test acc: 0.819211\n",
      "Epoch 186 : Step 72540-72930 of 19500000 took 7.751526s\n",
      "   train loss: 0.562688\n",
      "   train acc: 0.847596\n",
      "   test loss: 0.653877\n",
      "   test acc: 0.818610\n",
      "Epoch 187 : Step 72930-73320 of 19500000 took 7.888392s\n",
      "   train loss: 0.564191\n",
      "   train acc: 0.845954\n",
      "   test loss: 0.655704\n",
      "   test acc: 0.820613\n",
      "Epoch 188 : Step 73320-73710 of 19500000 took 7.862005s\n",
      "   train loss: 0.558387\n",
      "   train acc: 0.849880\n",
      "   test loss: 0.651281\n",
      "   test acc: 0.818910\n",
      "Epoch 189 : Step 73710-74100 of 19500000 took 7.871315s\n",
      "   train loss: 0.561734\n",
      "   train acc: 0.847055\n",
      "   test loss: 0.667875\n",
      "   test acc: 0.817408\n",
      "Epoch 190 : Step 74100-74490 of 19500000 took 7.849916s\n",
      "   train loss: 0.560834\n",
      "   train acc: 0.847917\n",
      "   test loss: 0.649639\n",
      "   test acc: 0.819111\n",
      "Epoch 191 : Step 74490-74880 of 19500000 took 7.849084s\n",
      "   train loss: 0.563492\n",
      "   train acc: 0.848117\n",
      "   test loss: 0.672992\n",
      "   test acc: 0.814704\n",
      "Epoch 192 : Step 74880-75270 of 19500000 took 7.844064s\n",
      "   train loss: 0.557582\n",
      "   train acc: 0.849319\n",
      "   test loss: 0.676934\n",
      "   test acc: 0.814804\n",
      "Epoch 193 : Step 75270-75660 of 19500000 took 7.787636s\n",
      "   train loss: 0.560877\n",
      "   train acc: 0.849379\n",
      "   test loss: 0.657008\n",
      "   test acc: 0.818510\n",
      "Epoch 194 : Step 75660-76050 of 19500000 took 7.937502s\n",
      "   train loss: 0.558679\n",
      "   train acc: 0.849359\n",
      "   test loss: 0.651770\n",
      "   test acc: 0.818610\n",
      "Epoch 195 : Step 76050-76440 of 19500000 took 7.908252s\n",
      "   train loss: 0.554814\n",
      "   train acc: 0.850341\n",
      "   test loss: 0.650226\n",
      "   test acc: 0.822917\n",
      "Epoch 196 : Step 76440-76830 of 19500000 took 7.848437s\n",
      "   train loss: 0.556231\n",
      "   train acc: 0.850200\n",
      "   test loss: 0.657974\n",
      "   test acc: 0.820212\n",
      "Epoch 197 : Step 76830-77220 of 19500000 took 7.888405s\n",
      "   train loss: 0.554600\n",
      "   train acc: 0.850421\n",
      "   test loss: 0.659577\n",
      "   test acc: 0.819311\n",
      "Epoch 198 : Step 77220-77610 of 19500000 took 7.799305s\n",
      "   train loss: 0.554872\n",
      "   train acc: 0.849780\n",
      "   test loss: 0.687747\n",
      "   test acc: 0.811799\n",
      "Epoch 199 : Step 77610-78000 of 19500000 took 7.828739s\n",
      "   train loss: 0.550547\n",
      "   train acc: 0.851743\n",
      "   test loss: 0.675398\n",
      "   test acc: 0.814804\n",
      "Save model !!!!!!!!!!\n",
      "[TL] [*] Saving TL params into model.npz\n",
      "[TL] [*] Saved\n",
      "Epoch 200 : Step 78000-78390 of 19500000 took 7.773017s\n",
      "   train loss: 0.552099\n",
      "   train acc: 0.852224\n",
      "   test loss: 0.651988\n",
      "   test acc: 0.818610\n",
      "Epoch 201 : Step 78390-78780 of 19500000 took 7.763466s\n",
      "   train loss: 0.552940\n",
      "   train acc: 0.850781\n",
      "   test loss: 0.670248\n",
      "   test acc: 0.815204\n",
      "Epoch 202 : Step 78780-79170 of 19500000 took 7.799874s\n",
      "   train loss: 0.547506\n",
      "   train acc: 0.852885\n",
      "   test loss: 0.660627\n",
      "   test acc: 0.820813\n",
      "Epoch 203 : Step 79170-79560 of 19500000 took 7.840152s\n",
      "   train loss: 0.557401\n",
      "   train acc: 0.849880\n",
      "   test loss: 0.651517\n",
      "   test acc: 0.821214\n",
      "Epoch 204 : Step 79560-79950 of 19500000 took 7.757587s\n",
      "   train loss: 0.546234\n",
      "   train acc: 0.854387\n",
      "   test loss: 0.652164\n",
      "   test acc: 0.822015\n",
      "Epoch 205 : Step 79950-80340 of 19500000 took 7.863516s\n",
      "   train loss: 0.542924\n",
      "   train acc: 0.854928\n",
      "   test loss: 0.649720\n",
      "   test acc: 0.822216\n",
      "Epoch 206 : Step 80340-80730 of 19500000 took 7.832162s\n",
      "   train loss: 0.546590\n",
      "   train acc: 0.855228\n",
      "   test loss: 0.680435\n",
      "   test acc: 0.813902\n",
      "Epoch 207 : Step 80730-81120 of 19500000 took 7.761079s\n",
      "   train loss: 0.547730\n",
      "   train acc: 0.850942\n",
      "   test loss: 0.659310\n",
      "   test acc: 0.818510\n",
      "Epoch 208 : Step 81120-81510 of 19500000 took 7.854346s\n",
      "   train loss: 0.544813\n",
      "   train acc: 0.852985\n",
      "   test loss: 0.660410\n",
      "   test acc: 0.821314\n",
      "Epoch 209 : Step 81510-81900 of 19500000 took 7.828242s\n",
      "   train loss: 0.547016\n",
      "   train acc: 0.852484\n",
      "   test loss: 0.652219\n",
      "   test acc: 0.822115\n",
      "Epoch 210 : Step 81900-82290 of 19500000 took 7.825501s\n",
      "   train loss: 0.546080\n",
      "   train acc: 0.854067\n",
      "   test loss: 0.659857\n",
      "   test acc: 0.813301\n",
      "Epoch 211 : Step 82290-82680 of 19500000 took 7.803118s\n",
      "   train loss: 0.543009\n",
      "   train acc: 0.854788\n",
      "   test loss: 0.664053\n",
      "   test acc: 0.819010\n",
      "Epoch 212 : Step 82680-83070 of 19500000 took 7.824818s\n",
      "   train loss: 0.543971\n",
      "   train acc: 0.854307\n",
      "   test loss: 0.663972\n",
      "   test acc: 0.817107\n",
      "Epoch 213 : Step 83070-83460 of 19500000 took 7.777974s\n",
      "   train loss: 0.540394\n",
      "   train acc: 0.855308\n",
      "   test loss: 0.654264\n",
      "   test acc: 0.822316\n",
      "Epoch 214 : Step 83460-83850 of 19500000 took 7.776396s\n",
      "   train loss: 0.541029\n",
      "   train acc: 0.855929\n",
      "   test loss: 0.658465\n",
      "   test acc: 0.822817\n",
      "Epoch 215 : Step 83850-84240 of 19500000 took 7.848799s\n",
      "   train loss: 0.543461\n",
      "   train acc: 0.854788\n",
      "   test loss: 0.659411\n",
      "   test acc: 0.822516\n",
      "Epoch 216 : Step 84240-84630 of 19500000 took 7.816830s\n",
      "   train loss: 0.538206\n",
      "   train acc: 0.856871\n",
      "   test loss: 0.661739\n",
      "   test acc: 0.822115\n",
      "Epoch 217 : Step 84630-85020 of 19500000 took 7.784748s\n",
      "   train loss: 0.539811\n",
      "   train acc: 0.856210\n",
      "   test loss: 0.664416\n",
      "   test acc: 0.816406\n",
      "Epoch 218 : Step 85020-85410 of 19500000 took 7.723353s\n",
      "   train loss: 0.539684\n",
      "   train acc: 0.856030\n",
      "   test loss: 0.672762\n",
      "   test acc: 0.818009\n",
      "Epoch 219 : Step 85410-85800 of 19500000 took 7.805047s\n",
      "   train loss: 0.536123\n",
      "   train acc: 0.856791\n",
      "   test loss: 0.652326\n",
      "   test acc: 0.823518\n",
      "Epoch 220 : Step 85800-86190 of 19500000 took 7.764661s\n",
      "   train loss: 0.541428\n",
      "   train acc: 0.855929\n",
      "   test loss: 0.666580\n",
      "   test acc: 0.820513\n",
      "Epoch 221 : Step 86190-86580 of 19500000 took 7.788380s\n",
      "   train loss: 0.539723\n",
      "   train acc: 0.856891\n",
      "   test loss: 0.651789\n",
      "   test acc: 0.821314\n",
      "Epoch 222 : Step 86580-86970 of 19500000 took 7.854607s\n",
      "   train loss: 0.538792\n",
      "   train acc: 0.855729\n",
      "   test loss: 0.659053\n",
      "   test acc: 0.820413\n",
      "Epoch 223 : Step 86970-87360 of 19500000 took 7.852927s\n",
      "   train loss: 0.533154\n",
      "   train acc: 0.857792\n",
      "   test loss: 0.655025\n",
      "   test acc: 0.821514\n",
      "Epoch 224 : Step 87360-87750 of 19500000 took 7.757970s\n",
      "   train loss: 0.532922\n",
      "   train acc: 0.859615\n",
      "   test loss: 0.659122\n",
      "   test acc: 0.819712\n",
      "Epoch 225 : Step 87750-88140 of 19500000 took 7.765125s\n",
      "   train loss: 0.532849\n",
      "   train acc: 0.859455\n",
      "   test loss: 0.650681\n",
      "   test acc: 0.826222\n",
      "Epoch 226 : Step 88140-88530 of 19500000 took 7.837824s\n",
      "   train loss: 0.536218\n",
      "   train acc: 0.855950\n",
      "   test loss: 0.668963\n",
      "   test acc: 0.816907\n",
      "Epoch 227 : Step 88530-88920 of 19500000 took 7.805487s\n",
      "   train loss: 0.533790\n",
      "   train acc: 0.858153\n",
      "   test loss: 0.654954\n",
      "   test acc: 0.821014\n",
      "Epoch 228 : Step 88920-89310 of 19500000 took 7.826909s\n",
      "   train loss: 0.530604\n",
      "   train acc: 0.859575\n",
      "   test loss: 0.657425\n",
      "   test acc: 0.824018\n",
      "Epoch 229 : Step 89310-89700 of 19500000 took 7.865802s\n",
      "   train loss: 0.529526\n",
      "   train acc: 0.860156\n",
      "   test loss: 0.663842\n",
      "   test acc: 0.818610\n",
      "Epoch 230 : Step 89700-90090 of 19500000 took 7.913929s\n",
      "   train loss: 0.528824\n",
      "   train acc: 0.859034\n",
      "   test loss: 0.647388\n",
      "   test acc: 0.823818\n",
      "Epoch 231 : Step 90090-90480 of 19500000 took 7.873448s\n",
      "   train loss: 0.529256\n",
      "   train acc: 0.858474\n",
      "   test loss: 0.655297\n",
      "   test acc: 0.819812\n",
      "Epoch 232 : Step 90480-90870 of 19500000 took 7.770808s\n",
      "   train loss: 0.519713\n",
      "   train acc: 0.863602\n",
      "   test loss: 0.677432\n",
      "   test acc: 0.813802\n",
      "Epoch 233 : Step 90870-91260 of 19500000 took 7.846378s\n",
      "   train loss: 0.530385\n",
      "   train acc: 0.860216\n",
      "   test loss: 0.657995\n",
      "   test acc: 0.824820\n",
      "Epoch 234 : Step 91260-91650 of 19500000 took 7.891412s\n",
      "   train loss: 0.533641\n",
      "   train acc: 0.858073\n",
      "   test loss: 0.663882\n",
      "   test acc: 0.818610\n",
      "Epoch 235 : Step 91650-92040 of 19500000 took 7.786963s\n",
      "   train loss: 0.525652\n",
      "   train acc: 0.860597\n",
      "   test loss: 0.658224\n",
      "   test acc: 0.820513\n",
      "Epoch 236 : Step 92040-92430 of 19500000 took 7.832767s\n",
      "   train loss: 0.523845\n",
      "   train acc: 0.861719\n",
      "   test loss: 0.646538\n",
      "   test acc: 0.824519\n",
      "Epoch 237 : Step 92430-92820 of 19500000 took 7.823189s\n",
      "   train loss: 0.525696\n",
      "   train acc: 0.859014\n",
      "   test loss: 0.647690\n",
      "   test acc: 0.828425\n",
      "Epoch 238 : Step 92820-93210 of 19500000 took 7.774754s\n",
      "   train loss: 0.524155\n",
      "   train acc: 0.860697\n",
      "   test loss: 0.654091\n",
      "   test acc: 0.823117\n",
      "Epoch 239 : Step 93210-93600 of 19500000 took 7.776022s\n",
      "   train loss: 0.526188\n",
      "   train acc: 0.860978\n",
      "   test loss: 0.651640\n",
      "   test acc: 0.824519\n",
      "Epoch 240 : Step 93600-93990 of 19500000 took 7.828956s\n",
      "   train loss: 0.522539\n",
      "   train acc: 0.860817\n",
      "   test loss: 0.657035\n",
      "   test acc: 0.824519\n",
      "Epoch 241 : Step 93990-94380 of 19500000 took 7.759784s\n",
      "   train loss: 0.521923\n",
      "   train acc: 0.861558\n",
      "   test loss: 0.678772\n",
      "   test acc: 0.818309\n",
      "Epoch 242 : Step 94380-94770 of 19500000 took 7.795073s\n",
      "   train loss: 0.525901\n",
      "   train acc: 0.861639\n",
      "   test loss: 0.664778\n",
      "   test acc: 0.824519\n",
      "Epoch 243 : Step 94770-95160 of 19500000 took 7.823019s\n",
      "   train loss: 0.520767\n",
      "   train acc: 0.864203\n",
      "   test loss: 0.651871\n",
      "   test acc: 0.825521\n",
      "Epoch 244 : Step 95160-95550 of 19500000 took 7.818038s\n",
      "   train loss: 0.523372\n",
      "   train acc: 0.863181\n",
      "   test loss: 0.653699\n",
      "   test acc: 0.825721\n",
      "Epoch 245 : Step 95550-95940 of 19500000 took 7.737525s\n",
      "   train loss: 0.523883\n",
      "   train acc: 0.864022\n",
      "   test loss: 0.658385\n",
      "   test acc: 0.823117\n",
      "Epoch 246 : Step 95940-96330 of 19500000 took 7.818628s\n",
      "   train loss: 0.516329\n",
      "   train acc: 0.865785\n",
      "   test loss: 0.657176\n",
      "   test acc: 0.822416\n",
      "Epoch 247 : Step 96330-96720 of 19500000 took 7.781466s\n",
      "   train loss: 0.521606\n",
      "   train acc: 0.861318\n",
      "   test loss: 0.659364\n",
      "   test acc: 0.823618\n",
      "Epoch 248 : Step 96720-97110 of 19500000 took 7.794578s\n",
      "   train loss: 0.517331\n",
      "   train acc: 0.863822\n",
      "   test loss: 0.659541\n",
      "   test acc: 0.827023\n",
      "Epoch 249 : Step 97110-97500 of 19500000 took 7.893930s\n",
      "   train loss: 0.516525\n",
      "   train acc: 0.865625\n",
      "   test loss: 0.664874\n",
      "   test acc: 0.821214\n",
      "Save model !!!!!!!!!!\n",
      "[TL] [*] Saving TL params into model.npz\n",
      "[TL] [*] Saved\n",
      "Epoch 250 : Step 97500-97890 of 19500000 took 7.825186s\n",
      "   train loss: 0.520654\n",
      "   train acc: 0.863702\n",
      "   test loss: 0.654722\n",
      "   test acc: 0.821114\n",
      "Epoch 251 : Step 97890-98280 of 19500000 took 7.797348s\n",
      "   train loss: 0.511566\n",
      "   train acc: 0.865765\n",
      "   test loss: 0.649947\n",
      "   test acc: 0.826322\n",
      "Epoch 252 : Step 98280-98670 of 19500000 took 7.722537s\n",
      "   train loss: 0.518565\n",
      "   train acc: 0.864042\n",
      "   test loss: 0.661512\n",
      "   test acc: 0.822817\n",
      "Epoch 253 : Step 98670-99060 of 19500000 took 7.765915s\n",
      "   train loss: 0.516206\n",
      "   train acc: 0.865405\n",
      "   test loss: 0.673879\n",
      "   test acc: 0.820713\n",
      "Epoch 254 : Step 99060-99450 of 19500000 took 7.741462s\n",
      "   train loss: 0.519625\n",
      "   train acc: 0.863562\n",
      "   test loss: 0.667112\n",
      "   test acc: 0.824319\n",
      "Epoch 255 : Step 99450-99840 of 19500000 took 7.784849s\n",
      "   train loss: 0.511985\n",
      "   train acc: 0.867889\n",
      "   test loss: 0.653219\n",
      "   test acc: 0.827624\n",
      "Epoch 256 : Step 99840-100230 of 19500000 took 7.854061s\n",
      "   train loss: 0.509352\n",
      "   train acc: 0.866847\n",
      "   test loss: 0.667550\n",
      "   test acc: 0.822216\n",
      "Epoch 257 : Step 100230-100620 of 19500000 took 7.846122s\n",
      "   train loss: 0.511914\n",
      "   train acc: 0.866106\n",
      "   test loss: 0.651851\n",
      "   test acc: 0.823718\n",
      "Epoch 258 : Step 100620-101010 of 19500000 took 7.766590s\n",
      "   train loss: 0.515076\n",
      "   train acc: 0.863602\n",
      "   test loss: 0.647882\n",
      "   test acc: 0.827724\n",
      "Epoch 259 : Step 101010-101400 of 19500000 took 7.766882s\n",
      "   train loss: 0.512927\n",
      "   train acc: 0.866266\n",
      "   test loss: 0.651078\n",
      "   test acc: 0.826923\n",
      "Epoch 260 : Step 101400-101790 of 19500000 took 7.799820s\n",
      "   train loss: 0.510069\n",
      "   train acc: 0.866567\n",
      "   test loss: 0.670638\n",
      "   test acc: 0.821615\n",
      "Epoch 261 : Step 101790-102180 of 19500000 took 7.788862s\n",
      "   train loss: 0.507543\n",
      "   train acc: 0.869271\n",
      "   test loss: 0.669011\n",
      "   test acc: 0.821815\n",
      "Epoch 262 : Step 102180-102570 of 19500000 took 7.861363s\n",
      "   train loss: 0.509031\n",
      "   train acc: 0.867388\n",
      "   test loss: 0.658225\n",
      "   test acc: 0.825120\n",
      "Epoch 263 : Step 102570-102960 of 19500000 took 7.931159s\n",
      "   train loss: 0.508028\n",
      "   train acc: 0.868309\n",
      "   test loss: 0.645454\n",
      "   test acc: 0.828626\n",
      "Epoch 264 : Step 102960-103350 of 19500000 took 7.883914s\n",
      "   train loss: 0.506193\n",
      "   train acc: 0.869331\n",
      "   test loss: 0.658901\n",
      "   test acc: 0.823718\n",
      "Epoch 265 : Step 103350-103740 of 19500000 took 7.792900s\n",
      "   train loss: 0.507206\n",
      "   train acc: 0.867748\n",
      "   test loss: 0.659791\n",
      "   test acc: 0.825421\n",
      "Epoch 266 : Step 103740-104130 of 19500000 took 7.759007s\n",
      "   train loss: 0.505297\n",
      "   train acc: 0.868409\n",
      "   test loss: 0.661643\n",
      "   test acc: 0.821314\n",
      "Epoch 267 : Step 104130-104520 of 19500000 took 7.843116s\n",
      "   train loss: 0.511100\n",
      "   train acc: 0.867628\n",
      "   test loss: 0.663375\n",
      "   test acc: 0.827324\n",
      "Epoch 268 : Step 104520-104910 of 19500000 took 7.780456s\n",
      "   train loss: 0.509795\n",
      "   train acc: 0.867448\n",
      "   test loss: 0.653589\n",
      "   test acc: 0.824319\n",
      "Epoch 269 : Step 104910-105300 of 19500000 took 7.887508s\n",
      "   train loss: 0.510747\n",
      "   train acc: 0.867147\n",
      "   test loss: 0.650349\n",
      "   test acc: 0.826522\n",
      "Epoch 270 : Step 105300-105690 of 19500000 took 7.839828s\n",
      "   train loss: 0.502703\n",
      "   train acc: 0.869972\n",
      "   test loss: 0.653941\n",
      "   test acc: 0.825220\n",
      "Epoch 271 : Step 105690-106080 of 19500000 took 7.837856s\n",
      "   train loss: 0.506683\n",
      "   train acc: 0.867047\n",
      "   test loss: 0.639064\n",
      "   test acc: 0.828826\n",
      "Epoch 272 : Step 106080-106470 of 19500000 took 7.765748s\n",
      "   train loss: 0.505624\n",
      "   train acc: 0.868570\n",
      "   test loss: 0.644651\n",
      "   test acc: 0.825721\n",
      "Epoch 273 : Step 106470-106860 of 19500000 took 7.789966s\n",
      "   train loss: 0.500205\n",
      "   train acc: 0.869611\n",
      "   test loss: 0.684210\n",
      "   test acc: 0.816306\n",
      "Epoch 274 : Step 106860-107250 of 19500000 took 7.841753s\n",
      "   train loss: 0.503044\n",
      "   train acc: 0.869251\n",
      "   test loss: 0.669613\n",
      "   test acc: 0.820112\n",
      "Epoch 275 : Step 107250-107640 of 19500000 took 7.822191s\n",
      "   train loss: 0.501652\n",
      "   train acc: 0.868029\n",
      "   test loss: 0.664952\n",
      "   test acc: 0.825421\n",
      "Epoch 276 : Step 107640-108030 of 19500000 took 7.864428s\n",
      "   train loss: 0.497819\n",
      "   train acc: 0.870132\n",
      "   test loss: 0.669751\n",
      "   test acc: 0.826322\n",
      "Epoch 277 : Step 108030-108420 of 19500000 took 7.848656s\n",
      "   train loss: 0.509704\n",
      "   train acc: 0.865585\n",
      "   test loss: 0.652563\n",
      "   test acc: 0.827224\n",
      "Epoch 278 : Step 108420-108810 of 19500000 took 7.766867s\n",
      "   train loss: 0.502503\n",
      "   train acc: 0.869932\n",
      "   test loss: 0.662949\n",
      "   test acc: 0.827925\n",
      "Epoch 279 : Step 108810-109200 of 19500000 took 7.822886s\n",
      "   train loss: 0.499410\n",
      "   train acc: 0.870613\n",
      "   test loss: 0.650017\n",
      "   test acc: 0.826122\n",
      "Epoch 280 : Step 109200-109590 of 19500000 took 7.781127s\n",
      "   train loss: 0.501215\n",
      "   train acc: 0.869291\n",
      "   test loss: 0.650657\n",
      "   test acc: 0.827123\n",
      "Epoch 281 : Step 109590-109980 of 19500000 took 7.786786s\n",
      "   train loss: 0.495674\n",
      "   train acc: 0.870873\n",
      "   test loss: 0.662466\n",
      "   test acc: 0.827424\n",
      "Epoch 282 : Step 109980-110370 of 19500000 took 7.805587s\n",
      "   train loss: 0.500706\n",
      "   train acc: 0.870212\n",
      "   test loss: 0.653602\n",
      "   test acc: 0.828526\n",
      "Epoch 283 : Step 110370-110760 of 19500000 took 7.833560s\n",
      "   train loss: 0.500720\n",
      "   train acc: 0.870853\n",
      "   test loss: 0.661021\n",
      "   test acc: 0.826122\n",
      "Epoch 284 : Step 110760-111150 of 19500000 took 7.825196s\n",
      "   train loss: 0.494099\n",
      "   train acc: 0.873698\n",
      "   test loss: 0.668672\n",
      "   test acc: 0.823317\n",
      "Epoch 285 : Step 111150-111540 of 19500000 took 7.749892s\n",
      "   train loss: 0.499992\n",
      "   train acc: 0.870713\n",
      "   test loss: 0.705124\n",
      "   test acc: 0.813802\n",
      "Epoch 286 : Step 111540-111930 of 19500000 took 7.722031s\n",
      "   train loss: 0.498111\n",
      "   train acc: 0.871955\n",
      "   test loss: 0.681686\n",
      "   test acc: 0.819411\n",
      "Epoch 287 : Step 111930-112320 of 19500000 took 7.822814s\n",
      "   train loss: 0.496958\n",
      "   train acc: 0.870773\n",
      "   test loss: 0.662679\n",
      "   test acc: 0.821615\n",
      "Epoch 288 : Step 112320-112710 of 19500000 took 7.827586s\n",
      "   train loss: 0.497447\n",
      "   train acc: 0.870873\n",
      "   test loss: 0.689025\n",
      "   test acc: 0.820212\n",
      "Epoch 289 : Step 112710-113100 of 19500000 took 7.836716s\n",
      "   train loss: 0.495686\n",
      "   train acc: 0.872135\n",
      "   test loss: 0.655771\n",
      "   test acc: 0.827123\n",
      "Epoch 290 : Step 113100-113490 of 19500000 took 7.877237s\n",
      "   train loss: 0.495505\n",
      "   train acc: 0.872216\n",
      "   test loss: 0.672323\n",
      "   test acc: 0.823818\n",
      "Epoch 291 : Step 113490-113880 of 19500000 took 7.825914s\n",
      "   train loss: 0.496352\n",
      "   train acc: 0.871214\n",
      "   test loss: 0.663759\n",
      "   test acc: 0.825521\n",
      "Epoch 292 : Step 113880-114270 of 19500000 took 7.877355s\n",
      "   train loss: 0.495893\n",
      "   train acc: 0.871995\n",
      "   test loss: 0.651112\n",
      "   test acc: 0.828926\n",
      "Epoch 293 : Step 114270-114660 of 19500000 took 7.788431s\n",
      "   train loss: 0.497160\n",
      "   train acc: 0.871675\n",
      "   test loss: 0.683555\n",
      "   test acc: 0.820012\n",
      "Epoch 294 : Step 114660-115050 of 19500000 took 7.770591s\n",
      "   train loss: 0.494394\n",
      "   train acc: 0.871775\n",
      "   test loss: 0.654975\n",
      "   test acc: 0.823518\n",
      "Epoch 295 : Step 115050-115440 of 19500000 took 7.815789s\n",
      "   train loss: 0.492031\n",
      "   train acc: 0.874379\n",
      "   test loss: 0.660131\n",
      "   test acc: 0.825321\n",
      "Epoch 296 : Step 115440-115830 of 19500000 took 7.825362s\n",
      "   train loss: 0.493857\n",
      "   train acc: 0.872175\n",
      "   test loss: 0.652887\n",
      "   test acc: 0.828726\n",
      "Epoch 297 : Step 115830-116220 of 19500000 took 7.896971s\n",
      "   train loss: 0.491679\n",
      "   train acc: 0.873618\n",
      "   test loss: 0.660872\n",
      "   test acc: 0.829427\n",
      "Epoch 298 : Step 116220-116610 of 19500000 took 7.918192s\n",
      "   train loss: 0.491602\n",
      "   train acc: 0.873718\n",
      "   test loss: 0.657199\n",
      "   test acc: 0.824419\n",
      "Epoch 299 : Step 116610-117000 of 19500000 took 7.753054s\n",
      "   train loss: 0.490796\n",
      "   train acc: 0.874920\n",
      "   test loss: 0.656743\n",
      "   test acc: 0.824319\n",
      "Save model !!!!!!!!!!\n",
      "[TL] [*] Saving TL params into model.npz\n",
      "[TL] [*] Saved\n",
      "Epoch 300 : Step 117000-117390 of 19500000 took 7.795191s\n",
      "   train loss: 0.492069\n",
      "   train acc: 0.873437\n",
      "   test loss: 0.659081\n",
      "   test acc: 0.826122\n",
      "Epoch 301 : Step 117390-117780 of 19500000 took 7.755862s\n",
      "   train loss: 0.497308\n",
      "   train acc: 0.871915\n",
      "   test loss: 0.648067\n",
      "   test acc: 0.827724\n",
      "Epoch 302 : Step 117780-118170 of 19500000 took 7.834219s\n",
      "   train loss: 0.492277\n",
      "   train acc: 0.872576\n",
      "   test loss: 0.656375\n",
      "   test acc: 0.826122\n",
      "Epoch 303 : Step 118170-118560 of 19500000 took 7.834347s\n",
      "   train loss: 0.489198\n",
      "   train acc: 0.873037\n",
      "   test loss: 0.656881\n",
      "   test acc: 0.826923\n",
      "Epoch 304 : Step 118560-118950 of 19500000 took 7.824284s\n",
      "   train loss: 0.483537\n",
      "   train acc: 0.877564\n",
      "   test loss: 0.657265\n",
      "   test acc: 0.825621\n",
      "Epoch 305 : Step 118950-119340 of 19500000 took 7.770699s\n",
      "   train loss: 0.497074\n",
      "   train acc: 0.872716\n",
      "   test loss: 0.655762\n",
      "   test acc: 0.824920\n",
      "Epoch 306 : Step 119340-119730 of 19500000 took 7.722700s\n",
      "   train loss: 0.487026\n",
      "   train acc: 0.872957\n",
      "   test loss: 0.652906\n",
      "   test acc: 0.830128\n",
      "Epoch 307 : Step 119730-120120 of 19500000 took 7.860652s\n",
      "   train loss: 0.489837\n",
      "   train acc: 0.874058\n",
      "   test loss: 0.654754\n",
      "   test acc: 0.827023\n",
      "Epoch 308 : Step 120120-120510 of 19500000 took 7.812187s\n",
      "   train loss: 0.483281\n",
      "   train acc: 0.875821\n",
      "   test loss: 0.647300\n",
      "   test acc: 0.828526\n",
      "Epoch 309 : Step 120510-120900 of 19500000 took 7.885515s\n",
      "   train loss: 0.485142\n",
      "   train acc: 0.874439\n",
      "   test loss: 0.649481\n",
      "   test acc: 0.824219\n",
      "Epoch 310 : Step 120900-121290 of 19500000 took 7.845971s\n",
      "   train loss: 0.487956\n",
      "   train acc: 0.874159\n",
      "   test loss: 0.641731\n",
      "   test acc: 0.827925\n",
      "Epoch 311 : Step 121290-121680 of 19500000 took 7.881694s\n",
      "   train loss: 0.489012\n",
      "   train acc: 0.874780\n",
      "   test loss: 0.648929\n",
      "   test acc: 0.829127\n",
      "Epoch 312 : Step 121680-122070 of 19500000 took 7.802662s\n",
      "   train loss: 0.485015\n",
      "   train acc: 0.875861\n",
      "   test loss: 0.672364\n",
      "   test acc: 0.818409\n",
      "Epoch 313 : Step 122070-122460 of 19500000 took 7.830788s\n",
      "   train loss: 0.487215\n",
      "   train acc: 0.875220\n",
      "   test loss: 0.646105\n",
      "   test acc: 0.831530\n",
      "Epoch 314 : Step 122460-122850 of 19500000 took 7.855137s\n",
      "   train loss: 0.483825\n",
      "   train acc: 0.874379\n",
      "   test loss: 0.653217\n",
      "   test acc: 0.827224\n",
      "Epoch 315 : Step 122850-123240 of 19500000 took 7.876200s\n",
      "   train loss: 0.480229\n",
      "   train acc: 0.877043\n",
      "   test loss: 0.649009\n",
      "   test acc: 0.829828\n",
      "Epoch 316 : Step 123240-123630 of 19500000 took 7.850290s\n",
      "   train loss: 0.482271\n",
      "   train acc: 0.875441\n",
      "   test loss: 0.649598\n",
      "   test acc: 0.832332\n",
      "Epoch 317 : Step 123630-124020 of 19500000 took 7.862631s\n",
      "   train loss: 0.481345\n",
      "   train acc: 0.877224\n",
      "   test loss: 0.647196\n",
      "   test acc: 0.829828\n",
      "Epoch 318 : Step 124020-124410 of 19500000 took 7.827704s\n",
      "   train loss: 0.480983\n",
      "   train acc: 0.877404\n",
      "   test loss: 0.669261\n",
      "   test acc: 0.824820\n",
      "Epoch 319 : Step 124410-124800 of 19500000 took 7.795404s\n",
      "   train loss: 0.483003\n",
      "   train acc: 0.877564\n",
      "   test loss: 0.679369\n",
      "   test acc: 0.826422\n",
      "Epoch 320 : Step 124800-125190 of 19500000 took 7.796983s\n",
      "   train loss: 0.484569\n",
      "   train acc: 0.875621\n",
      "   test loss: 0.650842\n",
      "   test acc: 0.828526\n",
      "Epoch 321 : Step 125190-125580 of 19500000 took 7.750715s\n",
      "   train loss: 0.483108\n",
      "   train acc: 0.876703\n",
      "   test loss: 0.643363\n",
      "   test acc: 0.832833\n",
      "Epoch 322 : Step 125580-125970 of 19500000 took 7.808294s\n",
      "   train loss: 0.483799\n",
      "   train acc: 0.875801\n",
      "   test loss: 0.662258\n",
      "   test acc: 0.825421\n",
      "Epoch 323 : Step 125970-126360 of 19500000 took 7.886896s\n",
      "   train loss: 0.483457\n",
      "   train acc: 0.876062\n",
      "   test loss: 0.639891\n",
      "   test acc: 0.833233\n",
      "Epoch 324 : Step 126360-126750 of 19500000 took 7.846503s\n",
      "   train loss: 0.479725\n",
      "   train acc: 0.878285\n",
      "   test loss: 0.653842\n",
      "   test acc: 0.829127\n",
      "Epoch 325 : Step 126750-127140 of 19500000 took 7.928112s\n",
      "   train loss: 0.484124\n",
      "   train acc: 0.874419\n",
      "   test loss: 0.653334\n",
      "   test acc: 0.830929\n",
      "Epoch 326 : Step 127140-127530 of 19500000 took 7.815392s\n",
      "   train loss: 0.485282\n",
      "   train acc: 0.874760\n",
      "   test loss: 0.666217\n",
      "   test acc: 0.826222\n",
      "Epoch 327 : Step 127530-127920 of 19500000 took 7.894034s\n",
      "   train loss: 0.482311\n",
      "   train acc: 0.875060\n",
      "   test loss: 0.671885\n",
      "   test acc: 0.823317\n",
      "Epoch 328 : Step 127920-128310 of 19500000 took 7.771367s\n",
      "   train loss: 0.478671\n",
      "   train acc: 0.878506\n",
      "   test loss: 0.670987\n",
      "   test acc: 0.823918\n",
      "Epoch 329 : Step 128310-128700 of 19500000 took 7.831573s\n",
      "   train loss: 0.481936\n",
      "   train acc: 0.877885\n",
      "   test loss: 0.675843\n",
      "   test acc: 0.820012\n",
      "Epoch 330 : Step 128700-129090 of 19500000 took 7.865582s\n",
      "   train loss: 0.479526\n",
      "   train acc: 0.876362\n",
      "   test loss: 0.657333\n",
      "   test acc: 0.827224\n",
      "Epoch 331 : Step 129090-129480 of 19500000 took 7.878053s\n",
      "   train loss: 0.478266\n",
      "   train acc: 0.877324\n",
      "   test loss: 0.675747\n",
      "   test acc: 0.822616\n",
      "Epoch 332 : Step 129480-129870 of 19500000 took 7.842714s\n",
      "   train loss: 0.476025\n",
      "   train acc: 0.879026\n",
      "   test loss: 0.662045\n",
      "   test acc: 0.825821\n",
      "Epoch 333 : Step 129870-130260 of 19500000 took 7.856408s\n",
      "   train loss: 0.476024\n",
      "   train acc: 0.878646\n",
      "   test loss: 0.658645\n",
      "   test acc: 0.825321\n",
      "Epoch 334 : Step 130260-130650 of 19500000 took 7.822506s\n",
      "   train loss: 0.470940\n",
      "   train acc: 0.880329\n",
      "   test loss: 0.655537\n",
      "   test acc: 0.830729\n",
      "Epoch 335 : Step 130650-131040 of 19500000 took 7.945963s\n",
      "   train loss: 0.477923\n",
      "   train acc: 0.877304\n",
      "   test loss: 0.649386\n",
      "   test acc: 0.828325\n",
      "Epoch 336 : Step 131040-131430 of 19500000 took 7.886750s\n",
      "   train loss: 0.474364\n",
      "   train acc: 0.877825\n",
      "   test loss: 0.648100\n",
      "   test acc: 0.833534\n",
      "Epoch 337 : Step 131430-131820 of 19500000 took 7.855633s\n",
      "   train loss: 0.472602\n",
      "   train acc: 0.879788\n",
      "   test loss: 0.687765\n",
      "   test acc: 0.824119\n",
      "Epoch 338 : Step 131820-132210 of 19500000 took 7.853606s\n",
      "   train loss: 0.475320\n",
      "   train acc: 0.877744\n",
      "   test loss: 0.648533\n",
      "   test acc: 0.831631\n",
      "Epoch 339 : Step 132210-132600 of 19500000 took 7.801376s\n",
      "   train loss: 0.469626\n",
      "   train acc: 0.881270\n",
      "   test loss: 0.656451\n",
      "   test acc: 0.832332\n",
      "Epoch 340 : Step 132600-132990 of 19500000 took 7.768365s\n",
      "   train loss: 0.471291\n",
      "   train acc: 0.879928\n",
      "   test loss: 0.663110\n",
      "   test acc: 0.827724\n",
      "Epoch 341 : Step 132990-133380 of 19500000 took 7.818854s\n",
      "   train loss: 0.472860\n",
      "   train acc: 0.880449\n",
      "   test loss: 0.662893\n",
      "   test acc: 0.827023\n",
      "Epoch 342 : Step 133380-133770 of 19500000 took 7.870989s\n",
      "   train loss: 0.472834\n",
      "   train acc: 0.880769\n",
      "   test loss: 0.663653\n",
      "   test acc: 0.824619\n",
      "Epoch 343 : Step 133770-134160 of 19500000 took 7.861925s\n",
      "   train loss: 0.477738\n",
      "   train acc: 0.878185\n",
      "   test loss: 0.677570\n",
      "   test acc: 0.822817\n",
      "Epoch 344 : Step 134160-134550 of 19500000 took 7.839934s\n",
      "   train loss: 0.473745\n",
      "   train acc: 0.880689\n",
      "   test loss: 0.668956\n",
      "   test acc: 0.828726\n",
      "Epoch 345 : Step 134550-134940 of 19500000 took 7.844013s\n",
      "   train loss: 0.470889\n",
      "   train acc: 0.880950\n",
      "   test loss: 0.666918\n",
      "   test acc: 0.827724\n",
      "Epoch 346 : Step 134940-135330 of 19500000 took 7.797933s\n",
      "   train loss: 0.473373\n",
      "   train acc: 0.878666\n",
      "   test loss: 0.664489\n",
      "   test acc: 0.828926\n",
      "Epoch 347 : Step 135330-135720 of 19500000 took 7.785620s\n",
      "   train loss: 0.473718\n",
      "   train acc: 0.880128\n",
      "   test loss: 0.656880\n",
      "   test acc: 0.829327\n",
      "Epoch 348 : Step 135720-136110 of 19500000 took 7.813532s\n",
      "   train loss: 0.471732\n",
      "   train acc: 0.880248\n",
      "   test loss: 0.655436\n",
      "   test acc: 0.830329\n",
      "Epoch 349 : Step 136110-136500 of 19500000 took 7.785289s\n",
      "   train loss: 0.472448\n",
      "   train acc: 0.880689\n",
      "   test loss: 0.667568\n",
      "   test acc: 0.823918\n",
      "Save model !!!!!!!!!!\n",
      "[TL] [*] Saving TL params into model.npz\n",
      "[TL] [*] Saved\n",
      "Epoch 350 : Step 136500-136890 of 19500000 took 7.867954s\n",
      "   train loss: 0.471708\n",
      "   train acc: 0.879748\n",
      "   test loss: 0.668399\n",
      "   test acc: 0.825521\n",
      "Epoch 351 : Step 136890-137280 of 19500000 took 7.905711s\n",
      "   train loss: 0.472011\n",
      "   train acc: 0.880769\n",
      "   test loss: 0.655996\n",
      "   test acc: 0.828726\n",
      "Epoch 352 : Step 137280-137670 of 19500000 took 7.843959s\n",
      "   train loss: 0.473225\n",
      "   train acc: 0.878786\n",
      "   test loss: 0.675576\n",
      "   test acc: 0.825421\n",
      "Epoch 353 : Step 137670-138060 of 19500000 took 7.775948s\n",
      "   train loss: 0.474197\n",
      "   train acc: 0.878606\n",
      "   test loss: 0.650016\n",
      "   test acc: 0.828125\n",
      "Epoch 354 : Step 138060-138450 of 19500000 took 7.807935s\n",
      "   train loss: 0.463206\n",
      "   train acc: 0.882652\n",
      "   test loss: 0.666551\n",
      "   test acc: 0.826823\n",
      "Epoch 355 : Step 138450-138840 of 19500000 took 7.790171s\n",
      "   train loss: 0.471907\n",
      "   train acc: 0.880248\n",
      "   test loss: 0.652411\n",
      "   test acc: 0.832332\n",
      "Epoch 356 : Step 138840-139230 of 19500000 took 7.816886s\n",
      "   train loss: 0.468623\n",
      "   train acc: 0.883113\n",
      "   test loss: 0.649627\n",
      "   test acc: 0.829828\n",
      "Epoch 357 : Step 139230-139620 of 19500000 took 7.873457s\n",
      "   train loss: 0.468275\n",
      "   train acc: 0.880148\n",
      "   test loss: 0.657557\n",
      "   test acc: 0.831731\n",
      "Epoch 358 : Step 139620-140010 of 19500000 took 7.868834s\n",
      "   train loss: 0.466862\n",
      "   train acc: 0.880749\n",
      "   test loss: 0.652291\n",
      "   test acc: 0.831831\n",
      "Epoch 359 : Step 140010-140400 of 19500000 took 7.785993s\n",
      "   train loss: 0.467027\n",
      "   train acc: 0.881110\n",
      "   test loss: 0.648157\n",
      "   test acc: 0.831831\n",
      "Epoch 360 : Step 140400-140790 of 19500000 took 7.780025s\n",
      "   train loss: 0.466817\n",
      "   train acc: 0.881731\n",
      "   test loss: 0.650988\n",
      "   test acc: 0.830228\n",
      "Epoch 361 : Step 140790-141180 of 19500000 took 7.869592s\n",
      "   train loss: 0.466019\n",
      "   train acc: 0.882212\n",
      "   test loss: 0.666008\n",
      "   test acc: 0.826623\n",
      "Epoch 362 : Step 141180-141570 of 19500000 took 7.870859s\n",
      "   train loss: 0.467546\n",
      "   train acc: 0.882392\n",
      "   test loss: 0.643143\n",
      "   test acc: 0.834535\n",
      "Epoch 363 : Step 141570-141960 of 19500000 took 7.882956s\n",
      "   train loss: 0.463967\n",
      "   train acc: 0.882973\n",
      "   test loss: 0.655970\n",
      "   test acc: 0.831631\n",
      "Epoch 364 : Step 141960-142350 of 19500000 took 7.933562s\n",
      "   train loss: 0.470530\n",
      "   train acc: 0.878846\n",
      "   test loss: 0.646805\n",
      "   test acc: 0.829327\n",
      "Epoch 365 : Step 142350-142740 of 19500000 took 7.947561s\n",
      "   train loss: 0.468287\n",
      "   train acc: 0.880769\n",
      "   test loss: 0.648959\n",
      "   test acc: 0.829828\n",
      "Epoch 366 : Step 142740-143130 of 19500000 took 7.777522s\n",
      "   train loss: 0.463764\n",
      "   train acc: 0.884115\n",
      "   test loss: 0.657573\n",
      "   test acc: 0.825821\n",
      "Epoch 367 : Step 143130-143520 of 19500000 took 7.760969s\n",
      "   train loss: 0.463441\n",
      "   train acc: 0.882452\n",
      "   test loss: 0.659287\n",
      "   test acc: 0.831130\n",
      "Epoch 368 : Step 143520-143910 of 19500000 took 7.867721s\n",
      "   train loss: 0.470202\n",
      "   train acc: 0.879768\n",
      "   test loss: 0.643884\n",
      "   test acc: 0.832131\n",
      "Epoch 369 : Step 143910-144300 of 19500000 took 7.808199s\n",
      "   train loss: 0.466943\n",
      "   train acc: 0.881891\n",
      "   test loss: 0.654210\n",
      "   test acc: 0.831931\n",
      "Epoch 370 : Step 144300-144690 of 19500000 took 7.850273s\n",
      "   train loss: 0.464687\n",
      "   train acc: 0.879868\n",
      "   test loss: 0.650845\n",
      "   test acc: 0.832432\n",
      "Epoch 371 : Step 144690-145080 of 19500000 took 7.835398s\n",
      "   train loss: 0.462990\n",
      "   train acc: 0.882031\n",
      "   test loss: 0.676690\n",
      "   test acc: 0.824319\n",
      "Epoch 372 : Step 145080-145470 of 19500000 took 7.842737s\n",
      "   train loss: 0.461276\n",
      "   train acc: 0.883033\n",
      "   test loss: 0.671681\n",
      "   test acc: 0.824319\n",
      "Epoch 373 : Step 145470-145860 of 19500000 took 7.771088s\n",
      "   train loss: 0.463178\n",
      "   train acc: 0.881891\n",
      "   test loss: 0.671559\n",
      "   test acc: 0.827023\n",
      "Epoch 374 : Step 145860-146250 of 19500000 took 7.849902s\n",
      "   train loss: 0.463946\n",
      "   train acc: 0.883233\n",
      "   test loss: 0.648841\n",
      "   test acc: 0.833333\n",
      "Epoch 375 : Step 146250-146640 of 19500000 took 7.848112s\n",
      "   train loss: 0.460356\n",
      "   train acc: 0.883113\n",
      "   test loss: 0.669284\n",
      "   test acc: 0.827023\n",
      "Epoch 376 : Step 146640-147030 of 19500000 took 7.843426s\n",
      "   train loss: 0.455829\n",
      "   train acc: 0.886599\n",
      "   test loss: 0.668834\n",
      "   test acc: 0.827224\n",
      "Epoch 377 : Step 147030-147420 of 19500000 took 7.917514s\n",
      "   train loss: 0.455578\n",
      "   train acc: 0.884916\n",
      "   test loss: 0.658921\n",
      "   test acc: 0.829427\n",
      "Epoch 378 : Step 147420-147810 of 19500000 took 7.870464s\n",
      "   train loss: 0.461504\n",
      "   train acc: 0.883013\n",
      "   test loss: 0.647709\n",
      "   test acc: 0.828425\n",
      "Epoch 379 : Step 147810-148200 of 19500000 took 7.878203s\n",
      "   train loss: 0.463122\n",
      "   train acc: 0.882873\n",
      "   test loss: 0.662155\n",
      "   test acc: 0.831430\n",
      "Epoch 380 : Step 148200-148590 of 19500000 took 7.817850s\n",
      "   train loss: 0.460244\n",
      "   train acc: 0.885156\n",
      "   test loss: 0.641554\n",
      "   test acc: 0.831831\n",
      "Epoch 381 : Step 148590-148980 of 19500000 took 7.861822s\n",
      "   train loss: 0.456453\n",
      "   train acc: 0.884655\n",
      "   test loss: 0.643276\n",
      "   test acc: 0.829627\n",
      "Epoch 382 : Step 148980-149370 of 19500000 took 7.841942s\n",
      "   train loss: 0.465448\n",
      "   train acc: 0.882873\n",
      "   test loss: 0.646615\n",
      "   test acc: 0.830429\n",
      "Epoch 383 : Step 149370-149760 of 19500000 took 7.858685s\n",
      "   train loss: 0.460692\n",
      "   train acc: 0.885196\n",
      "   test loss: 0.645171\n",
      "   test acc: 0.831530\n",
      "Epoch 384 : Step 149760-150150 of 19500000 took 7.869013s\n",
      "   train loss: 0.460939\n",
      "   train acc: 0.886398\n",
      "   test loss: 0.661394\n",
      "   test acc: 0.827424\n",
      "Epoch 385 : Step 150150-150540 of 19500000 took 7.854366s\n",
      "   train loss: 0.455488\n",
      "   train acc: 0.884155\n",
      "   test loss: 0.664357\n",
      "   test acc: 0.831130\n",
      "Epoch 386 : Step 150540-150930 of 19500000 took 7.831769s\n",
      "   train loss: 0.460446\n",
      "   train acc: 0.884395\n",
      "   test loss: 0.657632\n",
      "   test acc: 0.831030\n",
      "Epoch 387 : Step 150930-151320 of 19500000 took 7.830423s\n",
      "   train loss: 0.459989\n",
      "   train acc: 0.883914\n",
      "   test loss: 0.658013\n",
      "   test acc: 0.828926\n",
      "Epoch 388 : Step 151320-151710 of 19500000 took 7.843269s\n",
      "   train loss: 0.461085\n",
      "   train acc: 0.882692\n",
      "   test loss: 0.659538\n",
      "   test acc: 0.827624\n",
      "Epoch 389 : Step 151710-152100 of 19500000 took 7.809547s\n",
      "   train loss: 0.453604\n",
      "   train acc: 0.884856\n",
      "   test loss: 0.658805\n",
      "   test acc: 0.829928\n",
      "Epoch 390 : Step 152100-152490 of 19500000 took 7.871732s\n",
      "   train loss: 0.456460\n",
      "   train acc: 0.886599\n",
      "   test loss: 0.655102\n",
      "   test acc: 0.833734\n",
      "Epoch 391 : Step 152490-152880 of 19500000 took 7.872392s\n",
      "   train loss: 0.458165\n",
      "   train acc: 0.885697\n",
      "   test loss: 0.645994\n",
      "   test acc: 0.836739\n",
      "Epoch 392 : Step 152880-153270 of 19500000 took 7.897560s\n",
      "   train loss: 0.453661\n",
      "   train acc: 0.885477\n",
      "   test loss: 0.656907\n",
      "   test acc: 0.831931\n",
      "Epoch 393 : Step 153270-153660 of 19500000 took 7.882276s\n",
      "   train loss: 0.453446\n",
      "   train acc: 0.884796\n",
      "   test loss: 0.665955\n",
      "   test acc: 0.827624\n",
      "Epoch 394 : Step 153660-154050 of 19500000 took 7.797321s\n",
      "   train loss: 0.458951\n",
      "   train acc: 0.883674\n",
      "   test loss: 0.650997\n",
      "   test acc: 0.829928\n",
      "Epoch 395 : Step 154050-154440 of 19500000 took 7.849605s\n",
      "   train loss: 0.457573\n",
      "   train acc: 0.885016\n",
      "   test loss: 0.661833\n",
      "   test acc: 0.827724\n",
      "Epoch 396 : Step 154440-154830 of 19500000 took 7.840909s\n",
      "   train loss: 0.456089\n",
      "   train acc: 0.886839\n",
      "   test loss: 0.649026\n",
      "   test acc: 0.832432\n",
      "Epoch 397 : Step 154830-155220 of 19500000 took 7.902368s\n",
      "   train loss: 0.454083\n",
      "   train acc: 0.886478\n",
      "   test loss: 0.656761\n",
      "   test acc: 0.828025\n",
      "Epoch 398 : Step 155220-155610 of 19500000 took 7.921635s\n",
      "   train loss: 0.457100\n",
      "   train acc: 0.884275\n",
      "   test loss: 0.646770\n",
      "   test acc: 0.833734\n",
      "Epoch 399 : Step 155610-156000 of 19500000 took 7.815128s\n",
      "   train loss: 0.460576\n",
      "   train acc: 0.883714\n",
      "   test loss: 0.656976\n",
      "   test acc: 0.829427\n",
      "Save model !!!!!!!!!!\n",
      "[TL] [*] Saving TL params into model.npz\n",
      "[TL] [*] Saved\n",
      "Epoch 400 : Step 156000-156390 of 19500000 took 7.779569s\n",
      "   train loss: 0.450676\n",
      "   train acc: 0.887099\n",
      "   test loss: 0.692034\n",
      "   test acc: 0.823618\n",
      "Epoch 401 : Step 156390-156780 of 19500000 took 7.812902s\n",
      "   train loss: 0.457202\n",
      "   train acc: 0.885216\n",
      "   test loss: 0.655095\n",
      "   test acc: 0.830729\n",
      "Epoch 402 : Step 156780-157170 of 19500000 took 7.772659s\n",
      "   train loss: 0.452390\n",
      "   train acc: 0.886859\n",
      "   test loss: 0.663954\n",
      "   test acc: 0.829828\n",
      "Epoch 403 : Step 157170-157560 of 19500000 took 7.851847s\n",
      "   train loss: 0.458784\n",
      "   train acc: 0.884615\n",
      "   test loss: 0.686678\n",
      "   test acc: 0.821514\n",
      "Epoch 404 : Step 157560-157950 of 19500000 took 7.825139s\n",
      "   train loss: 0.451018\n",
      "   train acc: 0.887360\n",
      "   test loss: 0.660963\n",
      "   test acc: 0.827424\n",
      "Epoch 405 : Step 157950-158340 of 19500000 took 7.853027s\n",
      "   train loss: 0.453681\n",
      "   train acc: 0.888021\n",
      "   test loss: 0.661406\n",
      "   test acc: 0.830228\n",
      "Epoch 406 : Step 158340-158730 of 19500000 took 7.798506s\n",
      "   train loss: 0.452915\n",
      "   train acc: 0.886298\n",
      "   test loss: 0.651423\n",
      "   test acc: 0.831330\n",
      "Epoch 407 : Step 158730-159120 of 19500000 took 7.808921s\n",
      "   train loss: 0.453516\n",
      "   train acc: 0.886478\n",
      "   test loss: 0.651658\n",
      "   test acc: 0.834034\n",
      "Epoch 408 : Step 159120-159510 of 19500000 took 7.809869s\n",
      "   train loss: 0.446625\n",
      "   train acc: 0.887780\n",
      "   test loss: 0.669788\n",
      "   test acc: 0.826522\n",
      "Epoch 409 : Step 159510-159900 of 19500000 took 7.814255s\n",
      "   train loss: 0.449362\n",
      "   train acc: 0.888061\n",
      "   test loss: 0.645269\n",
      "   test acc: 0.833133\n",
      "Epoch 410 : Step 159900-160290 of 19500000 took 7.915417s\n",
      "   train loss: 0.448969\n",
      "   train acc: 0.887780\n",
      "   test loss: 0.652728\n",
      "   test acc: 0.830228\n",
      "Epoch 411 : Step 160290-160680 of 19500000 took 7.925314s\n",
      "   train loss: 0.449483\n",
      "   train acc: 0.888582\n",
      "   test loss: 0.660410\n",
      "   test acc: 0.831030\n",
      "Epoch 412 : Step 160680-161070 of 19500000 took 7.850854s\n",
      "   train loss: 0.453787\n",
      "   train acc: 0.886198\n",
      "   test loss: 0.651459\n",
      "   test acc: 0.829828\n",
      "Epoch 413 : Step 161070-161460 of 19500000 took 7.800642s\n",
      "   train loss: 0.452069\n",
      "   train acc: 0.887380\n",
      "   test loss: 0.647263\n",
      "   test acc: 0.836238\n",
      "Epoch 414 : Step 161460-161850 of 19500000 took 7.799597s\n",
      "   train loss: 0.446995\n",
      "   train acc: 0.889824\n",
      "   test loss: 0.663343\n",
      "   test acc: 0.831530\n",
      "Epoch 415 : Step 161850-162240 of 19500000 took 7.799609s\n",
      "   train loss: 0.450306\n",
      "   train acc: 0.887460\n",
      "   test loss: 0.639118\n",
      "   test acc: 0.834135\n",
      "Epoch 416 : Step 162240-162630 of 19500000 took 7.806092s\n",
      "   train loss: 0.449586\n",
      "   train acc: 0.887660\n",
      "   test loss: 0.643442\n",
      "   test acc: 0.833033\n",
      "Epoch 417 : Step 162630-163020 of 19500000 took 7.835656s\n",
      "   train loss: 0.453461\n",
      "   train acc: 0.883654\n",
      "   test loss: 0.666292\n",
      "   test acc: 0.826923\n",
      "Epoch 418 : Step 163020-163410 of 19500000 took 7.847534s\n",
      "   train loss: 0.447947\n",
      "   train acc: 0.887580\n",
      "   test loss: 0.658746\n",
      "   test acc: 0.830028\n",
      "Epoch 419 : Step 163410-163800 of 19500000 took 7.865463s\n",
      "   train loss: 0.455096\n",
      "   train acc: 0.886739\n",
      "   test loss: 0.654543\n",
      "   test acc: 0.830329\n",
      "Epoch 420 : Step 163800-164190 of 19500000 took 7.766356s\n",
      "   train loss: 0.449368\n",
      "   train acc: 0.888602\n",
      "   test loss: 0.647672\n",
      "   test acc: 0.831330\n",
      "Epoch 421 : Step 164190-164580 of 19500000 took 7.765920s\n",
      "   train loss: 0.451178\n",
      "   train acc: 0.886959\n",
      "   test loss: 0.649773\n",
      "   test acc: 0.830128\n",
      "Epoch 422 : Step 164580-164970 of 19500000 took 7.824871s\n",
      "   train loss: 0.448283\n",
      "   train acc: 0.889123\n",
      "   test loss: 0.660151\n",
      "   test acc: 0.831130\n",
      "Epoch 423 : Step 164970-165360 of 19500000 took 7.810380s\n",
      "   train loss: 0.451790\n",
      "   train acc: 0.886779\n",
      "   test loss: 0.657706\n",
      "   test acc: 0.830729\n",
      "Epoch 424 : Step 165360-165750 of 19500000 took 7.876028s\n",
      "   train loss: 0.445869\n",
      "   train acc: 0.889323\n",
      "   test loss: 0.651537\n",
      "   test acc: 0.833033\n",
      "Epoch 425 : Step 165750-166140 of 19500000 took 7.841170s\n",
      "   train loss: 0.449192\n",
      "   train acc: 0.885717\n",
      "   test loss: 0.667976\n",
      "   test acc: 0.828325\n",
      "Epoch 426 : Step 166140-166530 of 19500000 took 7.770416s\n",
      "   train loss: 0.445546\n",
      "   train acc: 0.889764\n",
      "   test loss: 0.653138\n",
      "   test acc: 0.832833\n",
      "Epoch 427 : Step 166530-166920 of 19500000 took 7.752024s\n",
      "   train loss: 0.445082\n",
      "   train acc: 0.888662\n",
      "   test loss: 0.667444\n",
      "   test acc: 0.831330\n",
      "Epoch 428 : Step 166920-167310 of 19500000 took 7.799627s\n",
      "   train loss: 0.444012\n",
      "   train acc: 0.889243\n",
      "   test loss: 0.657765\n",
      "   test acc: 0.830629\n",
      "Epoch 429 : Step 167310-167700 of 19500000 took 7.813883s\n",
      "   train loss: 0.444689\n",
      "   train acc: 0.889884\n",
      "   test loss: 0.653375\n",
      "   test acc: 0.829627\n",
      "Epoch 430 : Step 167700-168090 of 19500000 took 7.827059s\n",
      "   train loss: 0.447942\n",
      "   train acc: 0.888401\n",
      "   test loss: 0.654389\n",
      "   test acc: 0.829227\n",
      "Epoch 431 : Step 168090-168480 of 19500000 took 7.885406s\n",
      "   train loss: 0.446581\n",
      "   train acc: 0.888281\n",
      "   test loss: 0.671822\n",
      "   test acc: 0.826823\n",
      "Epoch 432 : Step 168480-168870 of 19500000 took 7.972925s\n",
      "   train loss: 0.443673\n",
      "   train acc: 0.889844\n",
      "   test loss: 0.662005\n",
      "   test acc: 0.827324\n",
      "Epoch 433 : Step 168870-169260 of 19500000 took 7.791357s\n",
      "   train loss: 0.444571\n",
      "   train acc: 0.889984\n",
      "   test loss: 0.648794\n",
      "   test acc: 0.833634\n",
      "Epoch 434 : Step 169260-169650 of 19500000 took 7.766245s\n",
      "   train loss: 0.443337\n",
      "   train acc: 0.889083\n",
      "   test loss: 0.662279\n",
      "   test acc: 0.833734\n",
      "Epoch 435 : Step 169650-170040 of 19500000 took 7.761470s\n",
      "   train loss: 0.446448\n",
      "   train acc: 0.887099\n",
      "   test loss: 0.665585\n",
      "   test acc: 0.830429\n",
      "Epoch 436 : Step 170040-170430 of 19500000 took 7.801279s\n",
      "   train loss: 0.439855\n",
      "   train acc: 0.890665\n",
      "   test loss: 0.650702\n",
      "   test acc: 0.832332\n",
      "Epoch 437 : Step 170430-170820 of 19500000 took 7.957343s\n",
      "   train loss: 0.438280\n",
      "   train acc: 0.891286\n",
      "   test loss: 0.653820\n",
      "   test acc: 0.833534\n",
      "Epoch 438 : Step 170820-171210 of 19500000 took 7.837530s\n",
      "   train loss: 0.447180\n",
      "   train acc: 0.888922\n",
      "   test loss: 0.681190\n",
      "   test acc: 0.831430\n",
      "Epoch 439 : Step 171210-171600 of 19500000 took 7.888371s\n",
      "   train loss: 0.441180\n",
      "   train acc: 0.890665\n",
      "   test loss: 0.645976\n",
      "   test acc: 0.834435\n",
      "Epoch 440 : Step 171600-171990 of 19500000 took 7.836687s\n",
      "   train loss: 0.445022\n",
      "   train acc: 0.888962\n",
      "   test loss: 0.655280\n",
      "   test acc: 0.834635\n",
      "Epoch 441 : Step 171990-172380 of 19500000 took 7.817620s\n",
      "   train loss: 0.447949\n",
      "   train acc: 0.888542\n",
      "   test loss: 0.648553\n",
      "   test acc: 0.834135\n",
      "Epoch 442 : Step 172380-172770 of 19500000 took 7.869184s\n",
      "   train loss: 0.448698\n",
      "   train acc: 0.888261\n",
      "   test loss: 0.654799\n",
      "   test acc: 0.832232\n",
      "Epoch 443 : Step 172770-173160 of 19500000 took 7.794440s\n",
      "   train loss: 0.440424\n",
      "   train acc: 0.890244\n",
      "   test loss: 0.669996\n",
      "   test acc: 0.826823\n",
      "Epoch 444 : Step 173160-173550 of 19500000 took 7.821792s\n",
      "   train loss: 0.444402\n",
      "   train acc: 0.890004\n",
      "   test loss: 0.648571\n",
      "   test acc: 0.837640\n",
      "Epoch 445 : Step 173550-173940 of 19500000 took 7.872999s\n",
      "   train loss: 0.442928\n",
      "   train acc: 0.890445\n",
      "   test loss: 0.669078\n",
      "   test acc: 0.825621\n",
      "Epoch 446 : Step 173940-174330 of 19500000 took 7.857068s\n",
      "   train loss: 0.445904\n",
      "   train acc: 0.888021\n",
      "   test loss: 0.652848\n",
      "   test acc: 0.834535\n",
      "Epoch 447 : Step 174330-174720 of 19500000 took 7.805881s\n",
      "   train loss: 0.443430\n",
      "   train acc: 0.890665\n",
      "   test loss: 0.653912\n",
      "   test acc: 0.834335\n",
      "Epoch 448 : Step 174720-175110 of 19500000 took 7.794339s\n",
      "   train loss: 0.444404\n",
      "   train acc: 0.888802\n",
      "   test loss: 0.668965\n",
      "   test acc: 0.829828\n",
      "Epoch 449 : Step 175110-175500 of 19500000 took 7.812395s\n",
      "   train loss: 0.442265\n",
      "   train acc: 0.890725\n",
      "   test loss: 0.649265\n",
      "   test acc: 0.836038\n",
      "Save model !!!!!!!!!!\n",
      "[TL] [*] Saving TL params into model.npz\n",
      "[TL] [*] Saved\n",
      "Epoch 450 : Step 175500-175890 of 19500000 took 7.805628s\n",
      "   train loss: 0.438562\n",
      "   train acc: 0.891106\n",
      "   test loss: 0.664441\n",
      "   test acc: 0.830329\n",
      "Epoch 451 : Step 175890-176280 of 19500000 took 7.840462s\n",
      "   train loss: 0.440033\n",
      "   train acc: 0.891907\n",
      "   test loss: 0.659994\n",
      "   test acc: 0.830629\n",
      "Epoch 452 : Step 176280-176670 of 19500000 took 7.829290s\n",
      "   train loss: 0.442614\n",
      "   train acc: 0.890525\n",
      "   test loss: 0.647603\n",
      "   test acc: 0.835737\n",
      "Epoch 453 : Step 176670-177060 of 19500000 took 7.775313s\n",
      "   train loss: 0.438721\n",
      "   train acc: 0.890605\n",
      "   test loss: 0.658523\n",
      "   test acc: 0.834435\n",
      "Epoch 454 : Step 177060-177450 of 19500000 took 7.768393s\n",
      "   train loss: 0.441140\n",
      "   train acc: 0.889824\n",
      "   test loss: 0.661392\n",
      "   test acc: 0.834335\n",
      "Epoch 455 : Step 177450-177840 of 19500000 took 7.810604s\n",
      "   train loss: 0.441989\n",
      "   train acc: 0.890144\n",
      "   test loss: 0.671741\n",
      "   test acc: 0.826723\n",
      "Epoch 456 : Step 177840-178230 of 19500000 took 7.766811s\n",
      "   train loss: 0.435417\n",
      "   train acc: 0.892107\n",
      "   test loss: 0.674271\n",
      "   test acc: 0.830228\n",
      "Epoch 457 : Step 178230-178620 of 19500000 took 7.782428s\n",
      "   train loss: 0.442888\n",
      "   train acc: 0.890946\n",
      "   test loss: 0.664408\n",
      "   test acc: 0.833433\n",
      "Epoch 458 : Step 178620-179010 of 19500000 took 7.827876s\n",
      "   train loss: 0.439862\n",
      "   train acc: 0.890665\n",
      "   test loss: 0.660093\n",
      "   test acc: 0.830429\n",
      "Epoch 459 : Step 179010-179400 of 19500000 took 7.822405s\n",
      "   train loss: 0.439615\n",
      "   train acc: 0.891306\n",
      "   test loss: 0.668026\n",
      "   test acc: 0.832732\n",
      "Epoch 460 : Step 179400-179790 of 19500000 took 7.815852s\n",
      "   train loss: 0.435546\n",
      "   train acc: 0.894311\n",
      "   test loss: 0.662527\n",
      "   test acc: 0.830729\n",
      "Epoch 461 : Step 179790-180180 of 19500000 took 7.739907s\n",
      "   train loss: 0.436970\n",
      "   train acc: 0.891587\n",
      "   test loss: 0.668771\n",
      "   test acc: 0.831430\n",
      "Epoch 462 : Step 180180-180570 of 19500000 took 7.743554s\n",
      "   train loss: 0.437737\n",
      "   train acc: 0.891747\n",
      "   test loss: 0.650265\n",
      "   test acc: 0.833734\n",
      "Epoch 463 : Step 180570-180960 of 19500000 took 7.825005s\n",
      "   train loss: 0.442998\n",
      "   train acc: 0.888642\n",
      "   test loss: 0.662436\n",
      "   test acc: 0.831631\n",
      "Epoch 464 : Step 180960-181350 of 19500000 took 7.821560s\n",
      "   train loss: 0.436216\n",
      "   train acc: 0.892067\n",
      "   test loss: 0.661672\n",
      "   test acc: 0.832833\n",
      "Epoch 465 : Step 181350-181740 of 19500000 took 7.812508s\n",
      "   train loss: 0.435433\n",
      "   train acc: 0.892568\n",
      "   test loss: 0.660903\n",
      "   test acc: 0.833734\n",
      "Epoch 466 : Step 181740-182130 of 19500000 took 7.809033s\n",
      "   train loss: 0.445572\n",
      "   train acc: 0.888762\n",
      "   test loss: 0.657814\n",
      "   test acc: 0.832432\n",
      "Epoch 467 : Step 182130-182520 of 19500000 took 7.757294s\n",
      "   train loss: 0.437914\n",
      "   train acc: 0.891486\n",
      "   test loss: 0.653380\n",
      "   test acc: 0.832232\n",
      "Epoch 468 : Step 182520-182910 of 19500000 took 7.789949s\n",
      "   train loss: 0.436384\n",
      "   train acc: 0.891767\n",
      "   test loss: 0.659090\n",
      "   test acc: 0.832532\n",
      "Epoch 469 : Step 182910-183300 of 19500000 took 7.769222s\n",
      "   train loss: 0.438658\n",
      "   train acc: 0.892228\n",
      "   test loss: 0.667316\n",
      "   test acc: 0.833934\n",
      "Epoch 470 : Step 183300-183690 of 19500000 took 7.877542s\n",
      "   train loss: 0.431244\n",
      "   train acc: 0.894872\n",
      "   test loss: 0.657530\n",
      "   test acc: 0.833734\n",
      "Epoch 471 : Step 183690-184080 of 19500000 took 7.891169s\n",
      "   train loss: 0.431957\n",
      "   train acc: 0.894451\n",
      "   test loss: 0.670205\n",
      "   test acc: 0.829227\n",
      "Epoch 472 : Step 184080-184470 of 19500000 took 7.778988s\n",
      "   train loss: 0.440492\n",
      "   train acc: 0.891186\n",
      "   test loss: 0.678468\n",
      "   test acc: 0.831330\n",
      "Epoch 473 : Step 184470-184860 of 19500000 took 7.831740s\n",
      "   train loss: 0.437898\n",
      "   train acc: 0.890204\n",
      "   test loss: 0.657474\n",
      "   test acc: 0.835437\n",
      "Epoch 474 : Step 184860-185250 of 19500000 took 7.770236s\n",
      "   train loss: 0.438092\n",
      "   train acc: 0.890164\n",
      "   test loss: 0.654614\n",
      "   test acc: 0.835136\n",
      "Epoch 475 : Step 185250-185640 of 19500000 took 7.809342s\n",
      "   train loss: 0.437554\n",
      "   train acc: 0.890705\n",
      "   test loss: 0.658179\n",
      "   test acc: 0.830329\n",
      "Epoch 476 : Step 185640-186030 of 19500000 took 7.858548s\n",
      "   train loss: 0.437263\n",
      "   train acc: 0.891947\n",
      "   test loss: 0.649278\n",
      "   test acc: 0.834034\n",
      "Epoch 477 : Step 186030-186420 of 19500000 took 7.858526s\n",
      "   train loss: 0.432600\n",
      "   train acc: 0.893950\n",
      "   test loss: 0.651522\n",
      "   test acc: 0.837841\n",
      "Epoch 478 : Step 186420-186810 of 19500000 took 7.848287s\n",
      "   train loss: 0.432933\n",
      "   train acc: 0.893570\n",
      "   test loss: 0.683322\n",
      "   test acc: 0.828125\n",
      "Epoch 479 : Step 186810-187200 of 19500000 took 7.862100s\n",
      "   train loss: 0.435567\n",
      "   train acc: 0.892047\n",
      "   test loss: 0.650468\n",
      "   test acc: 0.836238\n",
      "Epoch 480 : Step 187200-187590 of 19500000 took 7.862789s\n",
      "   train loss: 0.437174\n",
      "   train acc: 0.892448\n",
      "   test loss: 0.639788\n",
      "   test acc: 0.839744\n",
      "Epoch 481 : Step 187590-187980 of 19500000 took 7.755738s\n",
      "   train loss: 0.434516\n",
      "   train acc: 0.893409\n",
      "   test loss: 0.686049\n",
      "   test acc: 0.827724\n",
      "Epoch 482 : Step 187980-188370 of 19500000 took 7.788561s\n",
      "   train loss: 0.427737\n",
      "   train acc: 0.895773\n",
      "   test loss: 0.658816\n",
      "   test acc: 0.834435\n",
      "Epoch 483 : Step 188370-188760 of 19500000 took 7.768486s\n",
      "   train loss: 0.438003\n",
      "   train acc: 0.891146\n",
      "   test loss: 0.652294\n",
      "   test acc: 0.831931\n",
      "Epoch 484 : Step 188760-189150 of 19500000 took 7.840780s\n",
      "   train loss: 0.431236\n",
      "   train acc: 0.893650\n",
      "   test loss: 0.662823\n",
      "   test acc: 0.833734\n",
      "Epoch 485 : Step 189150-189540 of 19500000 took 7.854564s\n",
      "   train loss: 0.432373\n",
      "   train acc: 0.893530\n",
      "   test loss: 0.665721\n",
      "   test acc: 0.832031\n",
      "Epoch 486 : Step 189540-189930 of 19500000 took 7.825662s\n",
      "   train loss: 0.435108\n",
      "   train acc: 0.893950\n",
      "   test loss: 0.661925\n",
      "   test acc: 0.828626\n",
      "Epoch 487 : Step 189930-190320 of 19500000 took 7.812546s\n",
      "   train loss: 0.428878\n",
      "   train acc: 0.895653\n",
      "   test loss: 0.662709\n",
      "   test acc: 0.832031\n",
      "Epoch 488 : Step 190320-190710 of 19500000 took 7.767333s\n",
      "   train loss: 0.431442\n",
      "   train acc: 0.894591\n",
      "   test loss: 0.665336\n",
      "   test acc: 0.833934\n",
      "Epoch 489 : Step 190710-191100 of 19500000 took 7.735377s\n",
      "   train loss: 0.428971\n",
      "   train acc: 0.895312\n",
      "   test loss: 0.661193\n",
      "   test acc: 0.835036\n",
      "Epoch 490 : Step 191100-191490 of 19500000 took 7.773257s\n",
      "   train loss: 0.432985\n",
      "   train acc: 0.892929\n",
      "   test loss: 0.668801\n",
      "   test acc: 0.831831\n",
      "Epoch 491 : Step 191490-191880 of 19500000 took 7.864155s\n",
      "   train loss: 0.433494\n",
      "   train acc: 0.893409\n",
      "   test loss: 0.661007\n",
      "   test acc: 0.832432\n",
      "Epoch 492 : Step 191880-192270 of 19500000 took 7.843035s\n",
      "   train loss: 0.429863\n",
      "   train acc: 0.894712\n",
      "   test loss: 0.662733\n",
      "   test acc: 0.829527\n",
      "Epoch 493 : Step 192270-192660 of 19500000 took 7.765093s\n",
      "   train loss: 0.429700\n",
      "   train acc: 0.894231\n",
      "   test loss: 0.671731\n",
      "   test acc: 0.830629\n",
      "Epoch 494 : Step 192660-193050 of 19500000 took 7.766665s\n",
      "   train loss: 0.431430\n",
      "   train acc: 0.894371\n",
      "   test loss: 0.650689\n",
      "   test acc: 0.839343\n",
      "Epoch 495 : Step 193050-193440 of 19500000 took 7.724363s\n",
      "   train loss: 0.432992\n",
      "   train acc: 0.893349\n",
      "   test loss: 0.670284\n",
      "   test acc: 0.829327\n",
      "Epoch 496 : Step 193440-193830 of 19500000 took 7.709414s\n",
      "   train loss: 0.428792\n",
      "   train acc: 0.894071\n",
      "   test loss: 0.665266\n",
      "   test acc: 0.832532\n",
      "Epoch 497 : Step 193830-194220 of 19500000 took 7.831072s\n",
      "   train loss: 0.432447\n",
      "   train acc: 0.893950\n",
      "   test loss: 0.669862\n",
      "   test acc: 0.830329\n",
      "Epoch 498 : Step 194220-194610 of 19500000 took 7.849076s\n",
      "   train loss: 0.431531\n",
      "   train acc: 0.894091\n",
      "   test loss: 0.677761\n",
      "   test acc: 0.826022\n",
      "Epoch 499 : Step 194610-195000 of 19500000 took 7.829303s\n",
      "   train loss: 0.431661\n",
      "   train acc: 0.893830\n",
      "   test loss: 0.675615\n",
      "   test acc: 0.831831\n",
      "Save model !!!!!!!!!!\n",
      "[TL] [*] Saving TL params into model.npz\n",
      "[TL] [*] Saved\n",
      "Epoch 500 : Step 195000-195390 of 19500000 took 7.874349s\n",
      "   train loss: 0.428935\n",
      "   train acc: 0.895453\n",
      "   test loss: 0.667581\n",
      "   test acc: 0.829828\n",
      "Epoch 501 : Step 195390-195780 of 19500000 took 7.755152s\n",
      "   train loss: 0.431102\n",
      "   train acc: 0.893409\n",
      "   test loss: 0.672669\n",
      "   test acc: 0.827324\n",
      "Epoch 502 : Step 195780-196170 of 19500000 took 7.756042s\n",
      "   train loss: 0.431350\n",
      "   train acc: 0.895232\n",
      "   test loss: 0.654436\n",
      "   test acc: 0.834736\n",
      "Epoch 503 : Step 196170-196560 of 19500000 took 7.740666s\n",
      "   train loss: 0.430948\n",
      "   train acc: 0.894792\n",
      "   test loss: 0.646326\n",
      "   test acc: 0.834235\n",
      "Epoch 504 : Step 196560-196950 of 19500000 took 7.785172s\n",
      "   train loss: 0.425826\n",
      "   train acc: 0.895593\n",
      "   test loss: 0.652750\n",
      "   test acc: 0.832933\n",
      "Epoch 505 : Step 196950-197340 of 19500000 took 7.780809s\n",
      "   train loss: 0.432663\n",
      "   train acc: 0.893429\n",
      "   test loss: 0.656421\n",
      "   test acc: 0.829427\n",
      "Epoch 506 : Step 197340-197730 of 19500000 took 7.784043s\n",
      "   train loss: 0.426619\n",
      "   train acc: 0.895633\n",
      "   test loss: 0.669547\n",
      "   test acc: 0.832732\n",
      "Epoch 507 : Step 197730-198120 of 19500000 took 7.770595s\n",
      "   train loss: 0.432259\n",
      "   train acc: 0.893670\n",
      "   test loss: 0.666336\n",
      "   test acc: 0.833133\n",
      "Epoch 508 : Step 198120-198510 of 19500000 took 7.778733s\n",
      "   train loss: 0.431716\n",
      "   train acc: 0.895052\n",
      "   test loss: 0.670434\n",
      "   test acc: 0.830128\n",
      "Epoch 509 : Step 198510-198900 of 19500000 took 7.751695s\n",
      "   train loss: 0.429646\n",
      "   train acc: 0.893510\n",
      "   test loss: 0.667351\n",
      "   test acc: 0.834034\n",
      "Epoch 510 : Step 198900-199290 of 19500000 took 7.712434s\n",
      "   train loss: 0.431232\n",
      "   train acc: 0.894671\n",
      "   test loss: 0.650501\n",
      "   test acc: 0.832833\n",
      "Epoch 511 : Step 199290-199680 of 19500000 took 7.812891s\n",
      "   train loss: 0.422863\n",
      "   train acc: 0.898017\n",
      "   test loss: 0.647425\n",
      "   test acc: 0.831631\n",
      "Epoch 512 : Step 199680-200070 of 19500000 took 7.774406s\n",
      "   train loss: 0.426532\n",
      "   train acc: 0.894411\n",
      "   test loss: 0.645334\n",
      "   test acc: 0.834635\n",
      "Epoch 513 : Step 200070-200460 of 19500000 took 7.820362s\n",
      "   train loss: 0.430832\n",
      "   train acc: 0.895373\n",
      "   test loss: 0.660897\n",
      "   test acc: 0.832632\n",
      "Epoch 514 : Step 200460-200850 of 19500000 took 7.792084s\n",
      "   train loss: 0.434313\n",
      "   train acc: 0.893630\n",
      "   test loss: 0.651636\n",
      "   test acc: 0.833033\n",
      "Epoch 515 : Step 200850-201240 of 19500000 took 7.766512s\n",
      "   train loss: 0.423258\n",
      "   train acc: 0.897436\n",
      "   test loss: 0.657727\n",
      "   test acc: 0.834936\n",
      "Epoch 516 : Step 201240-201630 of 19500000 took 7.802415s\n",
      "   train loss: 0.424574\n",
      "   train acc: 0.896234\n",
      "   test loss: 0.661629\n",
      "   test acc: 0.834335\n",
      "Epoch 517 : Step 201630-202020 of 19500000 took 7.756634s\n",
      "   train loss: 0.431459\n",
      "   train acc: 0.892929\n",
      "   test loss: 0.659531\n",
      "   test acc: 0.833734\n",
      "Epoch 518 : Step 202020-202410 of 19500000 took 7.887460s\n",
      "   train loss: 0.424756\n",
      "   train acc: 0.896094\n",
      "   test loss: 0.672616\n",
      "   test acc: 0.830128\n",
      "Epoch 519 : Step 202410-202800 of 19500000 took 7.767887s\n",
      "   train loss: 0.427119\n",
      "   train acc: 0.895473\n",
      "   test loss: 0.663281\n",
      "   test acc: 0.831731\n",
      "Epoch 520 : Step 202800-203190 of 19500000 took 7.763423s\n",
      "   train loss: 0.426076\n",
      "   train acc: 0.896254\n",
      "   test loss: 0.665180\n",
      "   test acc: 0.827925\n",
      "Epoch 521 : Step 203190-203580 of 19500000 took 7.788245s\n",
      "   train loss: 0.425339\n",
      "   train acc: 0.897135\n",
      "   test loss: 0.665107\n",
      "   test acc: 0.828125\n",
      "Epoch 522 : Step 203580-203970 of 19500000 took 7.774605s\n",
      "   train loss: 0.428485\n",
      "   train acc: 0.894371\n",
      "   test loss: 0.644685\n",
      "   test acc: 0.837139\n",
      "Epoch 523 : Step 203970-204360 of 19500000 took 7.773232s\n",
      "   train loss: 0.427502\n",
      "   train acc: 0.893349\n",
      "   test loss: 0.671784\n",
      "   test acc: 0.830128\n",
      "Epoch 524 : Step 204360-204750 of 19500000 took 7.809115s\n",
      "   train loss: 0.419562\n",
      "   train acc: 0.898217\n",
      "   test loss: 0.654253\n",
      "   test acc: 0.838642\n",
      "Epoch 525 : Step 204750-205140 of 19500000 took 7.846860s\n",
      "   train loss: 0.419715\n",
      "   train acc: 0.897676\n",
      "   test loss: 0.673022\n",
      "   test acc: 0.834034\n",
      "Epoch 526 : Step 205140-205530 of 19500000 took 7.812783s\n",
      "   train loss: 0.429553\n",
      "   train acc: 0.894331\n",
      "   test loss: 0.674637\n",
      "   test acc: 0.831030\n",
      "Epoch 527 : Step 205530-205920 of 19500000 took 7.830211s\n",
      "   train loss: 0.425729\n",
      "   train acc: 0.895292\n",
      "   test loss: 0.663583\n",
      "   test acc: 0.836939\n",
      "Epoch 528 : Step 205920-206310 of 19500000 took 7.819302s\n",
      "   train loss: 0.427333\n",
      "   train acc: 0.895132\n",
      "   test loss: 0.685478\n",
      "   test acc: 0.831030\n",
      "Epoch 529 : Step 206310-206700 of 19500000 took 7.773081s\n",
      "   train loss: 0.422035\n",
      "   train acc: 0.898878\n",
      "   test loss: 0.664492\n",
      "   test acc: 0.834335\n",
      "Epoch 530 : Step 206700-207090 of 19500000 took 7.879752s\n",
      "   train loss: 0.422062\n",
      "   train acc: 0.896895\n",
      "   test loss: 0.659549\n",
      "   test acc: 0.835236\n",
      "Epoch 531 : Step 207090-207480 of 19500000 took 7.852012s\n",
      "   train loss: 0.426294\n",
      "   train acc: 0.895333\n",
      "   test loss: 0.687604\n",
      "   test acc: 0.826122\n",
      "Epoch 532 : Step 207480-207870 of 19500000 took 7.818044s\n",
      "   train loss: 0.425083\n",
      "   train acc: 0.897937\n",
      "   test loss: 0.663524\n",
      "   test acc: 0.833934\n",
      "Epoch 533 : Step 207870-208260 of 19500000 took 7.871398s\n",
      "   train loss: 0.420035\n",
      "   train acc: 0.897476\n",
      "   test loss: 0.649381\n",
      "   test acc: 0.833233\n",
      "Epoch 534 : Step 208260-208650 of 19500000 took 7.831213s\n",
      "   train loss: 0.423330\n",
      "   train acc: 0.897196\n",
      "   test loss: 0.667130\n",
      "   test acc: 0.829026\n",
      "Epoch 535 : Step 208650-209040 of 19500000 took 7.894893s\n",
      "   train loss: 0.424182\n",
      "   train acc: 0.896715\n",
      "   test loss: 0.662765\n",
      "   test acc: 0.833634\n",
      "Epoch 536 : Step 209040-209430 of 19500000 took 7.782449s\n",
      "   train loss: 0.420757\n",
      "   train acc: 0.897536\n",
      "   test loss: 0.649147\n",
      "   test acc: 0.839343\n",
      "Epoch 537 : Step 209430-209820 of 19500000 took 7.821767s\n",
      "   train loss: 0.422800\n",
      "   train acc: 0.896414\n",
      "   test loss: 0.663418\n",
      "   test acc: 0.835938\n",
      "Epoch 538 : Step 209820-210210 of 19500000 took 7.836709s\n",
      "   train loss: 0.423508\n",
      "   train acc: 0.896534\n",
      "   test loss: 0.657010\n",
      "   test acc: 0.839443\n",
      "Epoch 539 : Step 210210-210600 of 19500000 took 7.960140s\n",
      "   train loss: 0.422208\n",
      "   train acc: 0.896795\n",
      "   test loss: 0.669972\n",
      "   test acc: 0.831831\n",
      "Epoch 540 : Step 210600-210990 of 19500000 took 7.783420s\n",
      "   train loss: 0.422491\n",
      "   train acc: 0.897556\n",
      "   test loss: 0.672280\n",
      "   test acc: 0.831130\n",
      "Epoch 541 : Step 210990-211380 of 19500000 took 7.811995s\n",
      "   train loss: 0.420725\n",
      "   train acc: 0.897296\n",
      "   test loss: 0.661564\n",
      "   test acc: 0.830429\n",
      "Epoch 542 : Step 211380-211770 of 19500000 took 7.874174s\n",
      "   train loss: 0.424834\n",
      "   train acc: 0.896434\n",
      "   test loss: 0.667736\n",
      "   test acc: 0.833033\n",
      "Epoch 543 : Step 211770-212160 of 19500000 took 7.868354s\n",
      "   train loss: 0.418223\n",
      "   train acc: 0.897196\n",
      "   test loss: 0.671936\n",
      "   test acc: 0.828726\n",
      "Epoch 544 : Step 212160-212550 of 19500000 took 7.838327s\n",
      "   train loss: 0.417554\n",
      "   train acc: 0.898738\n",
      "   test loss: 0.650444\n",
      "   test acc: 0.835537\n",
      "Epoch 545 : Step 212550-212940 of 19500000 took 7.902632s\n",
      "   train loss: 0.425765\n",
      "   train acc: 0.896775\n",
      "   test loss: 0.670253\n",
      "   test acc: 0.827524\n",
      "Epoch 546 : Step 212940-213330 of 19500000 took 7.828280s\n",
      "   train loss: 0.418088\n",
      "   train acc: 0.896394\n",
      "   test loss: 0.666061\n",
      "   test acc: 0.833433\n",
      "Epoch 547 : Step 213330-213720 of 19500000 took 7.809383s\n",
      "   train loss: 0.419622\n",
      "   train acc: 0.897756\n",
      "   test loss: 0.671217\n",
      "   test acc: 0.831831\n",
      "Epoch 548 : Step 213720-214110 of 19500000 took 7.821719s\n",
      "   train loss: 0.420989\n",
      "   train acc: 0.897917\n",
      "   test loss: 0.671304\n",
      "   test acc: 0.833834\n",
      "Epoch 549 : Step 214110-214500 of 19500000 took 7.789460s\n",
      "   train loss: 0.420416\n",
      "   train acc: 0.898257\n",
      "   test loss: 0.667561\n",
      "   test acc: 0.833634\n",
      "Save model !!!!!!!!!!\n",
      "[TL] [*] Saving TL params into model.npz\n",
      "[TL] [*] Saved\n",
      "Epoch 550 : Step 214500-214890 of 19500000 took 7.793271s\n",
      "   train loss: 0.420316\n",
      "   train acc: 0.897436\n",
      "   test loss: 0.671606\n",
      "   test acc: 0.830829\n",
      "Epoch 551 : Step 214890-215280 of 19500000 took 7.837025s\n",
      "   train loss: 0.420916\n",
      "   train acc: 0.897436\n",
      "   test loss: 0.653858\n",
      "   test acc: 0.834535\n",
      "Epoch 552 : Step 215280-215670 of 19500000 took 7.874676s\n",
      "   train loss: 0.416602\n",
      "   train acc: 0.899619\n",
      "   test loss: 0.659015\n",
      "   test acc: 0.832532\n",
      "Epoch 553 : Step 215670-216060 of 19500000 took 7.847249s\n",
      "   train loss: 0.419375\n",
      "   train acc: 0.897857\n",
      "   test loss: 0.676343\n",
      "   test acc: 0.830529\n",
      "Epoch 554 : Step 216060-216450 of 19500000 took 7.822049s\n",
      "   train loss: 0.419073\n",
      "   train acc: 0.898878\n",
      "   test loss: 0.655654\n",
      "   test acc: 0.835837\n",
      "Epoch 555 : Step 216450-216840 of 19500000 took 7.800223s\n",
      "   train loss: 0.418658\n",
      "   train acc: 0.897476\n",
      "   test loss: 0.665386\n",
      "   test acc: 0.832131\n",
      "Epoch 556 : Step 216840-217230 of 19500000 took 7.802227s\n",
      "   train loss: 0.417526\n",
      "   train acc: 0.898678\n",
      "   test loss: 0.691936\n",
      "   test acc: 0.827023\n",
      "Epoch 557 : Step 217230-217620 of 19500000 took 7.837749s\n",
      "   train loss: 0.419720\n",
      "   train acc: 0.898117\n",
      "   test loss: 0.678900\n",
      "   test acc: 0.827224\n",
      "Epoch 558 : Step 217620-218010 of 19500000 took 7.907120s\n",
      "   train loss: 0.423824\n",
      "   train acc: 0.895132\n",
      "   test loss: 0.674441\n",
      "   test acc: 0.828325\n",
      "Epoch 559 : Step 218010-218400 of 19500000 took 7.891453s\n",
      "   train loss: 0.421991\n",
      "   train acc: 0.896354\n",
      "   test loss: 0.657016\n",
      "   test acc: 0.834034\n",
      "Epoch 560 : Step 218400-218790 of 19500000 took 7.882284s\n",
      "   train loss: 0.419653\n",
      "   train acc: 0.898057\n",
      "   test loss: 0.685109\n",
      "   test acc: 0.827123\n",
      "Epoch 561 : Step 218790-219180 of 19500000 took 7.852386s\n",
      "   train loss: 0.419130\n",
      "   train acc: 0.900461\n",
      "   test loss: 0.662002\n",
      "   test acc: 0.833133\n",
      "Epoch 562 : Step 219180-219570 of 19500000 took 7.842908s\n",
      "   train loss: 0.415708\n",
      "   train acc: 0.898878\n",
      "   test loss: 0.662522\n",
      "   test acc: 0.834235\n",
      "Epoch 563 : Step 219570-219960 of 19500000 took 7.847557s\n",
      "   train loss: 0.419163\n",
      "   train acc: 0.898758\n",
      "   test loss: 0.684836\n",
      "   test acc: 0.827424\n",
      "Epoch 564 : Step 219960-220350 of 19500000 took 7.832685s\n",
      "   train loss: 0.414628\n",
      "   train acc: 0.899920\n",
      "   test loss: 0.676488\n",
      "   test acc: 0.830829\n",
      "Epoch 565 : Step 220350-220740 of 19500000 took 7.915589s\n",
      "   train loss: 0.413586\n",
      "   train acc: 0.899018\n",
      "   test loss: 0.676963\n",
      "   test acc: 0.831330\n",
      "Epoch 566 : Step 220740-221130 of 19500000 took 7.916345s\n",
      "   train loss: 0.415283\n",
      "   train acc: 0.898898\n",
      "   test loss: 0.658918\n",
      "   test acc: 0.833934\n",
      "Epoch 567 : Step 221130-221520 of 19500000 took 7.829880s\n",
      "   train loss: 0.418836\n",
      "   train acc: 0.897636\n",
      "   test loss: 0.659260\n",
      "   test acc: 0.833133\n",
      "Epoch 568 : Step 221520-221910 of 19500000 took 7.864398s\n",
      "   train loss: 0.423107\n",
      "   train acc: 0.896534\n",
      "   test loss: 0.654260\n",
      "   test acc: 0.831330\n",
      "Epoch 569 : Step 221910-222300 of 19500000 took 7.835841s\n",
      "   train loss: 0.414938\n",
      "   train acc: 0.900481\n",
      "   test loss: 0.663016\n",
      "   test acc: 0.835737\n",
      "Epoch 570 : Step 222300-222690 of 19500000 took 7.911593s\n",
      "   train loss: 0.419449\n",
      "   train acc: 0.897376\n",
      "   test loss: 0.657424\n",
      "   test acc: 0.833634\n",
      "Epoch 571 : Step 222690-223080 of 19500000 took 7.877915s\n",
      "   train loss: 0.416217\n",
      "   train acc: 0.899740\n",
      "   test loss: 0.664954\n",
      "   test acc: 0.831631\n",
      "Epoch 572 : Step 223080-223470 of 19500000 took 7.865726s\n",
      "   train loss: 0.416655\n",
      "   train acc: 0.898377\n",
      "   test loss: 0.665668\n",
      "   test acc: 0.831030\n",
      "Epoch 573 : Step 223470-223860 of 19500000 took 7.839277s\n",
      "   train loss: 0.414378\n",
      "   train acc: 0.900000\n",
      "   test loss: 0.686223\n",
      "   test acc: 0.827123\n",
      "Epoch 574 : Step 223860-224250 of 19500000 took 7.887814s\n",
      "   train loss: 0.418561\n",
      "   train acc: 0.897656\n",
      "   test loss: 0.654677\n",
      "   test acc: 0.834435\n",
      "Epoch 575 : Step 224250-224640 of 19500000 took 7.900529s\n",
      "   train loss: 0.413793\n",
      "   train acc: 0.900000\n",
      "   test loss: 0.660377\n",
      "   test acc: 0.832131\n",
      "Epoch 576 : Step 224640-225030 of 19500000 took 7.870180s\n",
      "   train loss: 0.409605\n",
      "   train acc: 0.900661\n",
      "   test loss: 0.662913\n",
      "   test acc: 0.833333\n",
      "Epoch 577 : Step 225030-225420 of 19500000 took 7.841037s\n",
      "   train loss: 0.416395\n",
      "   train acc: 0.899038\n",
      "   test loss: 0.669386\n",
      "   test acc: 0.830228\n",
      "Epoch 578 : Step 225420-225810 of 19500000 took 7.895369s\n",
      "   train loss: 0.417229\n",
      "   train acc: 0.898638\n",
      "   test loss: 0.668056\n",
      "   test acc: 0.832232\n",
      "Epoch 579 : Step 225810-226200 of 19500000 took 7.915352s\n",
      "   train loss: 0.408459\n",
      "   train acc: 0.901382\n",
      "   test loss: 0.676755\n",
      "   test acc: 0.832232\n",
      "Epoch 580 : Step 226200-226590 of 19500000 took 7.859890s\n",
      "   train loss: 0.416009\n",
      "   train acc: 0.899159\n",
      "   test loss: 0.679554\n",
      "   test acc: 0.831330\n",
      "Epoch 581 : Step 226590-226980 of 19500000 took 7.917538s\n",
      "   train loss: 0.416329\n",
      "   train acc: 0.899279\n",
      "   test loss: 0.659985\n",
      "   test acc: 0.833834\n",
      "Epoch 582 : Step 226980-227370 of 19500000 took 7.814867s\n",
      "   train loss: 0.421367\n",
      "   train acc: 0.897596\n",
      "   test loss: 0.673851\n",
      "   test acc: 0.831931\n",
      "Epoch 583 : Step 227370-227760 of 19500000 took 7.892700s\n",
      "   train loss: 0.414537\n",
      "   train acc: 0.901362\n",
      "   test loss: 0.668105\n",
      "   test acc: 0.831030\n",
      "Epoch 584 : Step 227760-228150 of 19500000 took 7.938143s\n",
      "   train loss: 0.418774\n",
      "   train acc: 0.898397\n",
      "   test loss: 0.664516\n",
      "   test acc: 0.833734\n",
      "Epoch 585 : Step 228150-228540 of 19500000 took 7.909038s\n",
      "   train loss: 0.418917\n",
      "   train acc: 0.898618\n",
      "   test loss: 0.650287\n",
      "   test acc: 0.837039\n",
      "Epoch 586 : Step 228540-228930 of 19500000 took 7.939212s\n",
      "   train loss: 0.414012\n",
      "   train acc: 0.900120\n",
      "   test loss: 0.670373\n",
      "   test acc: 0.831931\n",
      "Epoch 587 : Step 228930-229320 of 19500000 took 7.863859s\n",
      "   train loss: 0.413422\n",
      "   train acc: 0.901522\n",
      "   test loss: 0.661735\n",
      "   test acc: 0.833133\n",
      "Epoch 588 : Step 229320-229710 of 19500000 took 7.861951s\n",
      "   train loss: 0.412866\n",
      "   train acc: 0.898998\n",
      "   test loss: 0.657149\n",
      "   test acc: 0.837540\n",
      "Epoch 589 : Step 229710-230100 of 19500000 took 7.832142s\n",
      "   train loss: 0.412100\n",
      "   train acc: 0.901542\n",
      "   test loss: 0.670072\n",
      "   test acc: 0.833333\n",
      "Epoch 590 : Step 230100-230490 of 19500000 took 7.946131s\n",
      "   train loss: 0.413831\n",
      "   train acc: 0.899740\n",
      "   test loss: 0.676264\n",
      "   test acc: 0.832332\n",
      "Epoch 591 : Step 230490-230880 of 19500000 took 7.898434s\n",
      "   train loss: 0.415939\n",
      "   train acc: 0.900921\n",
      "   test loss: 0.673948\n",
      "   test acc: 0.830429\n",
      "Epoch 592 : Step 230880-231270 of 19500000 took 7.931661s\n",
      "   train loss: 0.416530\n",
      "   train acc: 0.897476\n",
      "   test loss: 0.674633\n",
      "   test acc: 0.834836\n",
      "Epoch 593 : Step 231270-231660 of 19500000 took 7.865325s\n",
      "   train loss: 0.416056\n",
      "   train acc: 0.899459\n",
      "   test loss: 0.673482\n",
      "   test acc: 0.830128\n",
      "Epoch 594 : Step 231660-232050 of 19500000 took 7.862631s\n",
      "   train loss: 0.406701\n",
      "   train acc: 0.901923\n",
      "   test loss: 0.667141\n",
      "   test acc: 0.834635\n",
      "Epoch 595 : Step 232050-232440 of 19500000 took 7.870746s\n",
      "   train loss: 0.412098\n",
      "   train acc: 0.899940\n",
      "   test loss: 0.666077\n",
      "   test acc: 0.833934\n",
      "Epoch 596 : Step 232440-232830 of 19500000 took 7.893139s\n",
      "   train loss: 0.416804\n",
      "   train acc: 0.898678\n",
      "   test loss: 0.670008\n",
      "   test acc: 0.830429\n",
      "Epoch 597 : Step 232830-233220 of 19500000 took 7.836690s\n",
      "   train loss: 0.409250\n",
      "   train acc: 0.900200\n",
      "   test loss: 0.658612\n",
      "   test acc: 0.832031\n",
      "Epoch 598 : Step 233220-233610 of 19500000 took 7.906738s\n",
      "   train loss: 0.412724\n",
      "   train acc: 0.899679\n",
      "   test loss: 0.669486\n",
      "   test acc: 0.832933\n",
      "Epoch 599 : Step 233610-234000 of 19500000 took 7.952549s\n",
      "   train loss: 0.409293\n",
      "   train acc: 0.900721\n",
      "   test loss: 0.663553\n",
      "   test acc: 0.834736\n",
      "Save model !!!!!!!!!!\n",
      "[TL] [*] Saving TL params into model.npz\n",
      "[TL] [*] Saved\n",
      "Epoch 600 : Step 234000-234390 of 19500000 took 7.884102s\n",
      "   train loss: 0.414648\n",
      "   train acc: 0.899099\n",
      "   test loss: 0.657829\n",
      "   test acc: 0.838141\n",
      "Epoch 601 : Step 234390-234780 of 19500000 took 7.910148s\n",
      "   train loss: 0.410755\n",
      "   train acc: 0.900220\n",
      "   test loss: 0.654810\n",
      "   test acc: 0.839343\n",
      "Epoch 602 : Step 234780-235170 of 19500000 took 7.805382s\n",
      "   train loss: 0.413209\n",
      "   train acc: 0.900260\n",
      "   test loss: 0.651702\n",
      "   test acc: 0.834936\n",
      "Epoch 603 : Step 235170-235560 of 19500000 took 7.869744s\n",
      "   train loss: 0.417744\n",
      "   train acc: 0.898958\n",
      "   test loss: 0.672065\n",
      "   test acc: 0.834635\n",
      "Epoch 604 : Step 235560-235950 of 19500000 took 7.899416s\n",
      "   train loss: 0.406394\n",
      "   train acc: 0.903385\n",
      "   test loss: 0.654715\n",
      "   test acc: 0.838041\n",
      "Epoch 605 : Step 235950-236340 of 19500000 took 7.881264s\n",
      "   train loss: 0.409142\n",
      "   train acc: 0.900841\n",
      "   test loss: 0.668370\n",
      "   test acc: 0.833534\n",
      "Epoch 606 : Step 236340-236730 of 19500000 took 7.884384s\n",
      "   train loss: 0.415647\n",
      "   train acc: 0.898738\n",
      "   test loss: 0.659808\n",
      "   test acc: 0.834235\n",
      "Epoch 607 : Step 236730-237120 of 19500000 took 7.895165s\n",
      "   train loss: 0.410886\n",
      "   train acc: 0.900881\n",
      "   test loss: 0.662405\n",
      "   test acc: 0.831330\n",
      "Epoch 608 : Step 237120-237510 of 19500000 took 7.896256s\n",
      "   train loss: 0.409362\n",
      "   train acc: 0.901943\n",
      "   test loss: 0.675878\n",
      "   test acc: 0.833634\n",
      "Epoch 609 : Step 237510-237900 of 19500000 took 7.921913s\n",
      "   train loss: 0.411107\n",
      "   train acc: 0.901142\n",
      "   test loss: 0.650375\n",
      "   test acc: 0.837941\n",
      "Epoch 610 : Step 237900-238290 of 19500000 took 7.875135s\n",
      "   train loss: 0.413535\n",
      "   train acc: 0.900881\n",
      "   test loss: 0.658801\n",
      "   test acc: 0.835737\n",
      "Epoch 611 : Step 238290-238680 of 19500000 took 7.898284s\n",
      "   train loss: 0.413341\n",
      "   train acc: 0.900441\n",
      "   test loss: 0.661496\n",
      "   test acc: 0.835737\n",
      "Epoch 612 : Step 238680-239070 of 19500000 took 7.923273s\n",
      "   train loss: 0.406610\n",
      "   train acc: 0.903065\n",
      "   test loss: 0.687609\n",
      "   test acc: 0.830228\n",
      "Epoch 613 : Step 239070-239460 of 19500000 took 7.867944s\n",
      "   train loss: 0.408543\n",
      "   train acc: 0.902925\n",
      "   test loss: 0.659739\n",
      "   test acc: 0.834535\n",
      "Epoch 614 : Step 239460-239850 of 19500000 took 7.846644s\n",
      "   train loss: 0.411901\n",
      "   train acc: 0.899579\n",
      "   test loss: 0.651650\n",
      "   test acc: 0.837640\n",
      "Epoch 615 : Step 239850-240240 of 19500000 took 7.870464s\n",
      "   train loss: 0.410325\n",
      "   train acc: 0.899960\n",
      "   test loss: 0.653254\n",
      "   test acc: 0.838041\n",
      "Epoch 616 : Step 240240-240630 of 19500000 took 7.842097s\n",
      "   train loss: 0.407366\n",
      "   train acc: 0.901823\n",
      "   test loss: 0.655260\n",
      "   test acc: 0.837540\n",
      "Epoch 617 : Step 240630-241020 of 19500000 took 7.863553s\n",
      "   train loss: 0.408433\n",
      "   train acc: 0.902364\n",
      "   test loss: 0.660377\n",
      "   test acc: 0.834235\n",
      "Epoch 618 : Step 241020-241410 of 19500000 took 7.924917s\n",
      "   train loss: 0.409180\n",
      "   train acc: 0.900621\n",
      "   test loss: 0.652722\n",
      "   test acc: 0.834836\n",
      "Epoch 619 : Step 241410-241800 of 19500000 took 7.932704s\n",
      "   train loss: 0.409767\n",
      "   train acc: 0.902444\n",
      "   test loss: 0.656652\n",
      "   test acc: 0.834435\n",
      "Epoch 620 : Step 241800-242190 of 19500000 took 7.910089s\n",
      "   train loss: 0.408058\n",
      "   train acc: 0.901863\n",
      "   test loss: 0.677928\n",
      "   test acc: 0.832031\n",
      "Epoch 621 : Step 242190-242580 of 19500000 took 7.880804s\n",
      "   train loss: 0.404226\n",
      "   train acc: 0.902224\n",
      "   test loss: 0.665535\n",
      "   test acc: 0.834635\n",
      "Epoch 622 : Step 242580-242970 of 19500000 took 7.807445s\n",
      "   train loss: 0.411064\n",
      "   train acc: 0.900942\n",
      "   test loss: 0.652710\n",
      "   test acc: 0.834135\n",
      "Epoch 623 : Step 242970-243360 of 19500000 took 7.835407s\n",
      "   train loss: 0.405965\n",
      "   train acc: 0.902304\n",
      "   test loss: 0.677981\n",
      "   test acc: 0.834535\n",
      "Epoch 624 : Step 243360-243750 of 19500000 took 7.890902s\n",
      "   train loss: 0.414551\n",
      "   train acc: 0.898658\n",
      "   test loss: 0.682506\n",
      "   test acc: 0.832632\n",
      "Epoch 625 : Step 243750-244140 of 19500000 took 7.913428s\n",
      "   train loss: 0.410542\n",
      "   train acc: 0.900541\n",
      "   test loss: 0.678022\n",
      "   test acc: 0.835337\n",
      "Epoch 626 : Step 244140-244530 of 19500000 took 7.870727s\n",
      "   train loss: 0.411282\n",
      "   train acc: 0.900781\n",
      "   test loss: 0.650408\n",
      "   test acc: 0.840345\n",
      "Epoch 627 : Step 244530-244920 of 19500000 took 7.875404s\n",
      "   train loss: 0.409731\n",
      "   train acc: 0.902584\n",
      "   test loss: 0.647292\n",
      "   test acc: 0.838542\n",
      "Epoch 628 : Step 244920-245310 of 19500000 took 7.855319s\n",
      "   train loss: 0.406973\n",
      "   train acc: 0.902284\n",
      "   test loss: 0.666790\n",
      "   test acc: 0.834034\n",
      "Epoch 629 : Step 245310-245700 of 19500000 took 7.882504s\n",
      "   train loss: 0.408785\n",
      "   train acc: 0.902304\n",
      "   test loss: 0.657914\n",
      "   test acc: 0.837540\n",
      "Epoch 630 : Step 245700-246090 of 19500000 took 7.915529s\n",
      "   train loss: 0.409696\n",
      "   train acc: 0.902624\n",
      "   test loss: 0.646965\n",
      "   test acc: 0.838642\n",
      "Epoch 631 : Step 246090-246480 of 19500000 took 7.891809s\n",
      "   train loss: 0.407373\n",
      "   train acc: 0.901623\n",
      "   test loss: 0.667855\n",
      "   test acc: 0.832232\n",
      "Epoch 632 : Step 246480-246870 of 19500000 took 7.921013s\n",
      "   train loss: 0.413295\n",
      "   train acc: 0.900160\n",
      "   test loss: 0.643890\n",
      "   test acc: 0.837640\n",
      "Epoch 633 : Step 246870-247260 of 19500000 took 7.970542s\n",
      "   train loss: 0.401665\n",
      "   train acc: 0.904046\n",
      "   test loss: 0.677506\n",
      "   test acc: 0.831130\n",
      "Epoch 634 : Step 247260-247650 of 19500000 took 7.869743s\n",
      "   train loss: 0.414187\n",
      "   train acc: 0.899259\n",
      "   test loss: 0.655895\n",
      "   test acc: 0.836639\n",
      "Epoch 635 : Step 247650-248040 of 19500000 took 7.928342s\n",
      "   train loss: 0.405672\n",
      "   train acc: 0.902744\n",
      "   test loss: 0.662339\n",
      "   test acc: 0.835637\n",
      "Epoch 636 : Step 248040-248430 of 19500000 took 7.865183s\n",
      "   train loss: 0.405248\n",
      "   train acc: 0.902464\n",
      "   test loss: 0.648247\n",
      "   test acc: 0.840345\n",
      "Epoch 637 : Step 248430-248820 of 19500000 took 7.917125s\n",
      "   train loss: 0.410400\n",
      "   train acc: 0.901222\n",
      "   test loss: 0.649851\n",
      "   test acc: 0.833433\n",
      "Epoch 638 : Step 248820-249210 of 19500000 took 7.890960s\n",
      "   train loss: 0.401472\n",
      "   train acc: 0.904327\n",
      "   test loss: 0.654305\n",
      "   test acc: 0.833133\n",
      "Epoch 639 : Step 249210-249600 of 19500000 took 7.891033s\n",
      "   train loss: 0.407346\n",
      "   train acc: 0.902784\n",
      "   test loss: 0.659914\n",
      "   test acc: 0.838341\n",
      "Epoch 640 : Step 249600-249990 of 19500000 took 8.019545s\n",
      "   train loss: 0.407470\n",
      "   train acc: 0.900901\n",
      "   test loss: 0.660823\n",
      "   test acc: 0.833634\n",
      "Epoch 641 : Step 249990-250380 of 19500000 took 7.870872s\n",
      "   train loss: 0.402654\n",
      "   train acc: 0.902905\n",
      "   test loss: 0.662562\n",
      "   test acc: 0.832232\n",
      "Epoch 642 : Step 250380-250770 of 19500000 took 7.928007s\n",
      "   train loss: 0.408970\n",
      "   train acc: 0.900801\n",
      "   test loss: 0.658443\n",
      "   test acc: 0.834635\n",
      "Epoch 643 : Step 250770-251160 of 19500000 took 7.889831s\n",
      "   train loss: 0.404933\n",
      "   train acc: 0.904167\n",
      "   test loss: 0.658983\n",
      "   test acc: 0.838642\n",
      "Epoch 644 : Step 251160-251550 of 19500000 took 7.814450s\n",
      "   train loss: 0.403794\n",
      "   train acc: 0.902885\n",
      "   test loss: 0.661231\n",
      "   test acc: 0.835437\n",
      "Epoch 645 : Step 251550-251940 of 19500000 took 7.951514s\n",
      "   train loss: 0.401692\n",
      "   train acc: 0.904006\n",
      "   test loss: 0.675009\n",
      "   test acc: 0.831631\n",
      "Epoch 646 : Step 251940-252330 of 19500000 took 7.947397s\n",
      "   train loss: 0.402876\n",
      "   train acc: 0.904407\n",
      "   test loss: 0.669116\n",
      "   test acc: 0.830929\n",
      "Epoch 647 : Step 252330-252720 of 19500000 took 7.859972s\n",
      "   train loss: 0.404691\n",
      "   train acc: 0.901623\n",
      "   test loss: 0.665874\n",
      "   test acc: 0.829828\n",
      "Epoch 648 : Step 252720-253110 of 19500000 took 7.920516s\n",
      "   train loss: 0.404844\n",
      "   train acc: 0.902204\n",
      "   test loss: 0.652982\n",
      "   test acc: 0.836138\n",
      "Epoch 649 : Step 253110-253500 of 19500000 took 7.866192s\n",
      "   train loss: 0.403702\n",
      "   train acc: 0.902985\n",
      "   test loss: 0.662996\n",
      "   test acc: 0.834736\n",
      "Save model !!!!!!!!!!\n",
      "[TL] [*] Saving TL params into model.npz\n",
      "[TL] [*] Saved\n",
      "Epoch 650 : Step 253500-253890 of 19500000 took 7.823827s\n",
      "   train loss: 0.406496\n",
      "   train acc: 0.902644\n",
      "   test loss: 0.668144\n",
      "   test acc: 0.831530\n",
      "Epoch 651 : Step 253890-254280 of 19500000 took 7.963111s\n",
      "   train loss: 0.399900\n",
      "   train acc: 0.904467\n",
      "   test loss: 0.666147\n",
      "   test acc: 0.836438\n",
      "Epoch 652 : Step 254280-254670 of 19500000 took 7.905935s\n",
      "   train loss: 0.403679\n",
      "   train acc: 0.903746\n",
      "   test loss: 0.669622\n",
      "   test acc: 0.834836\n",
      "Epoch 653 : Step 254670-255060 of 19500000 took 7.904089s\n",
      "   train loss: 0.407037\n",
      "   train acc: 0.901703\n",
      "   test loss: 0.660233\n",
      "   test acc: 0.836038\n",
      "Epoch 654 : Step 255060-255450 of 19500000 took 7.870632s\n",
      "   train loss: 0.401739\n",
      "   train acc: 0.904247\n",
      "   test loss: 0.669356\n",
      "   test acc: 0.833734\n",
      "Epoch 655 : Step 255450-255840 of 19500000 took 9.372146s\n",
      "   train loss: 0.404155\n",
      "   train acc: 0.905649\n",
      "   test loss: 0.654185\n",
      "   test acc: 0.834936\n",
      "Epoch 656 : Step 255840-256230 of 19500000 took 7.891256s\n",
      "   train loss: 0.403008\n",
      "   train acc: 0.904067\n",
      "   test loss: 0.657322\n",
      "   test acc: 0.836438\n",
      "Epoch 657 : Step 256230-256620 of 19500000 took 7.895697s\n",
      "   train loss: 0.399833\n",
      "   train acc: 0.905048\n",
      "   test loss: 0.660667\n",
      "   test acc: 0.833634\n",
      "Epoch 658 : Step 256620-257010 of 19500000 took 7.947658s\n",
      "   train loss: 0.401300\n",
      "   train acc: 0.903766\n",
      "   test loss: 0.665456\n",
      "   test acc: 0.837440\n",
      "Epoch 659 : Step 257010-257400 of 19500000 took 7.899225s\n",
      "   train loss: 0.404653\n",
      "   train acc: 0.903626\n",
      "   test loss: 0.673477\n",
      "   test acc: 0.832732\n",
      "Epoch 660 : Step 257400-257790 of 19500000 took 7.917253s\n",
      "   train loss: 0.403493\n",
      "   train acc: 0.902644\n",
      "   test loss: 0.644636\n",
      "   test acc: 0.836739\n",
      "Epoch 661 : Step 257790-258180 of 19500000 took 7.927716s\n",
      "   train loss: 0.403626\n",
      "   train acc: 0.903225\n",
      "   test loss: 0.654197\n",
      "   test acc: 0.834435\n",
      "Epoch 662 : Step 258180-258570 of 19500000 took 7.872798s\n",
      "   train loss: 0.399906\n",
      "   train acc: 0.904247\n",
      "   test loss: 0.650606\n",
      "   test acc: 0.837340\n",
      "Epoch 663 : Step 258570-258960 of 19500000 took 7.913599s\n",
      "   train loss: 0.400217\n",
      "   train acc: 0.905389\n",
      "   test loss: 0.649626\n",
      "   test acc: 0.838241\n",
      "Epoch 664 : Step 258960-259350 of 19500000 took 7.909680s\n",
      "   train loss: 0.404126\n",
      "   train acc: 0.904046\n",
      "   test loss: 0.648783\n",
      "   test acc: 0.836939\n",
      "Epoch 665 : Step 259350-259740 of 19500000 took 7.942957s\n",
      "   train loss: 0.403562\n",
      "   train acc: 0.902845\n",
      "   test loss: 0.655079\n",
      "   test acc: 0.841046\n",
      "Epoch 666 : Step 259740-260130 of 19500000 took 7.958413s\n",
      "   train loss: 0.402954\n",
      "   train acc: 0.904006\n",
      "   test loss: 0.658449\n",
      "   test acc: 0.835537\n",
      "Epoch 667 : Step 260130-260520 of 19500000 took 7.878384s\n",
      "   train loss: 0.404283\n",
      "   train acc: 0.903105\n",
      "   test loss: 0.664441\n",
      "   test acc: 0.836639\n",
      "Epoch 668 : Step 260520-260910 of 19500000 took 7.898193s\n",
      "   train loss: 0.403193\n",
      "   train acc: 0.902885\n",
      "   test loss: 0.662538\n",
      "   test acc: 0.834135\n",
      "Epoch 669 : Step 260910-261300 of 19500000 took 7.911312s\n",
      "   train loss: 0.399989\n",
      "   train acc: 0.904167\n",
      "   test loss: 0.695399\n",
      "   test acc: 0.832732\n",
      "Epoch 670 : Step 261300-261690 of 19500000 took 7.881016s\n",
      "   train loss: 0.402838\n",
      "   train acc: 0.903285\n",
      "   test loss: 0.658240\n",
      "   test acc: 0.835737\n",
      "Epoch 671 : Step 261690-262080 of 19500000 took 7.929222s\n",
      "   train loss: 0.400173\n",
      "   train acc: 0.905449\n",
      "   test loss: 0.652067\n",
      "   test acc: 0.836038\n",
      "Epoch 672 : Step 262080-262470 of 19500000 took 7.987112s\n",
      "   train loss: 0.402162\n",
      "   train acc: 0.903846\n",
      "   test loss: 0.671849\n",
      "   test acc: 0.834135\n",
      "Epoch 673 : Step 262470-262860 of 19500000 took 7.941367s\n",
      "   train loss: 0.400673\n",
      "   train acc: 0.905349\n",
      "   test loss: 0.664476\n",
      "   test acc: 0.834936\n",
      "Epoch 674 : Step 262860-263250 of 19500000 took 7.916409s\n",
      "   train loss: 0.399777\n",
      "   train acc: 0.903626\n",
      "   test loss: 0.653407\n",
      "   test acc: 0.835637\n",
      "Epoch 675 : Step 263250-263640 of 19500000 took 7.889856s\n",
      "   train loss: 0.395624\n",
      "   train acc: 0.906010\n",
      "   test loss: 0.670336\n",
      "   test acc: 0.833834\n",
      "Epoch 676 : Step 263640-264030 of 19500000 took 7.930527s\n",
      "   train loss: 0.404854\n",
      "   train acc: 0.904768\n",
      "   test loss: 0.673015\n",
      "   test acc: 0.835737\n",
      "Epoch 677 : Step 264030-264420 of 19500000 took 7.921506s\n",
      "   train loss: 0.404477\n",
      "   train acc: 0.901362\n",
      "   test loss: 0.660579\n",
      "   test acc: 0.834836\n",
      "Epoch 678 : Step 264420-264810 of 19500000 took 7.966743s\n",
      "   train loss: 0.401037\n",
      "   train acc: 0.903906\n",
      "   test loss: 0.673835\n",
      "   test acc: 0.829427\n",
      "Epoch 679 : Step 264810-265200 of 19500000 took 7.919026s\n",
      "   train loss: 0.398946\n",
      "   train acc: 0.904447\n",
      "   test loss: 0.662474\n",
      "   test acc: 0.835837\n",
      "Epoch 680 : Step 265200-265590 of 19500000 took 7.954894s\n",
      "   train loss: 0.396369\n",
      "   train acc: 0.906851\n",
      "   test loss: 0.651897\n",
      "   test acc: 0.836238\n",
      "Epoch 681 : Step 265590-265980 of 19500000 took 7.894475s\n",
      "   train loss: 0.400413\n",
      "   train acc: 0.904487\n",
      "   test loss: 0.665136\n",
      "   test acc: 0.836338\n",
      "Epoch 682 : Step 265980-266370 of 19500000 took 7.891113s\n",
      "   train loss: 0.402258\n",
      "   train acc: 0.903886\n",
      "   test loss: 0.664633\n",
      "   test acc: 0.838542\n",
      "Epoch 683 : Step 266370-266760 of 19500000 took 7.897323s\n",
      "   train loss: 0.395631\n",
      "   train acc: 0.906851\n",
      "   test loss: 0.664223\n",
      "   test acc: 0.839042\n",
      "Epoch 684 : Step 266760-267150 of 19500000 took 7.941894s\n",
      "   train loss: 0.401140\n",
      "   train acc: 0.904748\n",
      "   test loss: 0.684756\n",
      "   test acc: 0.832532\n",
      "Epoch 685 : Step 267150-267540 of 19500000 took 7.969579s\n",
      "   train loss: 0.400211\n",
      "   train acc: 0.904407\n",
      "   test loss: 0.661006\n",
      "   test acc: 0.836939\n",
      "Epoch 686 : Step 267540-267930 of 19500000 took 7.931559s\n",
      "   train loss: 0.392049\n",
      "   train acc: 0.907091\n",
      "   test loss: 0.675379\n",
      "   test acc: 0.834135\n",
      "Epoch 687 : Step 267930-268320 of 19500000 took 7.889443s\n",
      "   train loss: 0.399774\n",
      "   train acc: 0.905108\n",
      "   test loss: 0.665103\n",
      "   test acc: 0.837540\n",
      "Epoch 688 : Step 268320-268710 of 19500000 took 7.908864s\n",
      "   train loss: 0.397376\n",
      "   train acc: 0.905869\n",
      "   test loss: 0.671858\n",
      "   test acc: 0.836739\n",
      "Epoch 689 : Step 268710-269100 of 19500000 took 7.893958s\n",
      "   train loss: 0.398072\n",
      "   train acc: 0.905970\n",
      "   test loss: 0.674227\n",
      "   test acc: 0.834435\n",
      "Epoch 690 : Step 269100-269490 of 19500000 took 7.923392s\n",
      "   train loss: 0.401390\n",
      "   train acc: 0.904307\n",
      "   test loss: 0.700807\n",
      "   test acc: 0.822416\n",
      "Epoch 691 : Step 269490-269880 of 19500000 took 7.967885s\n",
      "   train loss: 0.401017\n",
      "   train acc: 0.904487\n",
      "   test loss: 0.669935\n",
      "   test acc: 0.835837\n",
      "Epoch 692 : Step 269880-270270 of 19500000 took 7.907112s\n",
      "   train loss: 0.397490\n",
      "   train acc: 0.905970\n",
      "   test loss: 0.662999\n",
      "   test acc: 0.836138\n",
      "Epoch 693 : Step 270270-270660 of 19500000 took 7.956621s\n",
      "   train loss: 0.396091\n",
      "   train acc: 0.905689\n",
      "   test loss: 0.663478\n",
      "   test acc: 0.833934\n",
      "Epoch 694 : Step 270660-271050 of 19500000 took 7.944258s\n",
      "   train loss: 0.395286\n",
      "   train acc: 0.905950\n",
      "   test loss: 0.676232\n",
      "   test acc: 0.831430\n",
      "Epoch 695 : Step 271050-271440 of 19500000 took 7.869483s\n",
      "   train loss: 0.395676\n",
      "   train acc: 0.906911\n",
      "   test loss: 0.658039\n",
      "   test acc: 0.836238\n",
      "Epoch 696 : Step 271440-271830 of 19500000 took 7.860683s\n",
      "   train loss: 0.400793\n",
      "   train acc: 0.903526\n",
      "   test loss: 0.667570\n",
      "   test acc: 0.834435\n",
      "Epoch 697 : Step 271830-272220 of 19500000 took 7.925976s\n",
      "   train loss: 0.394386\n",
      "   train acc: 0.906611\n",
      "   test loss: 0.686858\n",
      "   test acc: 0.832232\n",
      "Epoch 698 : Step 272220-272610 of 19500000 took 7.970655s\n",
      "   train loss: 0.398074\n",
      "   train acc: 0.904768\n",
      "   test loss: 0.674058\n",
      "   test acc: 0.831831\n",
      "Epoch 699 : Step 272610-273000 of 19500000 took 8.016195s\n",
      "   train loss: 0.395944\n",
      "   train acc: 0.907011\n",
      "   test loss: 0.678752\n",
      "   test acc: 0.833634\n",
      "Save model !!!!!!!!!!\n",
      "[TL] [*] Saving TL params into model.npz\n",
      "[TL] [*] Saved\n",
      "Epoch 700 : Step 273000-273390 of 19500000 took 7.890068s\n",
      "   train loss: 0.396837\n",
      "   train acc: 0.905088\n",
      "   test loss: 0.684043\n",
      "   test acc: 0.828826\n",
      "Epoch 701 : Step 273390-273780 of 19500000 took 7.891117s\n",
      "   train loss: 0.400985\n",
      "   train acc: 0.904888\n",
      "   test loss: 0.668653\n",
      "   test acc: 0.834635\n",
      "Epoch 702 : Step 273780-274170 of 19500000 took 7.858016s\n",
      "   train loss: 0.392328\n",
      "   train acc: 0.907412\n",
      "   test loss: 0.668942\n",
      "   test acc: 0.837039\n",
      "Epoch 703 : Step 274170-274560 of 19500000 took 7.872605s\n",
      "   train loss: 0.396319\n",
      "   train acc: 0.906190\n",
      "   test loss: 0.675568\n",
      "   test acc: 0.831130\n",
      "Epoch 704 : Step 274560-274950 of 19500000 took 7.897893s\n",
      "   train loss: 0.395545\n",
      "   train acc: 0.906591\n",
      "   test loss: 0.659384\n",
      "   test acc: 0.836739\n",
      "Epoch 705 : Step 274950-275340 of 19500000 took 7.951353s\n",
      "   train loss: 0.394985\n",
      "   train acc: 0.906651\n",
      "   test loss: 0.664765\n",
      "   test acc: 0.837139\n",
      "Epoch 706 : Step 275340-275730 of 19500000 took 7.889306s\n",
      "   train loss: 0.393223\n",
      "   train acc: 0.906330\n",
      "   test loss: 0.684353\n",
      "   test acc: 0.833133\n",
      "Epoch 707 : Step 275730-276120 of 19500000 took 7.930240s\n",
      "   train loss: 0.398345\n",
      "   train acc: 0.905268\n",
      "   test loss: 0.668960\n",
      "   test acc: 0.837841\n",
      "Epoch 708 : Step 276120-276510 of 19500000 took 7.850974s\n",
      "   train loss: 0.397109\n",
      "   train acc: 0.905990\n",
      "   test loss: 0.666648\n",
      "   test acc: 0.833033\n",
      "Epoch 709 : Step 276510-276900 of 19500000 took 7.900653s\n",
      "   train loss: 0.391076\n",
      "   train acc: 0.906571\n",
      "   test loss: 0.672099\n",
      "   test acc: 0.833834\n",
      "Epoch 710 : Step 276900-277290 of 19500000 took 7.853654s\n",
      "   train loss: 0.394631\n",
      "   train acc: 0.905629\n",
      "   test loss: 0.673964\n",
      "   test acc: 0.836238\n",
      "Epoch 711 : Step 277290-277680 of 19500000 took 7.943357s\n",
      "   train loss: 0.395475\n",
      "   train acc: 0.906550\n",
      "   test loss: 0.659274\n",
      "   test acc: 0.837240\n",
      "Epoch 712 : Step 277680-278070 of 19500000 took 7.924207s\n",
      "   train loss: 0.398294\n",
      "   train acc: 0.905569\n",
      "   test loss: 0.682998\n",
      "   test acc: 0.833834\n",
      "Epoch 713 : Step 278070-278460 of 19500000 took 7.939687s\n",
      "   train loss: 0.399707\n",
      "   train acc: 0.904988\n",
      "   test loss: 0.662833\n",
      "   test acc: 0.835737\n",
      "Epoch 714 : Step 278460-278850 of 19500000 took 7.945436s\n",
      "   train loss: 0.395797\n",
      "   train acc: 0.906150\n",
      "   test loss: 0.685236\n",
      "   test acc: 0.831731\n",
      "Epoch 715 : Step 278850-279240 of 19500000 took 7.875213s\n",
      "   train loss: 0.396970\n",
      "   train acc: 0.907252\n",
      "   test loss: 0.661278\n",
      "   test acc: 0.836338\n",
      "Epoch 716 : Step 279240-279630 of 19500000 took 7.874893s\n",
      "   train loss: 0.389234\n",
      "   train acc: 0.908273\n",
      "   test loss: 0.676257\n",
      "   test acc: 0.833033\n",
      "Epoch 717 : Step 279630-280020 of 19500000 took 7.885826s\n",
      "   train loss: 0.398163\n",
      "   train acc: 0.905148\n",
      "   test loss: 0.654173\n",
      "   test acc: 0.839143\n",
      "Epoch 718 : Step 280020-280410 of 19500000 took 7.951862s\n",
      "   train loss: 0.396590\n",
      "   train acc: 0.906951\n",
      "   test loss: 0.680843\n",
      "   test acc: 0.836538\n",
      "Epoch 719 : Step 280410-280800 of 19500000 took 7.878129s\n",
      "   train loss: 0.391537\n",
      "   train acc: 0.909075\n",
      "   test loss: 0.657901\n",
      "   test acc: 0.840645\n",
      "Epoch 720 : Step 280800-281190 of 19500000 took 7.893669s\n",
      "   train loss: 0.395143\n",
      "   train acc: 0.907352\n",
      "   test loss: 0.675497\n",
      "   test acc: 0.832732\n",
      "Epoch 721 : Step 281190-281580 of 19500000 took 7.894134s\n",
      "   train loss: 0.393452\n",
      "   train acc: 0.907232\n",
      "   test loss: 0.678440\n",
      "   test acc: 0.831530\n",
      "Epoch 722 : Step 281580-281970 of 19500000 took 7.855985s\n",
      "   train loss: 0.394948\n",
      "   train acc: 0.906611\n",
      "   test loss: 0.673020\n",
      "   test acc: 0.836739\n",
      "Epoch 723 : Step 281970-282360 of 19500000 took 7.856143s\n",
      "   train loss: 0.397261\n",
      "   train acc: 0.906871\n",
      "   test loss: 0.669840\n",
      "   test acc: 0.834435\n",
      "Epoch 724 : Step 282360-282750 of 19500000 took 7.933852s\n",
      "   train loss: 0.393133\n",
      "   train acc: 0.907091\n",
      "   test loss: 0.658595\n",
      "   test acc: 0.835837\n",
      "Epoch 725 : Step 282750-283140 of 19500000 took 7.922396s\n",
      "   train loss: 0.391890\n",
      "   train acc: 0.907752\n",
      "   test loss: 0.667056\n",
      "   test acc: 0.833734\n",
      "Epoch 726 : Step 283140-283530 of 19500000 took 7.951958s\n",
      "   train loss: 0.393531\n",
      "   train acc: 0.907913\n",
      "   test loss: 0.656237\n",
      "   test acc: 0.839443\n",
      "Epoch 727 : Step 283530-283920 of 19500000 took 7.938067s\n",
      "   train loss: 0.396092\n",
      "   train acc: 0.906430\n",
      "   test loss: 0.643155\n",
      "   test acc: 0.839944\n",
      "Epoch 728 : Step 283920-284310 of 19500000 took 7.901702s\n",
      "   train loss: 0.392566\n",
      "   train acc: 0.907853\n",
      "   test loss: 0.671843\n",
      "   test acc: 0.835437\n",
      "Epoch 729 : Step 284310-284700 of 19500000 took 7.859175s\n",
      "   train loss: 0.398476\n",
      "   train acc: 0.905288\n",
      "   test loss: 0.672072\n",
      "   test acc: 0.834936\n",
      "Epoch 730 : Step 284700-285090 of 19500000 took 7.855712s\n",
      "   train loss: 0.390528\n",
      "   train acc: 0.908874\n",
      "   test loss: 0.671177\n",
      "   test acc: 0.837941\n",
      "Epoch 731 : Step 285090-285480 of 19500000 took 7.964989s\n",
      "   train loss: 0.391764\n",
      "   train acc: 0.906991\n",
      "   test loss: 0.667967\n",
      "   test acc: 0.839042\n",
      "Epoch 732 : Step 285480-285870 of 19500000 took 7.992400s\n",
      "   train loss: 0.399357\n",
      "   train acc: 0.905108\n",
      "   test loss: 0.665620\n",
      "   test acc: 0.836438\n",
      "Epoch 733 : Step 285870-286260 of 19500000 took 7.943299s\n",
      "   train loss: 0.391566\n",
      "   train acc: 0.907292\n",
      "   test loss: 0.667989\n",
      "   test acc: 0.835737\n",
      "Epoch 734 : Step 286260-286650 of 19500000 took 7.886858s\n",
      "   train loss: 0.389938\n",
      "   train acc: 0.907692\n",
      "   test loss: 0.676220\n",
      "   test acc: 0.834635\n",
      "Epoch 735 : Step 286650-287040 of 19500000 took 7.906973s\n",
      "   train loss: 0.391377\n",
      "   train acc: 0.907472\n",
      "   test loss: 0.665498\n",
      "   test acc: 0.837440\n",
      "Epoch 736 : Step 287040-287430 of 19500000 took 7.859318s\n",
      "   train loss: 0.385063\n",
      "   train acc: 0.910317\n",
      "   test loss: 0.673221\n",
      "   test acc: 0.836038\n",
      "Epoch 737 : Step 287430-287820 of 19500000 took 7.952740s\n",
      "   train loss: 0.393074\n",
      "   train acc: 0.906370\n",
      "   test loss: 0.668373\n",
      "   test acc: 0.838141\n",
      "Epoch 738 : Step 287820-288210 of 19500000 took 8.003963s\n",
      "   train loss: 0.391333\n",
      "   train acc: 0.907973\n",
      "   test loss: 0.646054\n",
      "   test acc: 0.841246\n",
      "Epoch 739 : Step 288210-288600 of 19500000 took 7.945243s\n",
      "   train loss: 0.385976\n",
      "   train acc: 0.909275\n",
      "   test loss: 0.667644\n",
      "   test acc: 0.836639\n",
      "Epoch 740 : Step 288600-288990 of 19500000 took 7.906555s\n",
      "   train loss: 0.390071\n",
      "   train acc: 0.908253\n",
      "   test loss: 0.655420\n",
      "   test acc: 0.837640\n",
      "Epoch 741 : Step 288990-289380 of 19500000 took 8.022048s\n",
      "   train loss: 0.395086\n",
      "   train acc: 0.906911\n",
      "   test loss: 0.671544\n",
      "   test acc: 0.834034\n",
      "Epoch 742 : Step 289380-289770 of 19500000 took 7.875090s\n",
      "   train loss: 0.394561\n",
      "   train acc: 0.906791\n",
      "   test loss: 0.662092\n",
      "   test acc: 0.834135\n",
      "Epoch 743 : Step 289770-290160 of 19500000 took 7.906880s\n",
      "   train loss: 0.391449\n",
      "   train acc: 0.908874\n",
      "   test loss: 0.662663\n",
      "   test acc: 0.835337\n",
      "Epoch 744 : Step 290160-290550 of 19500000 took 7.877940s\n",
      "   train loss: 0.390127\n",
      "   train acc: 0.907512\n",
      "   test loss: 0.662104\n",
      "   test acc: 0.835737\n",
      "Epoch 745 : Step 290550-290940 of 19500000 took 7.925165s\n",
      "   train loss: 0.386974\n",
      "   train acc: 0.909495\n",
      "   test loss: 0.665664\n",
      "   test acc: 0.839042\n",
      "Epoch 746 : Step 290940-291330 of 19500000 took 7.927724s\n",
      "   train loss: 0.387337\n",
      "   train acc: 0.908413\n",
      "   test loss: 0.648611\n",
      "   test acc: 0.838442\n",
      "Epoch 747 : Step 291330-291720 of 19500000 took 7.907403s\n",
      "   train loss: 0.391737\n",
      "   train acc: 0.908193\n",
      "   test loss: 0.665590\n",
      "   test acc: 0.837440\n",
      "Epoch 748 : Step 291720-292110 of 19500000 took 7.874094s\n",
      "   train loss: 0.390980\n",
      "   train acc: 0.907071\n",
      "   test loss: 0.649794\n",
      "   test acc: 0.838942\n",
      "Epoch 749 : Step 292110-292500 of 19500000 took 7.930575s\n",
      "   train loss: 0.385441\n",
      "   train acc: 0.908954\n",
      "   test loss: 0.658772\n",
      "   test acc: 0.836138\n",
      "Save model !!!!!!!!!!\n",
      "[TL] [*] Saving TL params into model.npz\n",
      "[TL] [*] Saved\n",
      "Epoch 750 : Step 292500-292890 of 19500000 took 7.885409s\n",
      "   train loss: 0.390567\n",
      "   train acc: 0.907071\n",
      "   test loss: 0.655985\n",
      "   test acc: 0.840144\n",
      "Epoch 751 : Step 292890-293280 of 19500000 took 7.996259s\n",
      "   train loss: 0.393143\n",
      "   train acc: 0.905349\n",
      "   test loss: 0.657915\n",
      "   test acc: 0.835437\n",
      "Epoch 752 : Step 293280-293670 of 19500000 took 7.944414s\n",
      "   train loss: 0.384004\n",
      "   train acc: 0.911558\n",
      "   test loss: 0.670418\n",
      "   test acc: 0.836538\n",
      "Epoch 753 : Step 293670-294060 of 19500000 took 7.876776s\n",
      "   train loss: 0.395302\n",
      "   train acc: 0.906030\n",
      "   test loss: 0.650241\n",
      "   test acc: 0.838442\n",
      "Epoch 754 : Step 294060-294450 of 19500000 took 7.889603s\n",
      "   train loss: 0.389102\n",
      "   train acc: 0.908173\n",
      "   test loss: 0.670964\n",
      "   test acc: 0.836138\n",
      "Epoch 755 : Step 294450-294840 of 19500000 took 7.906479s\n",
      "   train loss: 0.386801\n",
      "   train acc: 0.908053\n",
      "   test loss: 0.648290\n",
      "   test acc: 0.838842\n",
      "Epoch 756 : Step 294840-295230 of 19500000 took 7.857107s\n",
      "   train loss: 0.389860\n",
      "   train acc: 0.908734\n",
      "   test loss: 0.654570\n",
      "   test acc: 0.838341\n",
      "Epoch 757 : Step 295230-295620 of 19500000 took 7.923234s\n",
      "   train loss: 0.387735\n",
      "   train acc: 0.908574\n",
      "   test loss: 0.666240\n",
      "   test acc: 0.836338\n",
      "Epoch 758 : Step 295620-296010 of 19500000 took 7.941867s\n",
      "   train loss: 0.390139\n",
      "   train acc: 0.908373\n",
      "   test loss: 0.670726\n",
      "   test acc: 0.837841\n",
      "Epoch 759 : Step 296010-296400 of 19500000 took 7.995629s\n",
      "   train loss: 0.387950\n",
      "   train acc: 0.908594\n",
      "   test loss: 0.661831\n",
      "   test acc: 0.835337\n",
      "Epoch 760 : Step 296400-296790 of 19500000 took 7.909822s\n",
      "   train loss: 0.388348\n",
      "   train acc: 0.909095\n",
      "   test loss: 0.667968\n",
      "   test acc: 0.838742\n",
      "Epoch 761 : Step 296790-297180 of 19500000 took 7.896789s\n",
      "   train loss: 0.394833\n",
      "   train acc: 0.906871\n",
      "   test loss: 0.665549\n",
      "   test acc: 0.836739\n",
      "Epoch 762 : Step 297180-297570 of 19500000 took 7.902628s\n",
      "   train loss: 0.387351\n",
      "   train acc: 0.909916\n",
      "   test loss: 0.661080\n",
      "   test acc: 0.835938\n",
      "Epoch 763 : Step 297570-297960 of 19500000 took 7.883051s\n",
      "   train loss: 0.388451\n",
      "   train acc: 0.908814\n",
      "   test loss: 0.660905\n",
      "   test acc: 0.837941\n",
      "Epoch 764 : Step 297960-298350 of 19500000 took 7.932166s\n",
      "   train loss: 0.384672\n",
      "   train acc: 0.910076\n",
      "   test loss: 0.683880\n",
      "   test acc: 0.832732\n",
      "Epoch 765 : Step 298350-298740 of 19500000 took 7.976950s\n",
      "   train loss: 0.389387\n",
      "   train acc: 0.909595\n",
      "   test loss: 0.657298\n",
      "   test acc: 0.840745\n",
      "Epoch 766 : Step 298740-299130 of 19500000 took 7.980143s\n",
      "   train loss: 0.388765\n",
      "   train acc: 0.908534\n",
      "   test loss: 0.672590\n",
      "   test acc: 0.832232\n",
      "Epoch 767 : Step 299130-299520 of 19500000 took 7.960480s\n",
      "   train loss: 0.391831\n",
      "   train acc: 0.907011\n",
      "   test loss: 0.670870\n",
      "   test acc: 0.837340\n",
      "Epoch 768 : Step 299520-299910 of 19500000 took 7.890624s\n",
      "   train loss: 0.384414\n",
      "   train acc: 0.910657\n",
      "   test loss: 0.675500\n",
      "   test acc: 0.834335\n",
      "Epoch 769 : Step 299910-300300 of 19500000 took 7.856345s\n",
      "   train loss: 0.384311\n",
      "   train acc: 0.910757\n",
      "   test loss: 0.666771\n",
      "   test acc: 0.838542\n",
      "Epoch 770 : Step 300300-300690 of 19500000 took 7.863796s\n",
      "   train loss: 0.386764\n",
      "   train acc: 0.910196\n",
      "   test loss: 0.667463\n",
      "   test acc: 0.834635\n",
      "Epoch 771 : Step 300690-301080 of 19500000 took 7.985086s\n",
      "   train loss: 0.389308\n",
      "   train acc: 0.908293\n",
      "   test loss: 0.672232\n",
      "   test acc: 0.834435\n",
      "Epoch 772 : Step 301080-301470 of 19500000 took 7.897983s\n",
      "   train loss: 0.387347\n",
      "   train acc: 0.909776\n",
      "   test loss: 0.664417\n",
      "   test acc: 0.836639\n",
      "Epoch 773 : Step 301470-301860 of 19500000 took 7.920437s\n",
      "   train loss: 0.384211\n",
      "   train acc: 0.909996\n",
      "   test loss: 0.670648\n",
      "   test acc: 0.835637\n",
      "Epoch 774 : Step 301860-302250 of 19500000 took 7.899397s\n",
      "   train loss: 0.387410\n",
      "   train acc: 0.909595\n",
      "   test loss: 0.681106\n",
      "   test acc: 0.836038\n",
      "Epoch 775 : Step 302250-302640 of 19500000 took 7.946968s\n",
      "   train loss: 0.389751\n",
      "   train acc: 0.907572\n",
      "   test loss: 0.671181\n",
      "   test acc: 0.836939\n",
      "Epoch 776 : Step 302640-303030 of 19500000 took 7.939699s\n",
      "   train loss: 0.383119\n",
      "   train acc: 0.910457\n",
      "   test loss: 0.658663\n",
      "   test acc: 0.834034\n",
      "Epoch 777 : Step 303030-303420 of 19500000 took 7.924373s\n",
      "   train loss: 0.386198\n",
      "   train acc: 0.908874\n",
      "   test loss: 0.665219\n",
      "   test acc: 0.837340\n",
      "Epoch 778 : Step 303420-303810 of 19500000 took 7.944725s\n",
      "   train loss: 0.387143\n",
      "   train acc: 0.909195\n",
      "   test loss: 0.690164\n",
      "   test acc: 0.830929\n",
      "Epoch 779 : Step 303810-304200 of 19500000 took 7.929123s\n",
      "   train loss: 0.392486\n",
      "   train acc: 0.907632\n",
      "   test loss: 0.652800\n",
      "   test acc: 0.836739\n",
      "Epoch 780 : Step 304200-304590 of 19500000 took 7.903898s\n",
      "   train loss: 0.389798\n",
      "   train acc: 0.907772\n",
      "   test loss: 0.674873\n",
      "   test acc: 0.831831\n",
      "Epoch 781 : Step 304590-304980 of 19500000 took 7.935492s\n",
      "   train loss: 0.387007\n",
      "   train acc: 0.909896\n",
      "   test loss: 0.659924\n",
      "   test acc: 0.837740\n",
      "Epoch 782 : Step 304980-305370 of 19500000 took 7.899287s\n",
      "   train loss: 0.384954\n",
      "   train acc: 0.909716\n",
      "   test loss: 0.688029\n",
      "   test acc: 0.830629\n",
      "Epoch 783 : Step 305370-305760 of 19500000 took 7.905999s\n",
      "   train loss: 0.388654\n",
      "   train acc: 0.908353\n",
      "   test loss: 0.655573\n",
      "   test acc: 0.839343\n",
      "Epoch 784 : Step 305760-306150 of 19500000 took 7.957317s\n",
      "   train loss: 0.385804\n",
      "   train acc: 0.909615\n",
      "   test loss: 0.665395\n",
      "   test acc: 0.837240\n",
      "Epoch 785 : Step 306150-306540 of 19500000 took 7.957524s\n",
      "   train loss: 0.391398\n",
      "   train acc: 0.907953\n",
      "   test loss: 0.667755\n",
      "   test acc: 0.834936\n",
      "Epoch 786 : Step 306540-306930 of 19500000 took 7.923442s\n",
      "   train loss: 0.386065\n",
      "   train acc: 0.909956\n",
      "   test loss: 0.668881\n",
      "   test acc: 0.834736\n",
      "Epoch 787 : Step 306930-307320 of 19500000 took 7.935956s\n",
      "   train loss: 0.386549\n",
      "   train acc: 0.910797\n",
      "   test loss: 0.666454\n",
      "   test acc: 0.843249\n",
      "Epoch 788 : Step 307320-307710 of 19500000 took 7.877402s\n",
      "   train loss: 0.384354\n",
      "   train acc: 0.911078\n",
      "   test loss: 0.660582\n",
      "   test acc: 0.837941\n",
      "Epoch 789 : Step 307710-308100 of 19500000 took 7.896011s\n",
      "   train loss: 0.384482\n",
      "   train acc: 0.908914\n",
      "   test loss: 0.684967\n",
      "   test acc: 0.836639\n",
      "Epoch 790 : Step 308100-308490 of 19500000 took 7.941606s\n",
      "   train loss: 0.382289\n",
      "   train acc: 0.909916\n",
      "   test loss: 0.665941\n",
      "   test acc: 0.837240\n",
      "Epoch 791 : Step 308490-308880 of 19500000 took 8.030732s\n",
      "   train loss: 0.385814\n",
      "   train acc: 0.908894\n",
      "   test loss: 0.657737\n",
      "   test acc: 0.841947\n",
      "Epoch 792 : Step 308880-309270 of 19500000 took 7.940419s\n",
      "   train loss: 0.384852\n",
      "   train acc: 0.911298\n",
      "   test loss: 0.667009\n",
      "   test acc: 0.834936\n",
      "Epoch 793 : Step 309270-309660 of 19500000 took 7.949156s\n",
      "   train loss: 0.381335\n",
      "   train acc: 0.912700\n",
      "   test loss: 0.688565\n",
      "   test acc: 0.833934\n",
      "Epoch 794 : Step 309660-310050 of 19500000 took 7.938030s\n",
      "   train loss: 0.387342\n",
      "   train acc: 0.907833\n",
      "   test loss: 0.667620\n",
      "   test acc: 0.836939\n",
      "Epoch 795 : Step 310050-310440 of 19500000 took 7.979095s\n",
      "   train loss: 0.387828\n",
      "   train acc: 0.908894\n",
      "   test loss: 0.660353\n",
      "   test acc: 0.839744\n",
      "Epoch 796 : Step 310440-310830 of 19500000 took 7.927310s\n",
      "   train loss: 0.383948\n",
      "   train acc: 0.910737\n",
      "   test loss: 0.673344\n",
      "   test acc: 0.833834\n",
      "Epoch 797 : Step 310830-311220 of 19500000 took 7.935764s\n",
      "   train loss: 0.387834\n",
      "   train acc: 0.908614\n",
      "   test loss: 0.659958\n",
      "   test acc: 0.837240\n",
      "Epoch 798 : Step 311220-311610 of 19500000 took 8.003839s\n",
      "   train loss: 0.384859\n",
      "   train acc: 0.909736\n",
      "   test loss: 0.673878\n",
      "   test acc: 0.835537\n",
      "Epoch 799 : Step 311610-312000 of 19500000 took 7.986203s\n",
      "   train loss: 0.384601\n",
      "   train acc: 0.909675\n",
      "   test loss: 0.691962\n",
      "   test acc: 0.826422\n",
      "Save model !!!!!!!!!!\n",
      "[TL] [*] Saving TL params into model.npz\n",
      "[TL] [*] Saved\n",
      "Epoch 800 : Step 312000-312390 of 19500000 took 7.880336s\n",
      "   train loss: 0.384176\n",
      "   train acc: 0.910557\n",
      "   test loss: 0.653740\n",
      "   test acc: 0.841446\n",
      "Epoch 801 : Step 312390-312780 of 19500000 took 7.872664s\n",
      "   train loss: 0.385764\n",
      "   train acc: 0.909115\n",
      "   test loss: 0.661706\n",
      "   test acc: 0.836639\n",
      "Epoch 802 : Step 312780-313170 of 19500000 took 7.891583s\n",
      "   train loss: 0.384513\n",
      "   train acc: 0.910978\n",
      "   test loss: 0.658657\n",
      "   test acc: 0.836238\n",
      "Epoch 803 : Step 313170-313560 of 19500000 took 7.864577s\n",
      "   train loss: 0.386031\n",
      "   train acc: 0.909776\n",
      "   test loss: 0.655658\n",
      "   test acc: 0.837440\n",
      "Epoch 804 : Step 313560-313950 of 19500000 took 7.956956s\n",
      "   train loss: 0.387181\n",
      "   train acc: 0.909575\n",
      "   test loss: 0.653329\n",
      "   test acc: 0.839543\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "model_file_name = \"./model_cifar10_advanced.ckpt\"\n",
    "resume = True  # load model, resume from previous checkpoint?\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    config.allow_soft_placement = True\n",
    "    sess = tf.Session(config=config)\n",
    "\n",
    "    \"\"\" Prepare data in cpu \"\"\"\n",
    "    # the tensors for reader and distorter for a single Example\n",
    "    x_train_, y_train_ = read_and_decode(\"train.cifar10\", True)\n",
    "    x_test_, y_test_ = read_and_decode(\"test.cifar10\", False)\n",
    "\n",
    "    print_dataset_shape('x_train', x_train_, 'y_train', y_train_)\n",
    "    # (24, 24, 3)\n",
    "    # ()\n",
    "    print(x_train_)\n",
    "    print(y_train_)\n",
    "\n",
    "    print_dataset_shape('x_test', x_test_, 'y_test', y_test_)\n",
    "    print(x_test_)\n",
    "    print(y_test_)\n",
    "\n",
    "    # the tensors for streaming to the model with a batch augmented data per a training batch/step\n",
    "    # by using multi-threads.\n",
    "    x_train_batch, y_train_batch = tf.train.shuffle_batch(\n",
    "        [x_train_, y_train_], batch_size=batch_size, capacity=2000, min_after_dequeue=1000, num_threads=32\n",
    "        # set the number of threads here\n",
    "    )\n",
    "\n",
    "    # for testing, uses batch instead of shuffle_batch\n",
    "    x_test_batch, y_test_batch = tf.train.batch(\n",
    "        [x_test_, y_test_], batch_size=batch_size, capacity=50000, num_threads=32\n",
    "    )\n",
    "\n",
    "    print_dataset_shape('x_train_batch', x_train_batch, 'y_train_batch', y_train_batch)\n",
    "    # (batch_size, 24, 24, 3)\n",
    "    # (batch_size,)\n",
    "    print(x_train_batch)\n",
    "    print(y_train_batch)\n",
    "\n",
    "    print_dataset_shape('x_test_batch', x_test_batch, 'y_test_batch', y_test_batch)\n",
    "    print(x_test_batch)\n",
    "    print(y_test_batch)\n",
    "\n",
    "    # You can also use placeholder to feed_dict in data after using\n",
    "    # val, l = sess.run([x_train_batch, y_train_batch]) to pull a batch size of data\n",
    "    #\n",
    "    # Demo:\n",
    "    #   # Define the model using placeholder\n",
    "    #   x_crop = tf.placeholder(tf.float32, shape=[batch_size, 24, 24, 3])\n",
    "    #   y_ = tf.placeholder(tf.int32, shape=[batch_size,])\n",
    "    #   cost, acc, network = model(x_crop, y_, None)\n",
    "    #\n",
    "    #   ...\n",
    "    #\n",
    "    #   # In the loop of batches\n",
    "    #   for b in range(batch count in a epoch):\n",
    "    #     val, l = sess.run([x_train_batch, y_train_batch])   # pull a batch size of data\n",
    "    #\n",
    "    #     tl.visualize.images2d(val, second=3, saveable=False, name='batch', dtype=np.uint8, fig_idx=2020121)\n",
    "    #     err, ac, _ = sess.run([cost, acc, train_op], feed_dict={x_crop: val, y_: l})\n",
    "\n",
    "    with tf.device('/gpu:0'):  # <-- remove it if you don't have GPU\n",
    "        # using local response normalization\n",
    "        # network, cost, acc, = model(x_train_batch, y_train_batch, False)\n",
    "        # _, cost_test, acc_test = model(x_test_batch, y_test_batch, True)\n",
    "\n",
    "        # you may want to try batch normalization\n",
    "        network, cost, acc, = model_batch_norm(x_train_batch, y_train_batch, None, is_train=True)\n",
    "        _, cost_test, acc_test = model_batch_norm(x_test_batch, y_test_batch, True, is_train=False)\n",
    "\n",
    "    # train\n",
    "    n_epoch = 50000\n",
    "    learning_rate = 0.0001\n",
    "    print_freq = 1\n",
    "    n_step_epoch = int(len(y_train) / batch_size)\n",
    "    n_step = n_epoch * n_step_epoch\n",
    "\n",
    "    with tf.device('/gpu:0'):  # <-- remove it if you don't have GPU\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "    tl.layers.initialize_global_variables(sess)\n",
    "    if resume and os.path.isfile(model_file_name):\n",
    "        print(\"Load existing model \" + \"!\" * 10)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, model_file_name)\n",
    "\n",
    "    network.print_params(False)\n",
    "    network.print_layers()\n",
    "\n",
    "    print('   learning_rate: %f' % learning_rate)\n",
    "    print('   batch_size: %d' % batch_size)\n",
    "    print('   n_epoch: %d, step in an epoch: %d, total n_step: %d' %\n",
    "          (n_epoch, n_step_epoch, n_step))\n",
    "\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    step = 0\n",
    "    for epoch in range(n_epoch):\n",
    "        start_time = time.time()\n",
    "        train_loss, train_acc, n_batch = 0, 0, 0\n",
    "        for s in range(n_step_epoch):\n",
    "            # You can also use placeholder to feed_dict in data after using\n",
    "            # val, l = sess.run([x_train_batch, y_train_batch]) to pull a batch size of data\n",
    "            #\n",
    "            # tl.visualize.images2d(val, second=3, saveable=False, name='batch', dtype=np.uint8, fig_idx=2020121)\n",
    "            # err, ac, _ = sess.run([cost, acc, train_op], feed_dict={x_crop: val, y_: l})\n",
    "            err, ac, _ = sess.run([cost, acc, train_op])\n",
    "            step += 1\n",
    "            train_loss += err\n",
    "            train_acc += ac\n",
    "            n_batch += 1\n",
    "\n",
    "        if epoch + 1 == 1 or (epoch + 1) % print_freq == 0:\n",
    "            print(\n",
    "                \"Epoch %d : Step %d-%d of %d took %fs\" %\n",
    "                (epoch, step - n_step_epoch, step,\n",
    "                 n_step, time.time() - start_time)\n",
    "            )\n",
    "            print(\"   train loss: %f\" % (train_loss / n_batch))\n",
    "            print(\"   train acc: %f\" % (train_acc / n_batch))\n",
    "\n",
    "            test_loss, test_acc, n_batch = 0, 0, 0\n",
    "            for _ in range(int(len(y_test) / batch_size)):\n",
    "                err, ac = sess.run([cost_test, acc_test])\n",
    "                test_loss += err\n",
    "                test_acc += ac\n",
    "                n_batch += 1\n",
    "            print(\"   test loss: %f\" % (test_loss / n_batch))\n",
    "            print(\"   test acc: %f\" % (test_acc / n_batch))\n",
    "\n",
    "        if (epoch + 1) % (print_freq * 50) == 0:\n",
    "            print(\"Save model \" + \"!\" * 10)\n",
    "            saver = tf.train.Saver()\n",
    "            save_path = saver.save(sess, model_file_name)\n",
    "            # you can also save model into npz\n",
    "            tl.files.save_npz(network.all_params, name='model.npz', sess=sess)\n",
    "            # and restore it as follow:\n",
    "            # tl.files.load_and_assign_npz(sess=sess, name='model.npz', network=network)\n",
    "\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    sess.close()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "tutorial_cifar10_tfrecord.ipynb",
   "version": "0.3.2",
   "provenance": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
