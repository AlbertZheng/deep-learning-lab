{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "[View in Colaboratory](https://colab.research.google.com/github/AlbertZheng/deep-learning-lab/blob/master/tensorlayer/notebooks/tutorial_mnist_cnn.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bT-2sI89iF8f",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3046.0
    },
    "outputId": "ee95d0ad-1417-4ded-e3de-309169cb1531"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorlayer in /usr/local/lib/python3.6/dist-packages (1.10.1)\n",
      "Requirement already satisfied: scipy<1.2,>=1.1 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (1.1.0)\n",
      "Requirement already satisfied: progressbar2<3.39,>=3.38 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (3.38.0)\n",
      "Requirement already satisfied: numpy<1.16,>=1.14 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (1.14.5)\n",
      "Requirement already satisfied: wrapt<1.11,>=1.10 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (1.10.11)\n",
      "Requirement already satisfied: tqdm<4.26,>=4.23 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (4.25.0)\n",
      "Requirement already satisfied: imageio<2.5,>=2.3 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (2.4.1)\n",
      "Requirement already satisfied: scikit-image<0.15,>=0.14 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (0.14.0)\n",
      "Requirement already satisfied: matplotlib<2.3,>=2.2 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (2.2.3)\n",
      "Requirement already satisfied: requests<2.20,>=2.19 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (2.19.1)\n",
      "Requirement already satisfied: scikit-learn<0.20,>=0.19 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (0.19.2)\n",
      "Requirement already satisfied: lxml<4.3,>=4.2 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (4.2.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from progressbar2<3.39,>=3.38->tensorlayer) (1.11.0)\n",
      "Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2<3.39,>=3.38->tensorlayer) (2.3.0)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio<2.5,>=2.3->tensorlayer) (5.2.0)\n",
      "Requirement already satisfied: networkx>=1.8 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15,>=0.14->tensorlayer) (2.1)\n",
      "Requirement already satisfied: cloudpickle>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15,>=0.14->tensorlayer) (0.5.6)\n",
      "Requirement already satisfied: dask[array]>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15,>=0.14->tensorlayer) (0.19.1)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15,>=0.14->tensorlayer) (1.0.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib<2.3,>=2.2->tensorlayer) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib<2.3,>=2.2->tensorlayer) (2.5.3)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib<2.3,>=2.2->tensorlayer) (2.2.0)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from matplotlib<2.3,>=2.2->tensorlayer) (2018.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib<2.3,>=2.2->tensorlayer) (0.10.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<2.20,>=2.19->tensorlayer) (3.0.4)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<2.20,>=2.19->tensorlayer) (2.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<2.20,>=2.19->tensorlayer) (2018.8.24)\n",
      "Requirement already satisfied: urllib3<1.24,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<2.20,>=2.19->tensorlayer) (1.22)\n",
      "Requirement already satisfied: decorator>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=1.8->scikit-image<0.15,>=0.14->tensorlayer) (4.3.0)\n",
      "Requirement already satisfied: toolz>=0.7.3; extra == \"array\" in /usr/local/lib/python3.6/dist-packages (from dask[array]>=0.9.0->scikit-image<0.15,>=0.14->tensorlayer) (0.9.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib<2.3,>=2.2->tensorlayer) (39.1.0)\n",
      "[TL] Load or Download MNIST > data/mnist\n",
      "[TL] data/mnist/train-images-idx3-ubyte.gz\n",
      "[TL] data/mnist/t10k-images-idx3-ubyte.gz\n",
      "[TL] InputLayer  input: (128, 28, 28, 1)\n",
      "[TL] Conv2d cnn1: n_filter: 32 filter_size: (5, 5) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] MaxPool2d pool1: filter_size: (2, 2) strides: (2, 2) padding: SAME\n",
      "[TL] Conv2d cnn2: n_filter: 64 filter_size: (5, 5) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] MaxPool2d pool2: filter_size: (2, 2) strides: (2, 2) padding: SAME\n",
      "[TL] FlattenLayer flatten: 3136\n",
      "[TL] DropoutLayer drop1: keep: 0.500000 is_fix: False\n",
      "[TL] DenseLayer  relu1: 256 relu\n",
      "[TL] DropoutLayer drop2: keep: 0.500000 is_fix: False\n",
      "[TL] DenseLayer  output: 10 No Activation\n",
      "[TL] WARNING: Function: `tensorlayer.layers.utils.initialize_global_variables` (in file: /usr/local/lib/python3.6/dist-packages/tensorlayer/layers/utils.py) is deprecated and will be removed after 2018-09-30.\n",
      "Instructions for updating: This API is deprecated in favor of `tf.global_variables_initializer`\n",
      "\n",
      "[TL]   param   0: cnn1/kernel:0        (5, 5, 1, 32)      float32_ref (mean: 0.0008554545929655433, median: 0.00134909781627357, std: 0.01786465384066105)   \n",
      "[TL]   param   1: cnn1/bias:0          (32,)              float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
      "[TL]   param   2: cnn2/kernel:0        (5, 5, 32, 64)     float32_ref (mean: -0.00016000226605683565, median: -0.0001442007051082328, std: 0.017545467242598534)   \n",
      "[TL]   param   3: cnn2/bias:0          (64,)              float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
      "[TL]   param   4: relu1/W:0            (3136, 256)        float32_ref (mean: 0.00014824241225142032, median: 0.0001658967084949836, std: 0.08791803568601608)   \n",
      "[TL]   param   5: relu1/b:0            (256,)             float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
      "[TL]   param   6: output/W:0           (256, 10)          float32_ref (mean: 0.0016423467313870788, median: 0.0007836576551198959, std: 0.0877324715256691)   \n",
      "[TL]   param   7: output/b:0           (10,)              float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
      "[TL]   num of params: 857738\n",
      "[TL]   layer   0: Placeholder:0        (128, 28, 28, 1)    float32\n",
      "[TL]   layer   1: cnn1/Relu:0          (128, 28, 28, 32)    float32\n",
      "[TL]   layer   2: pool1/MaxPool:0      (128, 14, 14, 32)    float32\n",
      "[TL]   layer   3: cnn2/Relu:0          (128, 14, 14, 64)    float32\n",
      "[TL]   layer   4: pool2/MaxPool:0      (128, 7, 7, 64)    float32\n",
      "[TL]   layer   5: flatten:0            (128, 3136)        float32\n",
      "[TL]   layer   6: drop1/mul:0          (128, 3136)        float32\n",
      "[TL]   layer   7: relu1/Relu:0         (128, 256)         float32\n",
      "[TL]   layer   8: drop2/mul:0          (128, 256)         float32\n",
      "[TL]   layer   9: output/bias_add:0    (128, 10)          float32\n",
      "   learning_rate: 0.000100\n",
      "   batch_size: 128\n",
      "Epoch 1 of 200 took 6.043200s\n",
      "   train loss: 0.255148\n",
      "   train acc: 0.927965\n",
      "   val loss: 0.225104\n",
      "   val acc: 0.939603\n",
      "Epoch 10 of 200 took 5.138117s\n",
      "   train loss: 0.049991\n",
      "   train acc: 0.984575\n",
      "   val loss: 0.051681\n",
      "   val acc: 0.985677\n",
      "Epoch 20 of 200 took 5.146421s\n",
      "   train loss: 0.026944\n",
      "   train acc: 0.991607\n",
      "   val loss: 0.036890\n",
      "   val acc: 0.989784\n",
      "Epoch 30 of 200 took 5.152538s\n",
      "   train loss: 0.017651\n",
      "   train acc: 0.994872\n",
      "   val loss: 0.031301\n",
      "   val acc: 0.990885\n",
      "Epoch 40 of 200 took 5.157892s\n",
      "   train loss: 0.011751\n",
      "   train acc: 0.996554\n",
      "   val loss: 0.029081\n",
      "   val acc: 0.992288\n",
      "Epoch 50 of 200 took 5.170775s\n",
      "   train loss: 0.008435\n",
      "   train acc: 0.997456\n",
      "   val loss: 0.026415\n",
      "   val acc: 0.992588\n",
      "Epoch 60 of 200 took 5.158483s\n",
      "   train loss: 0.006269\n",
      "   train acc: 0.998217\n",
      "   val loss: 0.026871\n",
      "   val acc: 0.992488\n",
      "Epoch 70 of 200 took 5.170021s\n",
      "   train loss: 0.004534\n",
      "   train acc: 0.998778\n",
      "   val loss: 0.025572\n",
      "   val acc: 0.993389\n",
      "Epoch 80 of 200 took 5.165312s\n",
      "   train loss: 0.003342\n",
      "   train acc: 0.999159\n",
      "   val loss: 0.025503\n",
      "   val acc: 0.993590\n",
      "Epoch 90 of 200 took 5.164188s\n",
      "   train loss: 0.002443\n",
      "   train acc: 0.999459\n",
      "   val loss: 0.026247\n",
      "   val acc: 0.993490\n",
      "Epoch 100 of 200 took 5.158885s\n",
      "   train loss: 0.001862\n",
      "   train acc: 0.999499\n",
      "   val loss: 0.027898\n",
      "   val acc: 0.993590\n",
      "Epoch 110 of 200 took 5.167495s\n",
      "   train loss: 0.001281\n",
      "   train acc: 0.999720\n",
      "   val loss: 0.026838\n",
      "   val acc: 0.994091\n",
      "Epoch 120 of 200 took 5.144548s\n",
      "   train loss: 0.001103\n",
      "   train acc: 0.999740\n",
      "   val loss: 0.028052\n",
      "   val acc: 0.993690\n",
      "Epoch 130 of 200 took 5.173582s\n",
      "   train loss: 0.000731\n",
      "   train acc: 0.999800\n",
      "   val loss: 0.028687\n",
      "   val acc: 0.993790\n",
      "Epoch 140 of 200 took 5.156846s\n",
      "   train loss: 0.000596\n",
      "   train acc: 0.999860\n",
      "   val loss: 0.030914\n",
      "   val acc: 0.993590\n",
      "Epoch 150 of 200 took 5.160542s\n",
      "   train loss: 0.000405\n",
      "   train acc: 0.999940\n",
      "   val loss: 0.028283\n",
      "   val acc: 0.993990\n",
      "Epoch 160 of 200 took 5.184144s\n",
      "   train loss: 0.000308\n",
      "   train acc: 0.999960\n",
      "   val loss: 0.027973\n",
      "   val acc: 0.993690\n",
      "Epoch 170 of 200 took 5.156917s\n",
      "   train loss: 0.000271\n",
      "   train acc: 0.999980\n",
      "   val loss: 0.029359\n",
      "   val acc: 0.993590\n",
      "Epoch 180 of 200 took 5.173600s\n",
      "   train loss: 0.000195\n",
      "   train acc: 0.999980\n",
      "   val loss: 0.030378\n",
      "   val acc: 0.993990\n",
      "Epoch 190 of 200 took 5.163973s\n",
      "   train loss: 0.000171\n",
      "   train acc: 1.000000\n",
      "   val loss: 0.031358\n",
      "   val acc: 0.994091\n",
      "Epoch 200 of 200 took 5.172240s\n",
      "   train loss: 0.000117\n",
      "   train acc: 1.000000\n",
      "   val loss: 0.033235\n",
      "   val acc: 0.993690\n",
      "Total training time: 1082.440508s\n",
      "Start testing the network ...\n",
      "   test loss: 0.017186\n",
      "   test acc: 0.995593\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorlayer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated\")\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.DEBUG)\n",
    "tl.logging.set_verbosity(tl.logging.DEBUG)\n",
    "\n",
    "def main_test_cnn_layer():\n",
    "    \"\"\"Reimplementation of the TensorFlow official MNIST CNN tutorials:\n",
    "    - https://www.tensorflow.org/versions/r0.8/tutorials/mnist/pros/index.html\n",
    "    - https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/mnist/convolutional.py\n",
    "\n",
    "    More TensorFlow official CNN tutorials can be found here:\n",
    "    - tutorial_cifar10.py\n",
    "    - https://www.tensorflow.org/versions/master/tutorials/deep_cnn/index.html\n",
    "\n",
    "    - For simplified CNN layer see \"Convolutional layer (Simplified)\"\n",
    "      in read the docs website.\n",
    "    \"\"\"\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = tl.files.load_mnist_dataset(shape=(-1, 28, 28, 1))\n",
    "\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    # Define the batchsize at the begin, you can give the batchsize in x and y_label\n",
    "    # rather than 'None', this can allow TensorFlow to apply some optimizations\n",
    "    # â€“ especially for convolutional layers.\n",
    "    batch_size = 128\n",
    "\n",
    "    x = tf.placeholder(tf.float32, shape=[batch_size, 28, 28, 1])  # [batch_size, height, width, channels]\n",
    "    y_label = tf.placeholder(tf.int64, shape=[batch_size])\n",
    "\n",
    "    net = tl.layers.InputLayer(x, name='input')\n",
    "    # Professional conv API for tensorflow expert\n",
    "    # net = tl.layers.Conv2dLayer(net,\n",
    "    #                     act = tf.nn.relu,\n",
    "    #                     shape = [5, 5, 1, 32],  # 32 features for each 5x5 patch\n",
    "    #                     strides=[1, 1, 1, 1],\n",
    "    #                     padding='SAME',\n",
    "    #                     name ='cnn1')     # output: (?, 28, 28, 32)\n",
    "    # net = tl.layers.PoolLayer(net,\n",
    "    #                     ksize=[1, 2, 2, 1],\n",
    "    #                     strides=[1, 2, 2, 1],\n",
    "    #                     padding='SAME',\n",
    "    #                     pool = tf.nn.max_pool,\n",
    "    #                     name ='pool1',)   # output: (?, 14, 14, 32)\n",
    "    # net = tl.layers.Conv2dLayer(net,\n",
    "    #                     act = tf.nn.relu,\n",
    "    #                     shape = [5, 5, 32, 64], # 64 features for each 5x5 patch\n",
    "    #                     strides=[1, 1, 1, 1],\n",
    "    #                     padding='SAME',\n",
    "    #                     name ='cnn2')     # output: (?, 14, 14, 64)\n",
    "    # net = tl.layers.PoolLayer(net,\n",
    "    #                     ksize=[1, 2, 2, 1],\n",
    "    #                     strides=[1, 2, 2, 1],\n",
    "    #                     padding='SAME',\n",
    "    #                     pool = tf.nn.max_pool,\n",
    "    #                     name ='pool2',)   # output: (?, 7, 7, 64)\n",
    "    # Simplified conv API (the same with the above layers)\n",
    "    net = tl.layers.Conv2d(net, 32, (5, 5), (1, 1), act=tf.nn.relu, padding='SAME', name='cnn1')\n",
    "    net = tl.layers.MaxPool2d(net, (2, 2), (2, 2), padding='SAME', name='pool1')\n",
    "    net = tl.layers.Conv2d(net, 64, (5, 5), (1, 1), act=tf.nn.relu, padding='SAME', name='cnn2')\n",
    "    net = tl.layers.MaxPool2d(net, (2, 2), (2, 2), padding='SAME', name='pool2')\n",
    "    # end of conv\n",
    "    net = tl.layers.FlattenLayer(net, name='flatten')\n",
    "    net = tl.layers.DropoutLayer(net, keep=0.5, name='drop1')\n",
    "    net = tl.layers.DenseLayer(net, 256, act=tf.nn.relu, name='relu1')\n",
    "    net = tl.layers.DropoutLayer(net, keep=0.5, name='drop2')\n",
    "    net = tl.layers.DenseLayer(net, 10, act=None, name='output')\n",
    "\n",
    "    y_pred = net.outputs\n",
    "\n",
    "    cost = tl.cost.cross_entropy(y_pred, y_label, 'cost')\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), y_label)\n",
    "    acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    # train\n",
    "    n_epoch = 200\n",
    "    learning_rate = 0.0001\n",
    "    print_freq = 10\n",
    "\n",
    "    train_params = net.all_params\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(cost, var_list=train_params)\n",
    "\n",
    "    tl.layers.initialize_global_variables(sess)\n",
    "    net.print_params()\n",
    "    net.print_layers()\n",
    "\n",
    "    print('   learning_rate: %f' % learning_rate)\n",
    "    print('   batch_size: %d' % batch_size)\n",
    "\n",
    "    training_begin_time = time.time()\n",
    "    for epoch in range(n_epoch):\n",
    "        start_time = time.time()\n",
    "\n",
    "        for X_train_a, y_train_a in tl.iterate.minibatches(X_train, y_train, batch_size, shuffle=True):\n",
    "            feed_dict = {x: X_train_a, y_label: y_train_a}\n",
    "            feed_dict.update(net.all_drop)  # enable noise layers\n",
    "            sess.run(train_op, feed_dict=feed_dict)\n",
    "\n",
    "        if epoch + 1 == 1 or (epoch + 1) % print_freq == 0:\n",
    "            training_end_time = time.time()\n",
    "            print(\"Epoch %d of %d took %fs\" % (epoch + 1, n_epoch, training_end_time - start_time))\n",
    "            \n",
    "            train_loss, train_acc, n_batch = 0, 0, 0\n",
    "            for X_train_a, y_train_a in tl.iterate.minibatches(X_train, y_train, batch_size, shuffle=True):\n",
    "                dp_dict = tl.utils.dict_to_one(net.all_drop)  # disable noise layers\n",
    "                feed_dict = {x: X_train_a, y_label: y_train_a}\n",
    "                feed_dict.update(dp_dict)\n",
    "                err, ac = sess.run([cost, acc], feed_dict=feed_dict)\n",
    "                train_loss += err\n",
    "                train_acc += ac\n",
    "                n_batch += 1\n",
    "            print(\"   train loss: %f\" % (train_loss / n_batch))\n",
    "            print(\"   train acc: %f\" % (train_acc / n_batch))\n",
    "\n",
    "            val_loss, val_acc, n_batch = 0, 0, 0\n",
    "            for X_val_a, y_val_a in tl.iterate.minibatches(X_val, y_val, batch_size, shuffle=True):\n",
    "                dp_dict = tl.utils.dict_to_one(net.all_drop)  # disable noise layers\n",
    "                feed_dict = {x: X_val_a, y_label: y_val_a}\n",
    "                feed_dict.update(dp_dict)\n",
    "                err, ac = sess.run([cost, acc], feed_dict=feed_dict)\n",
    "                val_loss += err\n",
    "                val_acc += ac\n",
    "                n_batch += 1\n",
    "            print(\"   val loss: %f\" % (val_loss / n_batch))\n",
    "            print(\"   val acc: %f\" % (val_acc / n_batch))\n",
    "\n",
    "            # try:\n",
    "            #     tl.vis.CNN2d(net.all_params[0].eval(), second=10, saveable=True, name='cnn1_' + str(epoch + 1), fig_idx=2012)\n",
    "            # except:  # pylint: disable=bare-except\n",
    "            #     print(\"You should change vis.CNN(), if you want to save the feature images for different dataset\")\n",
    "\n",
    "    print('Total training time: %fs' % (training_end_time - training_begin_time))\n",
    "    \n",
    "    print('Start testing the network ...')\n",
    "    test_loss, test_acc, n_batch = 0, 0, 0\n",
    "    for X_test_a, y_test_a in tl.iterate.minibatches(X_test, y_test, batch_size, shuffle=True):\n",
    "        dp_dict = tl.utils.dict_to_one(net.all_drop)  # disable noise layers\n",
    "        feed_dict = {x: X_test_a, y_label: y_test_a}\n",
    "        feed_dict.update(dp_dict)\n",
    "        err, ac = sess.run([cost, acc], feed_dict=feed_dict)\n",
    "        test_loss += err\n",
    "        test_acc += ac\n",
    "        n_batch += 1\n",
    "    print(\"   test loss: %f\" % (test_loss / n_batch))\n",
    "    print(\"   test acc: %f\" % (test_acc / n_batch))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # CNN\n",
    "    main_test_cnn_layer()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "tutorial_mnist_cnn.ipynb",
   "version": "0.3.2",
   "provenance": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
