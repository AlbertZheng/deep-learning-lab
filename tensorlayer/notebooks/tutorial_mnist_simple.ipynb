{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tutorial_mnist_simple.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/AlbertZheng/deep-learning-lab/blob/master/tensorlayer/notebooks/tutorial_mnist_simple.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "UDWHmkoznUdd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **检测colab runtime是否有GPU可用？**"
      ]
    },
    {
      "metadata": {
        "id": "bj50-2b0QYY7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1414
        },
        "outputId": "e790d58b-fe97-49b5-f15d-0fb7c89d4061"
      },
      "cell_type": "code",
      "source": [
        "!pip install \"tensorlayer>=1.9\"\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorlayer as tl\n",
        "\n",
        "\n",
        "# Confirm TensorFlow can see the GPU on Colaboratory\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('### Found GPU at: {} ###'.format(device_name))\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorlayer>=1.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c4/e2/736458723564163cfd87e3a9719a9fdece9011429bf556fb910d3691352e/tensorlayer-1.10.1-py2.py3-none-any.whl (313kB)\n",
            "\u001b[K    100% |████████████████████████████████| 317kB 16.6MB/s \n",
            "\u001b[?25hCollecting progressbar2<3.39,>=3.38 (from tensorlayer>=1.9)\n",
            "  Downloading https://files.pythonhosted.org/packages/4f/6f/acb2dd76f2c77527584bd3a4c2509782bb35c481c610521fc3656de5a9e0/progressbar2-3.38.0-py2.py3-none-any.whl\n",
            "Collecting lxml<4.3,>=4.2 (from tensorlayer>=1.9)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/a4/9eea8035fc7c7670e5eab97f34ff2ef0ddd78a491bf96df5accedb0e63f5/lxml-4.2.5-cp36-cp36m-manylinux1_x86_64.whl (5.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 5.8MB 6.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn<0.20,>=0.19 in /usr/local/lib/python3.6/dist-packages (from tensorlayer>=1.9) (0.19.2)\n",
            "Collecting scikit-image<0.15,>=0.14 (from tensorlayer>=1.9)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/79/cefff573a53ca3fb4c390739d19541b95f371e24d2990aed4cd8837971f0/scikit_image-0.14.0-cp36-cp36m-manylinux1_x86_64.whl (25.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 25.3MB 1.6MB/s \n",
            "\u001b[?25hCollecting imageio<2.5,>=2.3 (from tensorlayer>=1.9)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/b4/cbb592964dfd71a9de6a5b08f882fd334fb99ae09ddc82081dbb2f718c81/imageio-2.4.1.tar.gz (3.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 3.3MB 977kB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt<1.11,>=1.10 in /usr/local/lib/python3.6/dist-packages (from tensorlayer>=1.9) (1.10.11)\n",
            "Collecting requests<2.20,>=2.19 (from tensorlayer>=1.9)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/47/7e02164a2a3db50ed6d8a6ab1d6d60b69c4c3fdf57a284257925dfc12bda/requests-2.19.1-py2.py3-none-any.whl (91kB)\n",
            "\u001b[K    100% |████████████████████████████████| 92kB 24.4MB/s \n",
            "\u001b[?25hCollecting matplotlib<2.3,>=2.2 (from tensorlayer>=1.9)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/59/f235ab21bbe7b7c6570c4abf17ffb893071f4fa3b9cf557b09b60359ad9a/matplotlib-2.2.3-cp36-cp36m-manylinux1_x86_64.whl (12.6MB)\n",
            "\u001b[K    100% |████████████████████████████████| 12.6MB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<1.16,>=1.14 in /usr/local/lib/python3.6/dist-packages (from tensorlayer>=1.9) (1.14.5)\n",
            "Collecting scipy<1.2,>=1.1 (from tensorlayer>=1.9)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/0b/f163da98d3a01b3e0ef1cab8dd2123c34aee2bafbb1c5bffa354cc8a1730/scipy-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (31.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 31.2MB 1.5MB/s \n",
            "\u001b[?25hCollecting tqdm<4.26,>=4.23 (from tensorlayer>=1.9)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c7/e0/52b2faaef4fd87f86eb8a8f1afa2cd6eb11146822033e29c04ac48ada32c/tqdm-4.25.0-py2.py3-none-any.whl (43kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 20.5MB/s \n",
            "\u001b[?25hCollecting python-utils>=2.3.0 (from progressbar2<3.39,>=3.38->tensorlayer>=1.9)\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/a0/19119d8b7c05be49baf6c593f11c432d571b70d805f2fe94c0585e55e4c8/python_utils-2.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from progressbar2<3.39,>=3.38->tensorlayer>=1.9) (1.11.0)\n",
            "Collecting cloudpickle>=0.2.1 (from scikit-image<0.15,>=0.14->tensorlayer>=1.9)\n",
            "  Downloading https://files.pythonhosted.org/packages/98/d6/a78a4589234cc6f47f29665c1225f30467db5fdaf4ca1fb52b0685bff108/cloudpickle-0.5.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15,>=0.14->tensorlayer>=1.9) (1.0.0)\n",
            "Requirement already satisfied: networkx>=1.8 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15,>=0.14->tensorlayer>=1.9) (2.1)\n",
            "Collecting dask[array]>=0.9.0 (from scikit-image<0.15,>=0.14->tensorlayer>=1.9)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/cc/d8b279ad3512e682069263de0494eeb9907ebe6b98bef01424be36421e13/dask-0.19.2-py2.py3-none-any.whl (657kB)\n",
            "\u001b[K    100% |████████████████████████████████| 665kB 23.7MB/s \n",
            "\u001b[?25hCollecting pillow>=4.3.0 (from scikit-image<0.15,>=0.14->tensorlayer>=1.9)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/24/f53ff6b61b3d728b90934bddb4f03f8ab584a7f49299bf3bde56e2952612/Pillow-5.2.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 16.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<2.8,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<2.20,>=2.19->tensorlayer>=1.9) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<2.20,>=2.19->tensorlayer>=1.9) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.24,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<2.20,>=2.19->tensorlayer>=1.9) (1.22)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<2.20,>=2.19->tensorlayer>=1.9) (2018.8.24)\n",
            "Collecting kiwisolver>=1.0.1 (from matplotlib<2.3,>=2.2->tensorlayer>=1.9)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/a7/88719d132b18300b4369fbffa741841cfd36d1e637e1990f27929945b538/kiwisolver-1.0.1-cp36-cp36m-manylinux1_x86_64.whl (949kB)\n",
            "\u001b[K    100% |████████████████████████████████| 952kB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib<2.3,>=2.2->tensorlayer>=1.9) (2.5.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib<2.3,>=2.2->tensorlayer>=1.9) (2.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib<2.3,>=2.2->tensorlayer>=1.9) (0.10.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from matplotlib<2.3,>=2.2->tensorlayer>=1.9) (2018.5)\n",
            "Requirement already satisfied: decorator>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=1.8->scikit-image<0.15,>=0.14->tensorlayer>=1.9) (4.3.0)\n",
            "Requirement already satisfied: toolz>=0.7.3; extra == \"array\" in /usr/local/lib/python3.6/dist-packages (from dask[array]>=0.9.0->scikit-image<0.15,>=0.14->tensorlayer>=1.9) (0.9.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib<2.3,>=2.2->tensorlayer>=1.9) (39.1.0)\n",
            "Building wheels for collected packages: imageio\n",
            "  Running setup.py bdist_wheel for imageio ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/e0/43/31/605de9372ceaf657f152d3d5e82f42cf265d81db8bbe63cde1\n",
            "Successfully built imageio\n",
            "Installing collected packages: python-utils, progressbar2, lxml, cloudpickle, scipy, dask, kiwisolver, matplotlib, pillow, scikit-image, imageio, requests, tqdm, tensorlayer\n",
            "  Found existing installation: scipy 0.19.1\n",
            "    Uninstalling scipy-0.19.1:\n",
            "      Successfully uninstalled scipy-0.19.1\n",
            "  Found existing installation: matplotlib 2.1.2\n",
            "    Uninstalling matplotlib-2.1.2:\n",
            "      Successfully uninstalled matplotlib-2.1.2\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "  Found existing installation: scikit-image 0.13.1\n",
            "    Uninstalling scikit-image-0.13.1:\n",
            "      Successfully uninstalled scikit-image-0.13.1\n",
            "  Found existing installation: requests 2.18.4\n",
            "    Uninstalling requests-2.18.4:\n",
            "      Successfully uninstalled requests-2.18.4\n",
            "  Found existing installation: tqdm 4.26.0\n",
            "    Uninstalling tqdm-4.26.0:\n",
            "      Successfully uninstalled tqdm-4.26.0\n",
            "Successfully installed cloudpickle-0.5.6 dask-0.19.2 imageio-2.4.1 kiwisolver-1.0.1 lxml-4.2.5 matplotlib-2.2.3 pillow-5.2.0 progressbar2-3.38.0 python-utils-2.3.0 requests-2.19.1 scikit-image-0.14.0 scipy-1.1.0 tensorlayer-1.10.1 tqdm-4.25.0\n",
            "### Found GPU at: /device:GPU:0 ###\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "s6-Nkua2njfA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **加载数据集**"
      ]
    },
    {
      "metadata": {
        "id": "aG-zzeFpP-PV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "outputId": "2706badb-8d5f-4f0f-c0b7-1d24aca2b4f6"
      },
      "cell_type": "code",
      "source": [
        "#########################################################################\n",
        "# Testing matrix multiply using GPU\n",
        "#########################################################################\n",
        "#sess = tf.InteractiveSession()\n",
        "\n",
        "#a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
        "#b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
        "#c = tf.matmul(a, b)\n",
        "# Runs the op.\n",
        "#print(sess.run(c))\n",
        "\n",
        "#sess.close()\n",
        "#########################################################################\n",
        "\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.DEBUG)\n",
        "tl.logging.set_verbosity(tl.logging.DEBUG)\n",
        "\n",
        "#sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
        "sess = tf.InteractiveSession()\n",
        "\n",
        "# prepare data\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = tl.files.load_mnist_dataset(shape=(-1, 784))\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(X_train[0].reshape(28,28),cmap='gray')\n",
        "plt.title(y_train[0])\n",
        "plt.show()\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[TL] Load or Download MNIST > data/mnist\n",
            "[TL] Downloading train-images-idx3-ubyte.gz...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100% (1211 of 1211) |####################| Elapsed Time: 0:00:03 ETA:  00:00:00"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[TL] Succesfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "[TL] data/mnist/train-images-idx3-ubyte.gz\n",
            "[TL] Downloading train-labels-idx1-ubyte.gz...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100% (4 of 4) |##########################| Elapsed Time: 0:00:00 ETA:  00:00:00"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[TL] Succesfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "[TL] Downloading t10k-images-idx3-ubyte.gz...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100% (202 of 202) |######################| Elapsed Time: 0:00:01 ETA:  00:00:00"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[TL] Succesfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "[TL] data/mnist/t10k-images-idx3-ubyte.gz\n",
            "[TL] Downloading t10k-labels-idx1-ubyte.gz...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100% (1 of 1) |##########################| Elapsed Time: 0:00:00 ETA:  00:00:00"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[TL] Succesfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "(50000, 784) (50000,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFZCAYAAAARqQ0OAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fbdd3074e80>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "9094Ox-unqL0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **构建网络模型，然后对模型进行训练、校验和测试**"
      ]
    },
    {
      "metadata": {
        "id": "gQW-n1n8jS88",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5715
        },
        "outputId": "033b687c-aedd-4166-899d-56059d8119c0"
      },
      "cell_type": "code",
      "source": [
        "# define placeholder\n",
        "x = tf.placeholder(tf.float32, shape=[None, 784], name='x')\n",
        "y_ = tf.placeholder(tf.int64, shape=[None], name='y_')\n",
        "\n",
        "# define the network\n",
        "network = tl.layers.InputLayer(x, name='input')\n",
        "network = tl.layers.DropoutLayer(network, keep=0.8, name='drop1')\n",
        "network = tl.layers.DenseLayer(network, 800, tf.nn.relu, name='relu1')\n",
        "network = tl.layers.DropoutLayer(network, keep=0.5, name='drop2')\n",
        "network = tl.layers.DenseLayer(network, 800, tf.nn.relu, name='relu2')\n",
        "network = tl.layers.DropoutLayer(network, keep=0.5, name='drop3')\n",
        "# the softmax is implemented internally in tl.cost.cross_entropy(y, y_) to\n",
        "# speed up computation, so we use identity here.\n",
        "# see tf.nn.sparse_softmax_cross_entropy_with_logits()\n",
        "network = tl.layers.DenseLayer(network, n_units=10, act=None, name='output')\n",
        "\n",
        "# define cost function and metric.\n",
        "y = network.outputs\n",
        "cost = tl.cost.cross_entropy(y, y_, name='cost')\n",
        "correct_prediction = tf.equal(tf.argmax(y, 1), y_)\n",
        "acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "y_op = tf.argmax(tf.nn.softmax(y), 1)\n",
        "\n",
        "# define the optimizer\n",
        "train_params = network.all_params\n",
        "train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost, var_list=train_params)\n",
        "\n",
        "# initialize all variables in the session\n",
        "tl.layers.initialize_global_variables(sess)\n",
        "\n",
        "# print network information\n",
        "network.print_params()\n",
        "network.print_layers()\n",
        "\n",
        "# train the network\n",
        "tl.utils.fit(sess, network, train_op, cost, X_train, y_train, x, y_, acc=acc, batch_size=500, \\\n",
        "    n_epoch=500, print_freq=5, X_val=X_val, y_val=y_val, eval_train=False)\n",
        "\n",
        "# evaluation\n",
        "tl.utils.test(sess, network, acc, X_test, y_test, x, y_, batch_size=None, cost=cost)\n",
        "\n",
        "# save the network to .npz file\n",
        "tl.files.save_npz(network.all_params, name='model.npz')\n",
        "\n",
        "sess.close()\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[TL] InputLayer  input: (?, 784)\n",
            "[TL] DropoutLayer drop1: keep: 0.800000 is_fix: False\n",
            "[TL] DenseLayer  relu1: 800 relu\n",
            "[TL] DropoutLayer drop2: keep: 0.500000 is_fix: False\n",
            "[TL] DenseLayer  relu2: 800 relu\n",
            "[TL] DropoutLayer drop3: keep: 0.500000 is_fix: False\n",
            "[TL] DenseLayer  output: 10 No Activation\n",
            "[TL] WARNING: Function: `tensorlayer.layers.utils.initialize_global_variables` (in file: /usr/local/lib/python3.6/dist-packages/tensorlayer/layers/utils.py) is deprecated and will be removed after 2018-09-30.\n",
            "Instructions for updating: This API is deprecated in favor of `tf.global_variables_initializer`\n",
            "\n",
            "[TL]   param   0: relu1/W:0            (784, 800)         float32_ref (mean: 6.204535748111084e-05, median: 4.676779644796625e-05, std: 0.08797505497932434)   \n",
            "[TL]   param   1: relu1/b:0            (800,)             float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
            "[TL]   param   2: relu2/W:0            (800, 800)         float32_ref (mean: 0.00019988165877293795, median: 0.00028623142861761153, std: 0.08798720687627792)   \n",
            "[TL]   param   3: relu2/b:0            (800,)             float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
            "[TL]   param   4: output/W:0           (800, 10)          float32_ref (mean: -0.0015980482567101717, median: -0.0015044535975903273, std: 0.08772966265678406)   \n",
            "[TL]   param   5: output/b:0           (10,)              float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
            "[TL]   num of params: 1276810\n",
            "[TL]   layer   0: x:0                  (?, 784)           float32\n",
            "[TL]   layer   1: drop1/mul:0          (?, 784)           float32\n",
            "[TL]   layer   2: relu1/Relu:0         (?, 800)           float32\n",
            "[TL]   layer   3: drop2/mul:0          (?, 800)           float32\n",
            "[TL]   layer   4: relu2/Relu:0         (?, 800)           float32\n",
            "[TL]   layer   5: drop3/mul:0          (?, 800)           float32\n",
            "[TL]   layer   6: output/bias_add:0    (?, 10)            float32\n",
            "[TL] Start training the network ...\n",
            "[TL] Epoch 1 of 500 took 0.962796s\n",
            "[TL]    val loss: 0.530463\n",
            "[TL]    val acc: 0.837800\n",
            "[TL] Epoch 5 of 500 took 0.674055s\n",
            "[TL]    val loss: 0.287761\n",
            "[TL]    val acc: 0.915600\n",
            "[TL] Epoch 10 of 500 took 0.668140s\n",
            "[TL]    val loss: 0.225964\n",
            "[TL]    val acc: 0.937600\n",
            "[TL] Epoch 15 of 500 took 0.668779s\n",
            "[TL]    val loss: 0.190002\n",
            "[TL]    val acc: 0.948500\n",
            "[TL] Epoch 20 of 500 took 0.684237s\n",
            "[TL]    val loss: 0.165031\n",
            "[TL]    val acc: 0.955200\n",
            "[TL] Epoch 25 of 500 took 0.679428s\n",
            "[TL]    val loss: 0.147252\n",
            "[TL]    val acc: 0.959700\n",
            "[TL] Epoch 30 of 500 took 0.675760s\n",
            "[TL]    val loss: 0.132869\n",
            "[TL]    val acc: 0.964400\n",
            "[TL] Epoch 35 of 500 took 0.680125s\n",
            "[TL]    val loss: 0.121056\n",
            "[TL]    val acc: 0.967800\n",
            "[TL] Epoch 40 of 500 took 0.685960s\n",
            "[TL]    val loss: 0.113342\n",
            "[TL]    val acc: 0.969900\n",
            "[TL] Epoch 45 of 500 took 0.686285s\n",
            "[TL]    val loss: 0.105987\n",
            "[TL]    val acc: 0.970800\n",
            "[TL] Epoch 50 of 500 took 0.681470s\n",
            "[TL]    val loss: 0.099945\n",
            "[TL]    val acc: 0.972500\n",
            "[TL] Epoch 55 of 500 took 0.670450s\n",
            "[TL]    val loss: 0.094075\n",
            "[TL]    val acc: 0.973400\n",
            "[TL] Epoch 60 of 500 took 0.664481s\n",
            "[TL]    val loss: 0.089687\n",
            "[TL]    val acc: 0.974400\n",
            "[TL] Epoch 65 of 500 took 0.673545s\n",
            "[TL]    val loss: 0.086161\n",
            "[TL]    val acc: 0.976200\n",
            "[TL] Epoch 70 of 500 took 0.666094s\n",
            "[TL]    val loss: 0.083401\n",
            "[TL]    val acc: 0.975500\n",
            "[TL] Epoch 75 of 500 took 0.678323s\n",
            "[TL]    val loss: 0.079102\n",
            "[TL]    val acc: 0.977400\n",
            "[TL] Epoch 80 of 500 took 0.686105s\n",
            "[TL]    val loss: 0.076373\n",
            "[TL]    val acc: 0.978100\n",
            "[TL] Epoch 85 of 500 took 0.684162s\n",
            "[TL]    val loss: 0.074624\n",
            "[TL]    val acc: 0.978700\n",
            "[TL] Epoch 90 of 500 took 0.673035s\n",
            "[TL]    val loss: 0.071478\n",
            "[TL]    val acc: 0.979000\n",
            "[TL] Epoch 95 of 500 took 0.695956s\n",
            "[TL]    val loss: 0.071229\n",
            "[TL]    val acc: 0.979400\n",
            "[TL] Epoch 100 of 500 took 0.679402s\n",
            "[TL]    val loss: 0.068043\n",
            "[TL]    val acc: 0.980200\n",
            "[TL] Epoch 105 of 500 took 0.677679s\n",
            "[TL]    val loss: 0.066826\n",
            "[TL]    val acc: 0.980100\n",
            "[TL] Epoch 110 of 500 took 0.655632s\n",
            "[TL]    val loss: 0.065622\n",
            "[TL]    val acc: 0.981300\n",
            "[TL] Epoch 115 of 500 took 0.665174s\n",
            "[TL]    val loss: 0.064673\n",
            "[TL]    val acc: 0.980800\n",
            "[TL] Epoch 120 of 500 took 0.684171s\n",
            "[TL]    val loss: 0.063199\n",
            "[TL]    val acc: 0.981500\n",
            "[TL] Epoch 125 of 500 took 0.675475s\n",
            "[TL]    val loss: 0.061749\n",
            "[TL]    val acc: 0.981700\n",
            "[TL] Epoch 130 of 500 took 0.676258s\n",
            "[TL]    val loss: 0.060713\n",
            "[TL]    val acc: 0.981900\n",
            "[TL] Epoch 135 of 500 took 0.667690s\n",
            "[TL]    val loss: 0.060498\n",
            "[TL]    val acc: 0.982500\n",
            "[TL] Epoch 140 of 500 took 0.672724s\n",
            "[TL]    val loss: 0.059537\n",
            "[TL]    val acc: 0.982600\n",
            "[TL] Epoch 145 of 500 took 0.667213s\n",
            "[TL]    val loss: 0.060370\n",
            "[TL]    val acc: 0.982000\n",
            "[TL] Epoch 150 of 500 took 0.665992s\n",
            "[TL]    val loss: 0.058682\n",
            "[TL]    val acc: 0.982400\n",
            "[TL] Epoch 155 of 500 took 0.678831s\n",
            "[TL]    val loss: 0.058262\n",
            "[TL]    val acc: 0.982800\n",
            "[TL] Epoch 160 of 500 took 0.682547s\n",
            "[TL]    val loss: 0.058274\n",
            "[TL]    val acc: 0.983000\n",
            "[TL] Epoch 165 of 500 took 0.683110s\n",
            "[TL]    val loss: 0.057429\n",
            "[TL]    val acc: 0.982900\n",
            "[TL] Epoch 170 of 500 took 0.680608s\n",
            "[TL]    val loss: 0.056750\n",
            "[TL]    val acc: 0.983300\n",
            "[TL] Epoch 175 of 500 took 0.669067s\n",
            "[TL]    val loss: 0.056304\n",
            "[TL]    val acc: 0.983600\n",
            "[TL] Epoch 180 of 500 took 0.672934s\n",
            "[TL]    val loss: 0.056410\n",
            "[TL]    val acc: 0.984300\n",
            "[TL] Epoch 185 of 500 took 0.681370s\n",
            "[TL]    val loss: 0.057252\n",
            "[TL]    val acc: 0.983300\n",
            "[TL] Epoch 190 of 500 took 0.675210s\n",
            "[TL]    val loss: 0.056128\n",
            "[TL]    val acc: 0.984500\n",
            "[TL] Epoch 195 of 500 took 0.659927s\n",
            "[TL]    val loss: 0.056166\n",
            "[TL]    val acc: 0.984100\n",
            "[TL] Epoch 200 of 500 took 0.672302s\n",
            "[TL]    val loss: 0.055931\n",
            "[TL]    val acc: 0.983000\n",
            "[TL] Epoch 205 of 500 took 0.669529s\n",
            "[TL]    val loss: 0.054859\n",
            "[TL]    val acc: 0.983800\n",
            "[TL] Epoch 210 of 500 took 0.681853s\n",
            "[TL]    val loss: 0.054552\n",
            "[TL]    val acc: 0.984100\n",
            "[TL] Epoch 215 of 500 took 0.678028s\n",
            "[TL]    val loss: 0.055083\n",
            "[TL]    val acc: 0.984300\n",
            "[TL] Epoch 220 of 500 took 0.675294s\n",
            "[TL]    val loss: 0.054814\n",
            "[TL]    val acc: 0.984000\n",
            "[TL] Epoch 225 of 500 took 0.676833s\n",
            "[TL]    val loss: 0.054465\n",
            "[TL]    val acc: 0.984400\n",
            "[TL] Epoch 230 of 500 took 0.675793s\n",
            "[TL]    val loss: 0.054678\n",
            "[TL]    val acc: 0.984800\n",
            "[TL] Epoch 235 of 500 took 0.678148s\n",
            "[TL]    val loss: 0.054870\n",
            "[TL]    val acc: 0.983800\n",
            "[TL] Epoch 240 of 500 took 0.665356s\n",
            "[TL]    val loss: 0.054891\n",
            "[TL]    val acc: 0.984400\n",
            "[TL] Epoch 245 of 500 took 0.665906s\n",
            "[TL]    val loss: 0.055614\n",
            "[TL]    val acc: 0.984200\n",
            "[TL] Epoch 250 of 500 took 0.676934s\n",
            "[TL]    val loss: 0.055113\n",
            "[TL]    val acc: 0.984100\n",
            "[TL] Epoch 255 of 500 took 0.677972s\n",
            "[TL]    val loss: 0.054534\n",
            "[TL]    val acc: 0.984700\n",
            "[TL] Epoch 260 of 500 took 0.663766s\n",
            "[TL]    val loss: 0.054261\n",
            "[TL]    val acc: 0.985000\n",
            "[TL] Epoch 265 of 500 took 0.666460s\n",
            "[TL]    val loss: 0.053286\n",
            "[TL]    val acc: 0.985000\n",
            "[TL] Epoch 270 of 500 took 0.671971s\n",
            "[TL]    val loss: 0.054240\n",
            "[TL]    val acc: 0.985300\n",
            "[TL] Epoch 275 of 500 took 0.680334s\n",
            "[TL]    val loss: 0.053713\n",
            "[TL]    val acc: 0.985700\n",
            "[TL] Epoch 280 of 500 took 0.681342s\n",
            "[TL]    val loss: 0.054360\n",
            "[TL]    val acc: 0.984800\n",
            "[TL] Epoch 285 of 500 took 0.662802s\n",
            "[TL]    val loss: 0.054510\n",
            "[TL]    val acc: 0.985400\n",
            "[TL] Epoch 290 of 500 took 0.681463s\n",
            "[TL]    val loss: 0.054560\n",
            "[TL]    val acc: 0.985600\n",
            "[TL] Epoch 295 of 500 took 0.674334s\n",
            "[TL]    val loss: 0.054038\n",
            "[TL]    val acc: 0.985900\n",
            "[TL] Epoch 300 of 500 took 0.675599s\n",
            "[TL]    val loss: 0.054257\n",
            "[TL]    val acc: 0.985800\n",
            "[TL] Epoch 305 of 500 took 0.663035s\n",
            "[TL]    val loss: 0.055588\n",
            "[TL]    val acc: 0.985600\n",
            "[TL] Epoch 310 of 500 took 0.672815s\n",
            "[TL]    val loss: 0.055339\n",
            "[TL]    val acc: 0.985700\n",
            "[TL] Epoch 315 of 500 took 0.680954s\n",
            "[TL]    val loss: 0.055832\n",
            "[TL]    val acc: 0.985000\n",
            "[TL] Epoch 320 of 500 took 0.676422s\n",
            "[TL]    val loss: 0.055498\n",
            "[TL]    val acc: 0.985700\n",
            "[TL] Epoch 325 of 500 took 0.667973s\n",
            "[TL]    val loss: 0.054606\n",
            "[TL]    val acc: 0.985400\n",
            "[TL] Epoch 330 of 500 took 0.665642s\n",
            "[TL]    val loss: 0.055200\n",
            "[TL]    val acc: 0.985000\n",
            "[TL] Epoch 335 of 500 took 0.677352s\n",
            "[TL]    val loss: 0.054744\n",
            "[TL]    val acc: 0.985600\n",
            "[TL] Epoch 340 of 500 took 0.675435s\n",
            "[TL]    val loss: 0.055576\n",
            "[TL]    val acc: 0.985300\n",
            "[TL] Epoch 345 of 500 took 0.665454s\n",
            "[TL]    val loss: 0.055453\n",
            "[TL]    val acc: 0.985800\n",
            "[TL] Epoch 350 of 500 took 0.676205s\n",
            "[TL]    val loss: 0.054235\n",
            "[TL]    val acc: 0.986300\n",
            "[TL] Epoch 355 of 500 took 0.682302s\n",
            "[TL]    val loss: 0.054250\n",
            "[TL]    val acc: 0.986300\n",
            "[TL] Epoch 360 of 500 took 0.681633s\n",
            "[TL]    val loss: 0.055687\n",
            "[TL]    val acc: 0.985900\n",
            "[TL] Epoch 365 of 500 took 0.669279s\n",
            "[TL]    val loss: 0.054569\n",
            "[TL]    val acc: 0.985500\n",
            "[TL] Epoch 370 of 500 took 0.661901s\n",
            "[TL]    val loss: 0.054479\n",
            "[TL]    val acc: 0.985400\n",
            "[TL] Epoch 375 of 500 took 0.675195s\n",
            "[TL]    val loss: 0.055301\n",
            "[TL]    val acc: 0.985300\n",
            "[TL] Epoch 380 of 500 took 0.687025s\n",
            "[TL]    val loss: 0.055942\n",
            "[TL]    val acc: 0.985300\n",
            "[TL] Epoch 385 of 500 took 0.683591s\n",
            "[TL]    val loss: 0.055649\n",
            "[TL]    val acc: 0.985200\n",
            "[TL] Epoch 390 of 500 took 0.661864s\n",
            "[TL]    val loss: 0.055304\n",
            "[TL]    val acc: 0.985000\n",
            "[TL] Epoch 395 of 500 took 0.675565s\n",
            "[TL]    val loss: 0.054964\n",
            "[TL]    val acc: 0.986000\n",
            "[TL] Epoch 400 of 500 took 0.686500s\n",
            "[TL]    val loss: 0.054708\n",
            "[TL]    val acc: 0.986300\n",
            "[TL] Epoch 405 of 500 took 0.674134s\n",
            "[TL]    val loss: 0.055338\n",
            "[TL]    val acc: 0.986100\n",
            "[TL] Epoch 410 of 500 took 0.684692s\n",
            "[TL]    val loss: 0.054898\n",
            "[TL]    val acc: 0.986700\n",
            "[TL] Epoch 415 of 500 took 0.671852s\n",
            "[TL]    val loss: 0.055323\n",
            "[TL]    val acc: 0.986100\n",
            "[TL] Epoch 420 of 500 took 0.659035s\n",
            "[TL]    val loss: 0.054686\n",
            "[TL]    val acc: 0.986200\n",
            "[TL] Epoch 425 of 500 took 0.685851s\n",
            "[TL]    val loss: 0.055840\n",
            "[TL]    val acc: 0.985900\n",
            "[TL] Epoch 430 of 500 took 0.679927s\n",
            "[TL]    val loss: 0.056396\n",
            "[TL]    val acc: 0.986100\n",
            "[TL] Epoch 435 of 500 took 0.665772s\n",
            "[TL]    val loss: 0.056398\n",
            "[TL]    val acc: 0.986300\n",
            "[TL] Epoch 440 of 500 took 0.672646s\n",
            "[TL]    val loss: 0.054419\n",
            "[TL]    val acc: 0.986500\n",
            "[TL] Epoch 445 of 500 took 0.682424s\n",
            "[TL]    val loss: 0.055646\n",
            "[TL]    val acc: 0.986600\n",
            "[TL] Epoch 450 of 500 took 0.675835s\n",
            "[TL]    val loss: 0.054888\n",
            "[TL]    val acc: 0.986800\n",
            "[TL] Epoch 455 of 500 took 0.676204s\n",
            "[TL]    val loss: 0.055698\n",
            "[TL]    val acc: 0.986400\n",
            "[TL] Epoch 460 of 500 took 0.669990s\n",
            "[TL]    val loss: 0.055515\n",
            "[TL]    val acc: 0.986700\n",
            "[TL] Epoch 465 of 500 took 0.688464s\n",
            "[TL]    val loss: 0.054941\n",
            "[TL]    val acc: 0.986700\n",
            "[TL] Epoch 470 of 500 took 0.683843s\n",
            "[TL]    val loss: 0.055188\n",
            "[TL]    val acc: 0.986700\n",
            "[TL] Epoch 475 of 500 took 0.676746s\n",
            "[TL]    val loss: 0.055766\n",
            "[TL]    val acc: 0.986600\n",
            "[TL] Epoch 480 of 500 took 0.681496s\n",
            "[TL]    val loss: 0.056567\n",
            "[TL]    val acc: 0.986600\n",
            "[TL] Epoch 485 of 500 took 0.680518s\n",
            "[TL]    val loss: 0.055953\n",
            "[TL]    val acc: 0.987000\n",
            "[TL] Epoch 490 of 500 took 0.678392s\n",
            "[TL]    val loss: 0.056751\n",
            "[TL]    val acc: 0.986700\n",
            "[TL] Epoch 495 of 500 took 0.683779s\n",
            "[TL]    val loss: 0.056010\n",
            "[TL]    val acc: 0.987000\n",
            "[TL] Epoch 500 of 500 took 0.683076s\n",
            "[TL]    val loss: 0.056677\n",
            "[TL]    val acc: 0.986400\n",
            "[TL] Total training time: 347.383082s\n",
            "[TL] Start testing the network ...\n",
            "[TL]    test loss: 0.047048\n",
            "[TL]    test acc: 0.987100\n",
            "[TL] [*] Saving TL params into model.npz\n",
            "[TL] [*] Saved\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}