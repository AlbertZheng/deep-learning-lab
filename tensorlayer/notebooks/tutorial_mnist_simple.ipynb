{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tutorial_mnist_simple.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/AlbertZheng/deep-learning-lab/blob/master/tensorlayer-lab/notebooks/tutorial_mnist_simple.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "gQW-n1n8jS88",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7245
        },
        "outputId": "ba3c3e83-e93b-4994-8773-4a370d3c1700"
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorlayer\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorlayer as tl\n",
        "\n",
        "\n",
        "# Confirm TensorFlow can see the GPU on Colaboratory\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "\n",
        "#########################################################################\n",
        "# Testing matrix multiply using GPU\n",
        "#########################################################################\n",
        "#sess = tf.InteractiveSession()\n",
        "\n",
        "#a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
        "#b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
        "#c = tf.matmul(a, b)\n",
        "# Runs the op.\n",
        "#print(sess.run(c))\n",
        "\n",
        "#sess.close()\n",
        "#########################################################################\n",
        "\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.DEBUG)\n",
        "tl.logging.set_verbosity(tl.logging.DEBUG)\n",
        "\n",
        "#sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
        "sess = tf.InteractiveSession()\n",
        "\n",
        "# prepare data\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = tl.files.load_mnist_dataset(shape=(-1, 784))\n",
        "# define placeholder\n",
        "x = tf.placeholder(tf.float32, shape=[None, 784], name='x')\n",
        "y_ = tf.placeholder(tf.int64, shape=[None], name='y_')\n",
        "\n",
        "# define the network\n",
        "network = tl.layers.InputLayer(x, name='input')\n",
        "network = tl.layers.DropoutLayer(network, keep=0.8, name='drop1')\n",
        "network = tl.layers.DenseLayer(network, 800, tf.nn.relu, name='relu1')\n",
        "network = tl.layers.DropoutLayer(network, keep=0.5, name='drop2')\n",
        "network = tl.layers.DenseLayer(network, 800, tf.nn.relu, name='relu2')\n",
        "network = tl.layers.DropoutLayer(network, keep=0.5, name='drop3')\n",
        "# the softmax is implemented internally in tl.cost.cross_entropy(y, y_) to\n",
        "# speed up computation, so we use identity here.\n",
        "# see tf.nn.sparse_softmax_cross_entropy_with_logits()\n",
        "network = tl.layers.DenseLayer(network, n_units=10, act=None, name='output')\n",
        "\n",
        "# define cost function and metric.\n",
        "y = network.outputs\n",
        "cost = tl.cost.cross_entropy(y, y_, name='cost')\n",
        "correct_prediction = tf.equal(tf.argmax(y, 1), y_)\n",
        "acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "y_op = tf.argmax(tf.nn.softmax(y), 1)\n",
        "\n",
        "# define the optimizer\n",
        "train_params = network.all_params\n",
        "train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost, var_list=train_params)\n",
        "\n",
        "# initialize all variables in the session\n",
        "tl.layers.initialize_global_variables(sess)\n",
        "\n",
        "# print network information\n",
        "network.print_params()\n",
        "network.print_layers()\n",
        "\n",
        "# train the network\n",
        "tl.utils.fit(sess, network, train_op, cost, X_train, y_train, x, y_, acc=acc, batch_size=500, \\\n",
        "    n_epoch=500, print_freq=5, X_val=X_val, y_val=y_val, eval_train=False)\n",
        "\n",
        "# evaluation\n",
        "tl.utils.test(sess, network, acc, X_test, y_test, x, y_, batch_size=None, cost=cost)\n",
        "\n",
        "# save the network to .npz file\n",
        "tl.files.save_npz(network.all_params, name='model.npz')\n",
        "\n",
        "sess.close()\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorlayer\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/e5/8878794d5a5b72d80803216e55ccd0fcaf4308ee593a6ec96bab2974c36d/tensorlayer-1.9.1-py2.py3-none-any.whl (245kB)\n",
            "\u001b[K    100% |████████████████████████████████| 245kB 4.8MB/s \n",
            "\u001b[?25hCollecting imageio<2.4,>=2.3 (from tensorlayer)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a7/1d/33c8686072148b3b0fcc12a2e0857dd8316b8ae20a0fa66c8d6a6d01c05c/imageio-2.3.0-py2.py3-none-any.whl (3.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 3.3MB 2.5MB/s \n",
            "\u001b[?25hCollecting requests<2.20,>=2.19 (from tensorlayer)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/47/7e02164a2a3db50ed6d8a6ab1d6d60b69c4c3fdf57a284257925dfc12bda/requests-2.19.1-py2.py3-none-any.whl (91kB)\n",
            "\u001b[K    100% |████████████████████████████████| 92kB 13.0MB/s \n",
            "\u001b[?25hCollecting wrapt<1.11,>=1.10 (from tensorlayer)\n",
            "  Downloading https://files.pythonhosted.org/packages/a0/47/66897906448185fcb77fc3c2b1bc20ed0ecca81a0f2f88eda3fc5a34fc3d/wrapt-1.10.11.tar.gz\n",
            "Collecting matplotlib<2.3,>=2.2 (from tensorlayer)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/59/f235ab21bbe7b7c6570c4abf17ffb893071f4fa3b9cf557b09b60359ad9a/matplotlib-2.2.3-cp36-cp36m-manylinux1_x86_64.whl (12.6MB)\n",
            "\u001b[K    100% |████████████████████████████████| 12.6MB 1.8MB/s \n",
            "\u001b[?25hCollecting lxml<4.3,>=4.2 (from tensorlayer)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/59/1db3c9c27049e4f832691c6d642df1f5b64763f73942172c44fee22de397/lxml-4.2.4-cp36-cp36m-manylinux1_x86_64.whl (5.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 5.8MB 1.7MB/s \n",
            "\u001b[?25hCollecting scipy<1.2,>=1.1 (from tensorlayer)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/0b/f163da98d3a01b3e0ef1cab8dd2123c34aee2bafbb1c5bffa354cc8a1730/scipy-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (31.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 31.2MB 635kB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn<0.20,>=0.19 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (0.19.2)\n",
            "Collecting tqdm<4.24,>=4.23 (from tensorlayer)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/24/6ab1df969db228aed36a648a8959d1027099ce45fad67532b9673d533318/tqdm-4.23.4-py2.py3-none-any.whl (42kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<1.15,>=1.14 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (1.14.5)\n",
            "Collecting scikit-image<0.15,>=0.14 (from tensorlayer)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/79/cefff573a53ca3fb4c390739d19541b95f371e24d2990aed4cd8837971f0/scikit_image-0.14.0-cp36-cp36m-manylinux1_x86_64.whl (25.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 25.3MB 1.2MB/s \n",
            "\u001b[?25hCollecting progressbar2<3.39,>=3.38 (from tensorlayer)\n",
            "  Downloading https://files.pythonhosted.org/packages/4f/6f/acb2dd76f2c77527584bd3a4c2509782bb35c481c610521fc3656de5a9e0/progressbar2-3.38.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio<2.4,>=2.3->tensorlayer) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<2.20,>=2.19->tensorlayer) (2018.8.13)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<2.20,>=2.19->tensorlayer) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.24,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<2.20,>=2.19->tensorlayer) (1.22)\n",
            "Requirement already satisfied: idna<2.8,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<2.20,>=2.19->tensorlayer) (2.6)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib<2.3,>=2.2->tensorlayer) (1.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib<2.3,>=2.2->tensorlayer) (2.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib<2.3,>=2.2->tensorlayer) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib<2.3,>=2.2->tensorlayer) (2.5.3)\n",
            "Collecting kiwisolver>=1.0.1 (from matplotlib<2.3,>=2.2->tensorlayer)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/a7/88719d132b18300b4369fbffa741841cfd36d1e637e1990f27929945b538/kiwisolver-1.0.1-cp36-cp36m-manylinux1_x86_64.whl (949kB)\n",
            "\u001b[K    100% |████████████████████████████████| 952kB 4.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from matplotlib<2.3,>=2.2->tensorlayer) (2018.5)\n",
            "Collecting dask[array]>=0.9.0 (from scikit-image<0.15,>=0.14->tensorlayer)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/52/427541141707ac533a6dcafa3bff7ff8b57ded3c9ea84d6ec014d15fbbff/dask-0.18.2-py2.py3-none-any.whl (645kB)\n",
            "\u001b[K    100% |████████████████████████████████| 655kB 4.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx>=1.8 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15,>=0.14->tensorlayer) (2.1)\n",
            "Collecting cloudpickle>=0.2.1 (from scikit-image<0.15,>=0.14->tensorlayer)\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/bf/60ae7ec1e8c6742d2abbb6819c39a48ee796793bcdb7e1d5e41a3e379ddd/cloudpickle-0.5.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15,>=0.14->tensorlayer) (0.5.2)\n",
            "Collecting python-utils>=2.3.0 (from progressbar2<3.39,>=3.38->tensorlayer)\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/a0/19119d8b7c05be49baf6c593f11c432d571b70d805f2fe94c0585e55e4c8/python_utils-2.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->imageio<2.4,>=2.3->tensorlayer) (0.45.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib<2.3,>=2.2->tensorlayer) (39.1.0)\n",
            "Requirement already satisfied: toolz>=0.7.3; extra == \"array\" in /usr/local/lib/python3.6/dist-packages (from dask[array]>=0.9.0->scikit-image<0.15,>=0.14->tensorlayer) (0.9.0)\n",
            "Requirement already satisfied: decorator>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=1.8->scikit-image<0.15,>=0.14->tensorlayer) (4.3.0)\n",
            "Building wheels for collected packages: wrapt\n",
            "  Running setup.py bdist_wheel for wrapt ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/48/5d/04/22361a593e70d23b1f7746d932802efe1f0e523376a74f321e\n",
            "Successfully built wrapt\n",
            "\u001b[31mscikit-image 0.14.0 has requirement pillow>=4.3.0, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: imageio, requests, wrapt, kiwisolver, matplotlib, lxml, scipy, tqdm, dask, cloudpickle, scikit-image, python-utils, progressbar2, tensorlayer\n",
            "  Found existing installation: requests 2.18.4\n",
            "    Uninstalling requests-2.18.4:\n",
            "      Successfully uninstalled requests-2.18.4\n",
            "  Found existing installation: matplotlib 2.1.2\n",
            "    Uninstalling matplotlib-2.1.2:\n",
            "      Successfully uninstalled matplotlib-2.1.2\n",
            "  Found existing installation: scipy 0.19.1\n",
            "    Uninstalling scipy-0.19.1:\n",
            "      Successfully uninstalled scipy-0.19.1\n",
            "  Found existing installation: scikit-image 0.13.1\n",
            "    Uninstalling scikit-image-0.13.1:\n",
            "      Successfully uninstalled scikit-image-0.13.1\n",
            "Successfully installed cloudpickle-0.5.3 dask-0.18.2 imageio-2.3.0 kiwisolver-1.0.1 lxml-4.2.4 matplotlib-2.2.3 progressbar2-3.38.0 python-utils-2.3.0 requests-2.19.1 scikit-image-0.14.0 scipy-1.1.0 tensorlayer-1.9.1 tqdm-4.23.4 wrapt-1.10.11\n",
            "Found GPU at: /device:GPU:0\n",
            "[TL] Load or Download MNIST > data/mnist\n",
            "[TL] Downloading train-images-idx3-ubyte.gz...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100% (1211 of 1211) |####################| Elapsed Time: 0:00:02 ETA:  00:00:00"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[TL] Succesfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "[TL] data/mnist/train-images-idx3-ubyte.gz\n",
            "[TL] Downloading train-labels-idx1-ubyte.gz...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100% (4 of 4) |##########################| Elapsed Time: 0:00:00 ETA:  00:00:00"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[TL] Succesfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "[TL] Downloading t10k-images-idx3-ubyte.gz...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100% (202 of 202) |######################| Elapsed Time: 0:00:00 ETA:  00:00:00"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[TL] Succesfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "[TL] data/mnist/t10k-images-idx3-ubyte.gz\n",
            "[TL] Downloading t10k-labels-idx1-ubyte.gz...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100% (1 of 1) |##########################| Elapsed Time: 0:00:00 ETA:  00:00:00"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[TL] Succesfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "[TL] InputLayer  input: (?, 784)\n",
            "[TL] DropoutLayer drop1: keep: 0.800000 is_fix: False\n",
            "[TL] DenseLayer  relu1: 800 relu\n",
            "[TL] DropoutLayer drop2: keep: 0.500000 is_fix: False\n",
            "[TL] DenseLayer  relu2: 800 relu\n",
            "[TL] DropoutLayer drop3: keep: 0.500000 is_fix: False\n",
            "[TL] DenseLayer  output: 10 No Activation\n",
            "[TL] WARNING: From <ipython-input-1-0d9ea6fee5cf>:65: initialize_global_variables (from tensorlayer.layers.utils) is deprecated and will be removed after 2018-09-30.\n",
            "Instructions for updating: This API is deprecated in favor of `tf.global_variables_initializer`\n",
            "\n",
            "[TL]   param   0: relu1/W:0            (784, 800)         float32_ref (mean: 3.110886245849542e-05, median: 9.065568883670494e-05, std: 0.08806448429822922)   \n",
            "[TL]   param   1: relu1/b:0            (800,)             float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
            "[TL]   param   2: relu2/W:0            (800, 800)         float32_ref (mean: -2.5472405468462966e-05, median: 4.993636321160011e-05, std: 0.0878605842590332)   \n",
            "[TL]   param   3: relu2/b:0            (800,)             float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
            "[TL]   param   4: output/W:0           (800, 10)          float32_ref (mean: 0.00031477652373723686, median: -2.054627657344099e-05, std: 0.08798110485076904)   \n",
            "[TL]   param   5: output/b:0           (10,)              float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
            "[TL]   num of params: 1276810\n",
            "[TL]   layer   0: x:0                  (?, 784)           float32\n",
            "[TL]   layer   1: drop1/mul:0          (?, 784)           float32\n",
            "[TL]   layer   2: relu1/Relu:0         (?, 800)           float32\n",
            "[TL]   layer   3: drop2/mul:0          (?, 800)           float32\n",
            "[TL]   layer   4: relu2/Relu:0         (?, 800)           float32\n",
            "[TL]   layer   5: drop3/mul:0          (?, 800)           float32\n",
            "[TL]   layer   6: output/bias_add:0    (?, 10)            float32\n",
            "[TL] Start training the network ...\n",
            "[TL] Epoch 1 of 500 took 0.961545s\n",
            "[TL]    val loss: 0.578859\n",
            "[TL]    val acc: 0.815800\n",
            "[TL] Epoch 5 of 500 took 0.677049s\n",
            "[TL]    val loss: 0.284044\n",
            "[TL]    val acc: 0.916900\n",
            "[TL] Epoch 10 of 500 took 0.698006s\n",
            "[TL]    val loss: 0.218201\n",
            "[TL]    val acc: 0.937900\n",
            "[TL] Epoch 15 of 500 took 0.672513s\n",
            "[TL]    val loss: 0.184893\n",
            "[TL]    val acc: 0.949500\n",
            "[TL] Epoch 20 of 500 took 0.668114s\n",
            "[TL]    val loss: 0.161451\n",
            "[TL]    val acc: 0.957400\n",
            "[TL] Epoch 25 of 500 took 0.666084s\n",
            "[TL]    val loss: 0.144805\n",
            "[TL]    val acc: 0.962700\n",
            "[TL] Epoch 30 of 500 took 0.674746s\n",
            "[TL]    val loss: 0.131748\n",
            "[TL]    val acc: 0.965700\n",
            "[TL] Epoch 35 of 500 took 0.668584s\n",
            "[TL]    val loss: 0.120899\n",
            "[TL]    val acc: 0.968400\n",
            "[TL] Epoch 40 of 500 took 0.669169s\n",
            "[TL]    val loss: 0.110795\n",
            "[TL]    val acc: 0.972000\n",
            "[TL] Epoch 45 of 500 took 0.664579s\n",
            "[TL]    val loss: 0.104189\n",
            "[TL]    val acc: 0.972300\n",
            "[TL] Epoch 50 of 500 took 0.686726s\n",
            "[TL]    val loss: 0.099046\n",
            "[TL]    val acc: 0.973600\n",
            "[TL] Epoch 55 of 500 took 0.682305s\n",
            "[TL]    val loss: 0.093027\n",
            "[TL]    val acc: 0.974200\n",
            "[TL] Epoch 60 of 500 took 0.667246s\n",
            "[TL]    val loss: 0.088943\n",
            "[TL]    val acc: 0.976100\n",
            "[TL] Epoch 65 of 500 took 0.659580s\n",
            "[TL]    val loss: 0.085272\n",
            "[TL]    val acc: 0.977500\n",
            "[TL] Epoch 70 of 500 took 0.665891s\n",
            "[TL]    val loss: 0.081104\n",
            "[TL]    val acc: 0.977500\n",
            "[TL] Epoch 75 of 500 took 0.663733s\n",
            "[TL]    val loss: 0.078894\n",
            "[TL]    val acc: 0.977900\n",
            "[TL] Epoch 80 of 500 took 0.660675s\n",
            "[TL]    val loss: 0.075822\n",
            "[TL]    val acc: 0.978900\n",
            "[TL] Epoch 85 of 500 took 0.661946s\n",
            "[TL]    val loss: 0.072972\n",
            "[TL]    val acc: 0.980200\n",
            "[TL] Epoch 90 of 500 took 0.672184s\n",
            "[TL]    val loss: 0.070848\n",
            "[TL]    val acc: 0.980500\n",
            "[TL] Epoch 95 of 500 took 0.674381s\n",
            "[TL]    val loss: 0.070430\n",
            "[TL]    val acc: 0.980900\n",
            "[TL] Epoch 100 of 500 took 0.673003s\n",
            "[TL]    val loss: 0.067420\n",
            "[TL]    val acc: 0.981900\n",
            "[TL] Epoch 105 of 500 took 0.674073s\n",
            "[TL]    val loss: 0.065623\n",
            "[TL]    val acc: 0.982700\n",
            "[TL] Epoch 110 of 500 took 0.672477s\n",
            "[TL]    val loss: 0.064597\n",
            "[TL]    val acc: 0.981600\n",
            "[TL] Epoch 115 of 500 took 0.674374s\n",
            "[TL]    val loss: 0.063861\n",
            "[TL]    val acc: 0.982700\n",
            "[TL] Epoch 120 of 500 took 0.663636s\n",
            "[TL]    val loss: 0.062970\n",
            "[TL]    val acc: 0.982500\n",
            "[TL] Epoch 125 of 500 took 0.667859s\n",
            "[TL]    val loss: 0.062207\n",
            "[TL]    val acc: 0.983200\n",
            "[TL] Epoch 130 of 500 took 0.671733s\n",
            "[TL]    val loss: 0.060377\n",
            "[TL]    val acc: 0.983900\n",
            "[TL] Epoch 135 of 500 took 0.673106s\n",
            "[TL]    val loss: 0.059444\n",
            "[TL]    val acc: 0.983600\n",
            "[TL] Epoch 140 of 500 took 0.666709s\n",
            "[TL]    val loss: 0.058081\n",
            "[TL]    val acc: 0.984100\n",
            "[TL] Epoch 145 of 500 took 0.669608s\n",
            "[TL]    val loss: 0.058795\n",
            "[TL]    val acc: 0.984100\n",
            "[TL] Epoch 150 of 500 took 0.678958s\n",
            "[TL]    val loss: 0.057504\n",
            "[TL]    val acc: 0.984500\n",
            "[TL] Epoch 155 of 500 took 0.677900s\n",
            "[TL]    val loss: 0.056926\n",
            "[TL]    val acc: 0.984100\n",
            "[TL] Epoch 160 of 500 took 0.665554s\n",
            "[TL]    val loss: 0.056091\n",
            "[TL]    val acc: 0.983900\n",
            "[TL] Epoch 165 of 500 took 0.674880s\n",
            "[TL]    val loss: 0.057035\n",
            "[TL]    val acc: 0.984700\n",
            "[TL] Epoch 170 of 500 took 0.673144s\n",
            "[TL]    val loss: 0.056205\n",
            "[TL]    val acc: 0.984800\n",
            "[TL] Epoch 175 of 500 took 0.671817s\n",
            "[TL]    val loss: 0.054877\n",
            "[TL]    val acc: 0.984700\n",
            "[TL] Epoch 180 of 500 took 0.666777s\n",
            "[TL]    val loss: 0.054750\n",
            "[TL]    val acc: 0.984800\n",
            "[TL] Epoch 185 of 500 took 0.659790s\n",
            "[TL]    val loss: 0.054736\n",
            "[TL]    val acc: 0.985100\n",
            "[TL] Epoch 190 of 500 took 0.679442s\n",
            "[TL]    val loss: 0.054379\n",
            "[TL]    val acc: 0.985200\n",
            "[TL] Epoch 195 of 500 took 0.685446s\n",
            "[TL]    val loss: 0.053818\n",
            "[TL]    val acc: 0.985200\n",
            "[TL] Epoch 200 of 500 took 0.668872s\n",
            "[TL]    val loss: 0.052792\n",
            "[TL]    val acc: 0.984800\n",
            "[TL] Epoch 205 of 500 took 0.656487s\n",
            "[TL]    val loss: 0.053177\n",
            "[TL]    val acc: 0.985600\n",
            "[TL] Epoch 210 of 500 took 0.671416s\n",
            "[TL]    val loss: 0.051813\n",
            "[TL]    val acc: 0.986000\n",
            "[TL] Epoch 215 of 500 took 0.670697s\n",
            "[TL]    val loss: 0.053433\n",
            "[TL]    val acc: 0.985600\n",
            "[TL] Epoch 220 of 500 took 0.667447s\n",
            "[TL]    val loss: 0.052935\n",
            "[TL]    val acc: 0.986300\n",
            "[TL] Epoch 225 of 500 took 0.669397s\n",
            "[TL]    val loss: 0.052373\n",
            "[TL]    val acc: 0.985700\n",
            "[TL] Epoch 230 of 500 took 0.682873s\n",
            "[TL]    val loss: 0.052584\n",
            "[TL]    val acc: 0.985500\n",
            "[TL] Epoch 235 of 500 took 0.671882s\n",
            "[TL]    val loss: 0.052355\n",
            "[TL]    val acc: 0.985700\n",
            "[TL] Epoch 240 of 500 took 0.672833s\n",
            "[TL]    val loss: 0.051193\n",
            "[TL]    val acc: 0.986000\n",
            "[TL] Epoch 245 of 500 took 0.668748s\n",
            "[TL]    val loss: 0.052906\n",
            "[TL]    val acc: 0.985900\n",
            "[TL] Epoch 250 of 500 took 0.673801s\n",
            "[TL]    val loss: 0.051679\n",
            "[TL]    val acc: 0.986100\n",
            "[TL] Epoch 255 of 500 took 0.693095s\n",
            "[TL]    val loss: 0.051550\n",
            "[TL]    val acc: 0.986200\n",
            "[TL] Epoch 260 of 500 took 0.679912s\n",
            "[TL]    val loss: 0.051376\n",
            "[TL]    val acc: 0.986000\n",
            "[TL] Epoch 265 of 500 took 0.670385s\n",
            "[TL]    val loss: 0.051202\n",
            "[TL]    val acc: 0.986300\n",
            "[TL] Epoch 270 of 500 took 0.672667s\n",
            "[TL]    val loss: 0.052151\n",
            "[TL]    val acc: 0.986600\n",
            "[TL] Epoch 275 of 500 took 0.685272s\n",
            "[TL]    val loss: 0.051025\n",
            "[TL]    val acc: 0.986400\n",
            "[TL] Epoch 280 of 500 took 0.673586s\n",
            "[TL]    val loss: 0.051875\n",
            "[TL]    val acc: 0.986300\n",
            "[TL] Epoch 285 of 500 took 0.667048s\n",
            "[TL]    val loss: 0.051933\n",
            "[TL]    val acc: 0.986400\n",
            "[TL] Epoch 290 of 500 took 0.667807s\n",
            "[TL]    val loss: 0.051316\n",
            "[TL]    val acc: 0.986000\n",
            "[TL] Epoch 295 of 500 took 0.680096s\n",
            "[TL]    val loss: 0.050825\n",
            "[TL]    val acc: 0.986400\n",
            "[TL] Epoch 300 of 500 took 0.679137s\n",
            "[TL]    val loss: 0.051013\n",
            "[TL]    val acc: 0.985900\n",
            "[TL] Epoch 305 of 500 took 0.667626s\n",
            "[TL]    val loss: 0.050834\n",
            "[TL]    val acc: 0.986200\n",
            "[TL] Epoch 310 of 500 took 0.660889s\n",
            "[TL]    val loss: 0.051122\n",
            "[TL]    val acc: 0.987000\n",
            "[TL] Epoch 315 of 500 took 0.677363s\n",
            "[TL]    val loss: 0.051092\n",
            "[TL]    val acc: 0.986300\n",
            "[TL] Epoch 320 of 500 took 0.683370s\n",
            "[TL]    val loss: 0.052215\n",
            "[TL]    val acc: 0.986400\n",
            "[TL] Epoch 325 of 500 took 0.664530s\n",
            "[TL]    val loss: 0.051948\n",
            "[TL]    val acc: 0.986600\n",
            "[TL] Epoch 330 of 500 took 0.667166s\n",
            "[TL]    val loss: 0.052129\n",
            "[TL]    val acc: 0.986300\n",
            "[TL] Epoch 335 of 500 took 0.671487s\n",
            "[TL]    val loss: 0.051865\n",
            "[TL]    val acc: 0.985800\n",
            "[TL] Epoch 340 of 500 took 0.675347s\n",
            "[TL]    val loss: 0.051218\n",
            "[TL]    val acc: 0.986700\n",
            "[TL] Epoch 345 of 500 took 0.671046s\n",
            "[TL]    val loss: 0.051917\n",
            "[TL]    val acc: 0.986500\n",
            "[TL] Epoch 350 of 500 took 0.660663s\n",
            "[TL]    val loss: 0.051431\n",
            "[TL]    val acc: 0.986800\n",
            "[TL] Epoch 355 of 500 took 0.667686s\n",
            "[TL]    val loss: 0.051565\n",
            "[TL]    val acc: 0.987100\n",
            "[TL] Epoch 360 of 500 took 0.673475s\n",
            "[TL]    val loss: 0.053174\n",
            "[TL]    val acc: 0.986600\n",
            "[TL] Epoch 365 of 500 took 0.674868s\n",
            "[TL]    val loss: 0.052148\n",
            "[TL]    val acc: 0.986800\n",
            "[TL] Epoch 370 of 500 took 0.669914s\n",
            "[TL]    val loss: 0.051259\n",
            "[TL]    val acc: 0.987300\n",
            "[TL] Epoch 375 of 500 took 0.668682s\n",
            "[TL]    val loss: 0.051590\n",
            "[TL]    val acc: 0.986700\n",
            "[TL] Epoch 380 of 500 took 0.674853s\n",
            "[TL]    val loss: 0.051644\n",
            "[TL]    val acc: 0.987000\n",
            "[TL] Epoch 385 of 500 took 0.681219s\n",
            "[TL]    val loss: 0.051078\n",
            "[TL]    val acc: 0.987500\n",
            "[TL] Epoch 390 of 500 took 0.668974s\n",
            "[TL]    val loss: 0.051927\n",
            "[TL]    val acc: 0.987000\n",
            "[TL] Epoch 395 of 500 took 0.665996s\n",
            "[TL]    val loss: 0.052682\n",
            "[TL]    val acc: 0.986600\n",
            "[TL] Epoch 400 of 500 took 0.689002s\n",
            "[TL]    val loss: 0.052893\n",
            "[TL]    val acc: 0.986800\n",
            "[TL] Epoch 405 of 500 took 0.677715s\n",
            "[TL]    val loss: 0.052011\n",
            "[TL]    val acc: 0.987200\n",
            "[TL] Epoch 410 of 500 took 0.663342s\n",
            "[TL]    val loss: 0.051393\n",
            "[TL]    val acc: 0.987500\n",
            "[TL] Epoch 415 of 500 took 0.671945s\n",
            "[TL]    val loss: 0.052371\n",
            "[TL]    val acc: 0.987000\n",
            "[TL] Epoch 420 of 500 took 0.678513s\n",
            "[TL]    val loss: 0.053100\n",
            "[TL]    val acc: 0.987000\n",
            "[TL] Epoch 425 of 500 took 0.673100s\n",
            "[TL]    val loss: 0.053917\n",
            "[TL]    val acc: 0.987600\n",
            "[TL] Epoch 430 of 500 took 0.662943s\n",
            "[TL]    val loss: 0.052697\n",
            "[TL]    val acc: 0.986900\n",
            "[TL] Epoch 435 of 500 took 0.662738s\n",
            "[TL]    val loss: 0.052093\n",
            "[TL]    val acc: 0.986500\n",
            "[TL] Epoch 440 of 500 took 0.698344s\n",
            "[TL]    val loss: 0.052220\n",
            "[TL]    val acc: 0.987300\n",
            "[TL] Epoch 445 of 500 took 0.685054s\n",
            "[TL]    val loss: 0.051836\n",
            "[TL]    val acc: 0.987400\n",
            "[TL] Epoch 450 of 500 took 0.682452s\n",
            "[TL]    val loss: 0.052720\n",
            "[TL]    val acc: 0.987100\n",
            "[TL] Epoch 455 of 500 took 0.686900s\n",
            "[TL]    val loss: 0.051609\n",
            "[TL]    val acc: 0.987400\n",
            "[TL] Epoch 460 of 500 took 0.681710s\n",
            "[TL]    val loss: 0.053298\n",
            "[TL]    val acc: 0.986600\n",
            "[TL] Epoch 465 of 500 took 0.665167s\n",
            "[TL]    val loss: 0.052567\n",
            "[TL]    val acc: 0.987400\n",
            "[TL] Epoch 470 of 500 took 0.677474s\n",
            "[TL]    val loss: 0.051548\n",
            "[TL]    val acc: 0.988000\n",
            "[TL] Epoch 475 of 500 took 0.661663s\n",
            "[TL]    val loss: 0.053592\n",
            "[TL]    val acc: 0.986900\n",
            "[TL] Epoch 480 of 500 took 0.665063s\n",
            "[TL]    val loss: 0.051748\n",
            "[TL]    val acc: 0.986800\n",
            "[TL] Epoch 485 of 500 took 0.677938s\n",
            "[TL]    val loss: 0.053014\n",
            "[TL]    val acc: 0.987200\n",
            "[TL] Epoch 490 of 500 took 0.672514s\n",
            "[TL]    val loss: 0.052775\n",
            "[TL]    val acc: 0.987300\n",
            "[TL] Epoch 495 of 500 took 0.663398s\n",
            "[TL]    val loss: 0.053380\n",
            "[TL]    val acc: 0.987200\n",
            "[TL] Epoch 500 of 500 took 0.681317s\n",
            "[TL]    val loss: 0.053046\n",
            "[TL]    val acc: 0.987000\n",
            "[TL] Total training time: 345.280445s\n",
            "[TL] Start testing the network ...\n",
            "[TL]    test loss: 0.045024\n",
            "[TL]    test acc: 0.987500\n",
            "[TL] [*] model.npz saved\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}